defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart42Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: utri; with a semi-colon" do
    input = "&utri;"
    output = [["Character", "â–µ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: utrif; with a semi-colon" do
    input = "&utrif;"
    output = [["Character", "â–´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: uuarr; with a semi-colon" do
    input = "&uuarr;"
    output = [["Character", "â‡ˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: uuml without a semi-colon" do
    input = "&uuml"
    output = [["Character", "Ã¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: uuml; with a semi-colon" do
    input = "&uuml;"
    output = [["Character", "Ã¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: uwangle; with a semi-colon" do
    input = "&uwangle;"
    output = [["Character", "â¦§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vArr; with a semi-colon" do
    input = "&vArr;"
    output = [["Character", "â‡•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vBar; with a semi-colon" do
    input = "&vBar;"
    output = [["Character", "â«¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vBarv; with a semi-colon" do
    input = "&vBarv;"
    output = [["Character", "â«©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vDash; with a semi-colon" do
    input = "&vDash;"
    output = [["Character", "âŠ¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vangrt; with a semi-colon" do
    input = "&vangrt;"
    output = [["Character", "â¦œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varepsilon; with a semi-colon" do
    input = "&varepsilon;"
    output = [["Character", "Ïµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varkappa; with a semi-colon" do
    input = "&varkappa;"
    output = [["Character", "Ï°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varnothing; with a semi-colon" do
    input = "&varnothing;"
    output = [["Character", "âˆ…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varphi; with a semi-colon" do
    input = "&varphi;"
    output = [["Character", "Ï•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varpi; with a semi-colon" do
    input = "&varpi;"
    output = [["Character", "Ï–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varpropto; with a semi-colon" do
    input = "&varpropto;"
    output = [["Character", "âˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varr; with a semi-colon" do
    input = "&varr;"
    output = [["Character", "â†•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varrho; with a semi-colon" do
    input = "&varrho;"
    output = [["Character", "Ï±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsigma; with a semi-colon" do
    input = "&varsigma;"
    output = [["Character", "Ï‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsubsetneq; with a semi-colon" do
    input = "&varsubsetneq;"
    output = [["Character", "âŠŠï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsubsetneqq; with a semi-colon" do
    input = "&varsubsetneqq;"
    output = [["Character", "â«‹ï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsupsetneq; with a semi-colon" do
    input = "&varsupsetneq;"
    output = [["Character", "âŠ‹ï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsupsetneqq; with a semi-colon" do
    input = "&varsupsetneqq;"
    output = [["Character", "â«Œï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vartheta; with a semi-colon" do
    input = "&vartheta;"
    output = [["Character", "Ï‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vartriangleleft; with a semi-colon" do
    input = "&vartriangleleft;"
    output = [["Character", "âŠ²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vartriangleright; with a semi-colon" do
    input = "&vartriangleright;"
    output = [["Character", "âŠ³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vcy; with a semi-colon" do
    input = "&vcy;"
    output = [["Character", "Ð²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vdash; with a semi-colon" do
    input = "&vdash;"
    output = [["Character", "âŠ¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vee; with a semi-colon" do
    input = "&vee;"
    output = [["Character", "âˆ¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: veebar; with a semi-colon" do
    input = "&veebar;"
    output = [["Character", "âŠ»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: veeeq; with a semi-colon" do
    input = "&veeeq;"
    output = [["Character", "â‰š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vellip; with a semi-colon" do
    input = "&vellip;"
    output = [["Character", "â‹®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: verbar; with a semi-colon" do
    input = "&verbar;"
    output = [["Character", "|"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vert; with a semi-colon" do
    input = "&vert;"
    output = [["Character", "|"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vfr; with a semi-colon" do
    input = "&vfr;"
    output = [["Character", "ð”³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vltri; with a semi-colon" do
    input = "&vltri;"
    output = [["Character", "âŠ²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vnsub; with a semi-colon" do
    input = "&vnsub;"
    output = [["Character", "âŠ‚âƒ’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vnsup; with a semi-colon" do
    input = "&vnsup;"
    output = [["Character", "âŠƒâƒ’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vopf; with a semi-colon" do
    input = "&vopf;"
    output = [["Character", "ð•§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vprop; with a semi-colon" do
    input = "&vprop;"
    output = [["Character", "âˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vrtri; with a semi-colon" do
    input = "&vrtri;"
    output = [["Character", "âŠ³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vscr; with a semi-colon" do
    input = "&vscr;"
    output = [["Character", "ð“‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vsubnE; with a semi-colon" do
    input = "&vsubnE;"
    output = [["Character", "â«‹ï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vsubne; with a semi-colon" do
    input = "&vsubne;"
    output = [["Character", "âŠŠï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vsupnE; with a semi-colon" do
    input = "&vsupnE;"
    output = [["Character", "â«Œï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vsupne; with a semi-colon" do
    input = "&vsupne;"
    output = [["Character", "âŠ‹ï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vzigzag; with a semi-colon" do
    input = "&vzigzag;"
    output = [["Character", "â¦š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wcirc; with a semi-colon" do
    input = "&wcirc;"
    output = [["Character", "Åµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wedbar; with a semi-colon" do
    input = "&wedbar;"
    output = [["Character", "â©Ÿ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wedge; with a semi-colon" do
    input = "&wedge;"
    output = [["Character", "âˆ§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wedgeq; with a semi-colon" do
    input = "&wedgeq;"
    output = [["Character", "â‰™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: weierp; with a semi-colon" do
    input = "&weierp;"
    output = [["Character", "â„˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wfr; with a semi-colon" do
    input = "&wfr;"
    output = [["Character", "ð”´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wopf; with a semi-colon" do
    input = "&wopf;"
    output = [["Character", "ð•¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wp; with a semi-colon" do
    input = "&wp;"
    output = [["Character", "â„˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wr; with a semi-colon" do
    input = "&wr;"
    output = [["Character", "â‰€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wreath; with a semi-colon" do
    input = "&wreath;"
    output = [["Character", "â‰€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wscr; with a semi-colon" do
    input = "&wscr;"
    output = [["Character", "ð“Œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xcap; with a semi-colon" do
    input = "&xcap;"
    output = [["Character", "â‹‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xcirc; with a semi-colon" do
    input = "&xcirc;"
    output = [["Character", "â—¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xcup; with a semi-colon" do
    input = "&xcup;"
    output = [["Character", "â‹ƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xdtri; with a semi-colon" do
    input = "&xdtri;"
    output = [["Character", "â–½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xfr; with a semi-colon" do
    input = "&xfr;"
    output = [["Character", "ð”µ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xhArr; with a semi-colon" do
    input = "&xhArr;"
    output = [["Character", "âŸº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xharr; with a semi-colon" do
    input = "&xharr;"
    output = [["Character", "âŸ·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xi; with a semi-colon" do
    input = "&xi;"
    output = [["Character", "Î¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xlArr; with a semi-colon" do
    input = "&xlArr;"
    output = [["Character", "âŸ¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xlarr; with a semi-colon" do
    input = "&xlarr;"
    output = [["Character", "âŸµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xmap; with a semi-colon" do
    input = "&xmap;"
    output = [["Character", "âŸ¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xnis; with a semi-colon" do
    input = "&xnis;"
    output = [["Character", "â‹»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xodot; with a semi-colon" do
    input = "&xodot;"
    output = [["Character", "â¨€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xopf; with a semi-colon" do
    input = "&xopf;"
    output = [["Character", "ð•©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xoplus; with a semi-colon" do
    input = "&xoplus;"
    output = [["Character", "â¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xotime; with a semi-colon" do
    input = "&xotime;"
    output = [["Character", "â¨‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xrArr; with a semi-colon" do
    input = "&xrArr;"
    output = [["Character", "âŸ¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xrarr; with a semi-colon" do
    input = "&xrarr;"
    output = [["Character", "âŸ¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xscr; with a semi-colon" do
    input = "&xscr;"
    output = [["Character", "ð“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xsqcup; with a semi-colon" do
    input = "&xsqcup;"
    output = [["Character", "â¨†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xuplus; with a semi-colon" do
    input = "&xuplus;"
    output = [["Character", "â¨„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xutri; with a semi-colon" do
    input = "&xutri;"
    output = [["Character", "â–³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xvee; with a semi-colon" do
    input = "&xvee;"
    output = [["Character", "â‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xwedge; with a semi-colon" do
    input = "&xwedge;"
    output = [["Character", "â‹€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yacute without a semi-colon" do
    input = "&yacute"
    output = [["Character", "Ã½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yacute; with a semi-colon" do
    input = "&yacute;"
    output = [["Character", "Ã½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yacy; with a semi-colon" do
    input = "&yacy;"
    output = [["Character", "Ñ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ycirc; with a semi-colon" do
    input = "&ycirc;"
    output = [["Character", "Å·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ycy; with a semi-colon" do
    input = "&ycy;"
    output = [["Character", "Ñ‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yen without a semi-colon" do
    input = "&yen"
    output = [["Character", "Â¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yen; with a semi-colon" do
    input = "&yen;"
    output = [["Character", "Â¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yfr; with a semi-colon" do
    input = "&yfr;"
    output = [["Character", "ð”¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yicy; with a semi-colon" do
    input = "&yicy;"
    output = [["Character", "Ñ—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yopf; with a semi-colon" do
    input = "&yopf;"
    output = [["Character", "ð•ª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yscr; with a semi-colon" do
    input = "&yscr;"
    output = [["Character", "ð“Ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yucy; with a semi-colon" do
    input = "&yucy;"
    output = [["Character", "ÑŽ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yuml without a semi-colon" do
    input = "&yuml"
    output = [["Character", "Ã¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yuml; with a semi-colon" do
    input = "&yuml;"
    output = [["Character", "Ã¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: zacute; with a semi-colon" do
    input = "&zacute;"
    output = [["Character", "Åº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: zcaron; with a semi-colon" do
    input = "&zcaron;"
    output = [["Character", "Å¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: zcy; with a semi-colon" do
    input = "&zcy;"
    output = [["Character", "Ð·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end