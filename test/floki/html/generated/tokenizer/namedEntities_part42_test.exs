defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart42Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: utri; with a semi-colon" do
    input = "&utri;"
    output = [["Character", "▵"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: utrif; with a semi-colon" do
    input = "&utrif;"
    output = [["Character", "▴"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: uuarr; with a semi-colon" do
    input = "&uuarr;"
    output = [["Character", "⇈"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: uuml without a semi-colon" do
    input = "&uuml"
    output = [["Character", "ü"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: uuml; with a semi-colon" do
    input = "&uuml;"
    output = [["Character", "ü"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: uwangle; with a semi-colon" do
    input = "&uwangle;"
    output = [["Character", "⦧"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vArr; with a semi-colon" do
    input = "&vArr;"
    output = [["Character", "⇕"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vBar; with a semi-colon" do
    input = "&vBar;"
    output = [["Character", "⫨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vBarv; with a semi-colon" do
    input = "&vBarv;"
    output = [["Character", "⫩"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vDash; with a semi-colon" do
    input = "&vDash;"
    output = [["Character", "⊨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vangrt; with a semi-colon" do
    input = "&vangrt;"
    output = [["Character", "⦜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varepsilon; with a semi-colon" do
    input = "&varepsilon;"
    output = [["Character", "ϵ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varkappa; with a semi-colon" do
    input = "&varkappa;"
    output = [["Character", "ϰ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varnothing; with a semi-colon" do
    input = "&varnothing;"
    output = [["Character", "∅"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varphi; with a semi-colon" do
    input = "&varphi;"
    output = [["Character", "ϕ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varpi; with a semi-colon" do
    input = "&varpi;"
    output = [["Character", "ϖ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varpropto; with a semi-colon" do
    input = "&varpropto;"
    output = [["Character", "∝"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varr; with a semi-colon" do
    input = "&varr;"
    output = [["Character", "↕"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varrho; with a semi-colon" do
    input = "&varrho;"
    output = [["Character", "ϱ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsigma; with a semi-colon" do
    input = "&varsigma;"
    output = [["Character", "ς"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsubsetneq; with a semi-colon" do
    input = "&varsubsetneq;"
    output = [["Character", "⊊︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsubsetneqq; with a semi-colon" do
    input = "&varsubsetneqq;"
    output = [["Character", "⫋︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsupsetneq; with a semi-colon" do
    input = "&varsupsetneq;"
    output = [["Character", "⊋︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: varsupsetneqq; with a semi-colon" do
    input = "&varsupsetneqq;"
    output = [["Character", "⫌︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vartheta; with a semi-colon" do
    input = "&vartheta;"
    output = [["Character", "ϑ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vartriangleleft; with a semi-colon" do
    input = "&vartriangleleft;"
    output = [["Character", "⊲"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vartriangleright; with a semi-colon" do
    input = "&vartriangleright;"
    output = [["Character", "⊳"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vcy; with a semi-colon" do
    input = "&vcy;"
    output = [["Character", "в"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vdash; with a semi-colon" do
    input = "&vdash;"
    output = [["Character", "⊢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vee; with a semi-colon" do
    input = "&vee;"
    output = [["Character", "∨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: veebar; with a semi-colon" do
    input = "&veebar;"
    output = [["Character", "⊻"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: veeeq; with a semi-colon" do
    input = "&veeeq;"
    output = [["Character", "≚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vellip; with a semi-colon" do
    input = "&vellip;"
    output = [["Character", "⋮"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: verbar; with a semi-colon" do
    input = "&verbar;"
    output = [["Character", "|"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vert; with a semi-colon" do
    input = "&vert;"
    output = [["Character", "|"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vfr; with a semi-colon" do
    input = "&vfr;"
    output = [["Character", "𝔳"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vltri; with a semi-colon" do
    input = "&vltri;"
    output = [["Character", "⊲"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vnsub; with a semi-colon" do
    input = "&vnsub;"
    output = [["Character", "⊂⃒"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vnsup; with a semi-colon" do
    input = "&vnsup;"
    output = [["Character", "⊃⃒"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vopf; with a semi-colon" do
    input = "&vopf;"
    output = [["Character", "𝕧"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vprop; with a semi-colon" do
    input = "&vprop;"
    output = [["Character", "∝"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vrtri; with a semi-colon" do
    input = "&vrtri;"
    output = [["Character", "⊳"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vscr; with a semi-colon" do
    input = "&vscr;"
    output = [["Character", "𝓋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vsubnE; with a semi-colon" do
    input = "&vsubnE;"
    output = [["Character", "⫋︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vsubne; with a semi-colon" do
    input = "&vsubne;"
    output = [["Character", "⊊︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vsupnE; with a semi-colon" do
    input = "&vsupnE;"
    output = [["Character", "⫌︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vsupne; with a semi-colon" do
    input = "&vsupne;"
    output = [["Character", "⊋︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: vzigzag; with a semi-colon" do
    input = "&vzigzag;"
    output = [["Character", "⦚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wcirc; with a semi-colon" do
    input = "&wcirc;"
    output = [["Character", "ŵ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wedbar; with a semi-colon" do
    input = "&wedbar;"
    output = [["Character", "⩟"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wedge; with a semi-colon" do
    input = "&wedge;"
    output = [["Character", "∧"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wedgeq; with a semi-colon" do
    input = "&wedgeq;"
    output = [["Character", "≙"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: weierp; with a semi-colon" do
    input = "&weierp;"
    output = [["Character", "℘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wfr; with a semi-colon" do
    input = "&wfr;"
    output = [["Character", "𝔴"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wopf; with a semi-colon" do
    input = "&wopf;"
    output = [["Character", "𝕨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wp; with a semi-colon" do
    input = "&wp;"
    output = [["Character", "℘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wr; with a semi-colon" do
    input = "&wr;"
    output = [["Character", "≀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wreath; with a semi-colon" do
    input = "&wreath;"
    output = [["Character", "≀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: wscr; with a semi-colon" do
    input = "&wscr;"
    output = [["Character", "𝓌"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xcap; with a semi-colon" do
    input = "&xcap;"
    output = [["Character", "⋂"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xcirc; with a semi-colon" do
    input = "&xcirc;"
    output = [["Character", "◯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xcup; with a semi-colon" do
    input = "&xcup;"
    output = [["Character", "⋃"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xdtri; with a semi-colon" do
    input = "&xdtri;"
    output = [["Character", "▽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xfr; with a semi-colon" do
    input = "&xfr;"
    output = [["Character", "𝔵"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xhArr; with a semi-colon" do
    input = "&xhArr;"
    output = [["Character", "⟺"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xharr; with a semi-colon" do
    input = "&xharr;"
    output = [["Character", "⟷"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xi; with a semi-colon" do
    input = "&xi;"
    output = [["Character", "ξ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xlArr; with a semi-colon" do
    input = "&xlArr;"
    output = [["Character", "⟸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xlarr; with a semi-colon" do
    input = "&xlarr;"
    output = [["Character", "⟵"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xmap; with a semi-colon" do
    input = "&xmap;"
    output = [["Character", "⟼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xnis; with a semi-colon" do
    input = "&xnis;"
    output = [["Character", "⋻"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xodot; with a semi-colon" do
    input = "&xodot;"
    output = [["Character", "⨀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xopf; with a semi-colon" do
    input = "&xopf;"
    output = [["Character", "𝕩"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xoplus; with a semi-colon" do
    input = "&xoplus;"
    output = [["Character", "⨁"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xotime; with a semi-colon" do
    input = "&xotime;"
    output = [["Character", "⨂"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xrArr; with a semi-colon" do
    input = "&xrArr;"
    output = [["Character", "⟹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xrarr; with a semi-colon" do
    input = "&xrarr;"
    output = [["Character", "⟶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xscr; with a semi-colon" do
    input = "&xscr;"
    output = [["Character", "𝓍"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xsqcup; with a semi-colon" do
    input = "&xsqcup;"
    output = [["Character", "⨆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xuplus; with a semi-colon" do
    input = "&xuplus;"
    output = [["Character", "⨄"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xutri; with a semi-colon" do
    input = "&xutri;"
    output = [["Character", "△"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xvee; with a semi-colon" do
    input = "&xvee;"
    output = [["Character", "⋁"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: xwedge; with a semi-colon" do
    input = "&xwedge;"
    output = [["Character", "⋀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yacute without a semi-colon" do
    input = "&yacute"
    output = [["Character", "ý"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yacute; with a semi-colon" do
    input = "&yacute;"
    output = [["Character", "ý"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yacy; with a semi-colon" do
    input = "&yacy;"
    output = [["Character", "я"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ycirc; with a semi-colon" do
    input = "&ycirc;"
    output = [["Character", "ŷ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ycy; with a semi-colon" do
    input = "&ycy;"
    output = [["Character", "ы"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yen without a semi-colon" do
    input = "&yen"
    output = [["Character", "¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yen; with a semi-colon" do
    input = "&yen;"
    output = [["Character", "¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yfr; with a semi-colon" do
    input = "&yfr;"
    output = [["Character", "𝔶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yicy; with a semi-colon" do
    input = "&yicy;"
    output = [["Character", "ї"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yopf; with a semi-colon" do
    input = "&yopf;"
    output = [["Character", "𝕪"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yscr; with a semi-colon" do
    input = "&yscr;"
    output = [["Character", "𝓎"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yucy; with a semi-colon" do
    input = "&yucy;"
    output = [["Character", "ю"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yuml without a semi-colon" do
    input = "&yuml"
    output = [["Character", "ÿ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: yuml; with a semi-colon" do
    input = "&yuml;"
    output = [["Character", "ÿ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: zacute; with a semi-colon" do
    input = "&zacute;"
    output = [["Character", "ź"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: zcaron; with a semi-colon" do
    input = "&zcaron;"
    output = [["Character", "ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: zcy; with a semi-colon" do
    input = "&zcy;"
    output = [["Character", "з"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end