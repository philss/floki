defmodule Floki.HTML.Generated.Tokenizer.NumericentitiesPart2Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests numericEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Valid numeric entity character U+0009" do
    input = "&#x0009;"
    output = [["Character", "\t"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+000A" do
    input = "&#x000a;"
    output = [["Character", "\n"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0020" do
    input = "&#x0020;"
    output = [["Character", " "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0021" do
    input = "&#x0021;"
    output = [["Character", "!"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0022" do
    input = "&#x0022;"
    output = [["Character", "\""]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0023" do
    input = "&#x0023;"
    output = [["Character", "#"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0024" do
    input = "&#x0024;"
    output = [["Character", "$"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0025" do
    input = "&#x0025;"
    output = [["Character", "%"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0026" do
    input = "&#x0026;"
    output = [["Character", "&"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0027" do
    input = "&#x0027;"
    output = [["Character", "'"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0028" do
    input = "&#x0028;"
    output = [["Character", "("]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0029" do
    input = "&#x0029;"
    output = [["Character", ")"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+002A" do
    input = "&#x002a;"
    output = [["Character", "*"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+002B" do
    input = "&#x002b;"
    output = [["Character", "+"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+002C" do
    input = "&#x002c;"
    output = [["Character", ","]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+002D" do
    input = "&#x002d;"
    output = [["Character", "-"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+002E" do
    input = "&#x002e;"
    output = [["Character", "."]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+002F" do
    input = "&#x002f;"
    output = [["Character", "/"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0030" do
    input = "&#x0030;"
    output = [["Character", "0"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0031" do
    input = "&#x0031;"
    output = [["Character", "1"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0032" do
    input = "&#x0032;"
    output = [["Character", "2"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0033" do
    input = "&#x0033;"
    output = [["Character", "3"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0034" do
    input = "&#x0034;"
    output = [["Character", "4"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0035" do
    input = "&#x0035;"
    output = [["Character", "5"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0036" do
    input = "&#x0036;"
    output = [["Character", "6"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0037" do
    input = "&#x0037;"
    output = [["Character", "7"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0038" do
    input = "&#x0038;"
    output = [["Character", "8"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0039" do
    input = "&#x0039;"
    output = [["Character", "9"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+003A" do
    input = "&#x003a;"
    output = [["Character", ":"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+003B" do
    input = "&#x003b;"
    output = [["Character", ";"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+003C" do
    input = "&#x003c;"
    output = [["Character", "<"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+003D" do
    input = "&#x003d;"
    output = [["Character", "="]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+003E" do
    input = "&#x003e;"
    output = [["Character", ">"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+003F" do
    input = "&#x003f;"
    output = [["Character", "?"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0040" do
    input = "&#x0040;"
    output = [["Character", "@"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0041" do
    input = "&#x0041;"
    output = [["Character", "A"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0042" do
    input = "&#x0042;"
    output = [["Character", "B"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0043" do
    input = "&#x0043;"
    output = [["Character", "C"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0044" do
    input = "&#x0044;"
    output = [["Character", "D"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0045" do
    input = "&#x0045;"
    output = [["Character", "E"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0046" do
    input = "&#x0046;"
    output = [["Character", "F"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0047" do
    input = "&#x0047;"
    output = [["Character", "G"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0048" do
    input = "&#x0048;"
    output = [["Character", "H"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0049" do
    input = "&#x0049;"
    output = [["Character", "I"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+004A" do
    input = "&#x004a;"
    output = [["Character", "J"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+004B" do
    input = "&#x004b;"
    output = [["Character", "K"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+004C" do
    input = "&#x004c;"
    output = [["Character", "L"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+004D" do
    input = "&#x004d;"
    output = [["Character", "M"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+004E" do
    input = "&#x004e;"
    output = [["Character", "N"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+004F" do
    input = "&#x004f;"
    output = [["Character", "O"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0050" do
    input = "&#x0050;"
    output = [["Character", "P"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0051" do
    input = "&#x0051;"
    output = [["Character", "Q"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0052" do
    input = "&#x0052;"
    output = [["Character", "R"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0053" do
    input = "&#x0053;"
    output = [["Character", "S"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0054" do
    input = "&#x0054;"
    output = [["Character", "T"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0055" do
    input = "&#x0055;"
    output = [["Character", "U"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0056" do
    input = "&#x0056;"
    output = [["Character", "V"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0057" do
    input = "&#x0057;"
    output = [["Character", "W"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0058" do
    input = "&#x0058;"
    output = [["Character", "X"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0059" do
    input = "&#x0059;"
    output = [["Character", "Y"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+005A" do
    input = "&#x005a;"
    output = [["Character", "Z"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+005B" do
    input = "&#x005b;"
    output = [["Character", "["]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+005C" do
    input = "&#x005c;"
    output = [["Character", "\\"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+005D" do
    input = "&#x005d;"
    output = [["Character", "]"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+005E" do
    input = "&#x005e;"
    output = [["Character", "^"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+005F" do
    input = "&#x005f;"
    output = [["Character", "_"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0060" do
    input = "&#x0060;"
    output = [["Character", "`"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0061" do
    input = "&#x0061;"
    output = [["Character", "a"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0062" do
    input = "&#x0062;"
    output = [["Character", "b"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0063" do
    input = "&#x0063;"
    output = [["Character", "c"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0064" do
    input = "&#x0064;"
    output = [["Character", "d"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0065" do
    input = "&#x0065;"
    output = [["Character", "e"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0066" do
    input = "&#x0066;"
    output = [["Character", "f"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0067" do
    input = "&#x0067;"
    output = [["Character", "g"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0068" do
    input = "&#x0068;"
    output = [["Character", "h"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0069" do
    input = "&#x0069;"
    output = [["Character", "i"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+006A" do
    input = "&#x006a;"
    output = [["Character", "j"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+006B" do
    input = "&#x006b;"
    output = [["Character", "k"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+006C" do
    input = "&#x006c;"
    output = [["Character", "l"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+006D" do
    input = "&#x006d;"
    output = [["Character", "m"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+006E" do
    input = "&#x006e;"
    output = [["Character", "n"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+006F" do
    input = "&#x006f;"
    output = [["Character", "o"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0070" do
    input = "&#x0070;"
    output = [["Character", "p"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0071" do
    input = "&#x0071;"
    output = [["Character", "q"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0072" do
    input = "&#x0072;"
    output = [["Character", "r"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0073" do
    input = "&#x0073;"
    output = [["Character", "s"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0074" do
    input = "&#x0074;"
    output = [["Character", "t"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0075" do
    input = "&#x0075;"
    output = [["Character", "u"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0076" do
    input = "&#x0076;"
    output = [["Character", "v"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0077" do
    input = "&#x0077;"
    output = [["Character", "w"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0078" do
    input = "&#x0078;"
    output = [["Character", "x"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+0079" do
    input = "&#x0079;"
    output = [["Character", "y"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+007A" do
    input = "&#x007a;"
    output = [["Character", "z"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+007B" do
    input = "&#x007b;"
    output = [["Character", "{"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+007C" do
    input = "&#x007c;"
    output = [["Character", "|"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+007D" do
    input = "&#x007d;"
    output = [["Character", "}"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+007E" do
    input = "&#x007e;"
    output = [["Character", "~"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A0" do
    input = "&#x00a0;"
    output = [["Character", " "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A1" do
    input = "&#x00a1;"
    output = [["Character", "¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A2" do
    input = "&#x00a2;"
    output = [["Character", "¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end