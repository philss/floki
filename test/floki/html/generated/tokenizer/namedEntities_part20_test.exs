defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart20Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Bad named entity: vartriangleleft without a semi-colon" do
    input = "&vartriangleleft"
    output = [["Character", "&vartriangleleft"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vartriangleright without a semi-colon" do
    input = "&vartriangleright"
    output = [["Character", "&vartriangleright"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vcy without a semi-colon" do
    input = "&vcy"
    output = [["Character", "&vcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vdash without a semi-colon" do
    input = "&vdash"
    output = [["Character", "&vdash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vee without a semi-colon" do
    input = "&vee"
    output = [["Character", "&vee"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: veebar without a semi-colon" do
    input = "&veebar"
    output = [["Character", "&veebar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: veeeq without a semi-colon" do
    input = "&veeeq"
    output = [["Character", "&veeeq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vellip without a semi-colon" do
    input = "&vellip"
    output = [["Character", "&vellip"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: verbar without a semi-colon" do
    input = "&verbar"
    output = [["Character", "&verbar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vert without a semi-colon" do
    input = "&vert"
    output = [["Character", "&vert"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vfr without a semi-colon" do
    input = "&vfr"
    output = [["Character", "&vfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vltri without a semi-colon" do
    input = "&vltri"
    output = [["Character", "&vltri"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vnsub without a semi-colon" do
    input = "&vnsub"
    output = [["Character", "&vnsub"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vnsup without a semi-colon" do
    input = "&vnsup"
    output = [["Character", "&vnsup"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vopf without a semi-colon" do
    input = "&vopf"
    output = [["Character", "&vopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vprop without a semi-colon" do
    input = "&vprop"
    output = [["Character", "&vprop"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vrtri without a semi-colon" do
    input = "&vrtri"
    output = [["Character", "&vrtri"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vscr without a semi-colon" do
    input = "&vscr"
    output = [["Character", "&vscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vsubnE without a semi-colon" do
    input = "&vsubnE"
    output = [["Character", "&vsubnE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vsubne without a semi-colon" do
    input = "&vsubne"
    output = [["Character", "&vsubne"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vsupnE without a semi-colon" do
    input = "&vsupnE"
    output = [["Character", "&vsupnE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vsupne without a semi-colon" do
    input = "&vsupne"
    output = [["Character", "&vsupne"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vzigzag without a semi-colon" do
    input = "&vzigzag"
    output = [["Character", "&vzigzag"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wcirc without a semi-colon" do
    input = "&wcirc"
    output = [["Character", "&wcirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wedbar without a semi-colon" do
    input = "&wedbar"
    output = [["Character", "&wedbar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wedge without a semi-colon" do
    input = "&wedge"
    output = [["Character", "&wedge"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wedgeq without a semi-colon" do
    input = "&wedgeq"
    output = [["Character", "&wedgeq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: weierp without a semi-colon" do
    input = "&weierp"
    output = [["Character", "&weierp"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wfr without a semi-colon" do
    input = "&wfr"
    output = [["Character", "&wfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wopf without a semi-colon" do
    input = "&wopf"
    output = [["Character", "&wopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wp without a semi-colon" do
    input = "&wp"
    output = [["Character", "&wp"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wr without a semi-colon" do
    input = "&wr"
    output = [["Character", "&wr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wreath without a semi-colon" do
    input = "&wreath"
    output = [["Character", "&wreath"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: wscr without a semi-colon" do
    input = "&wscr"
    output = [["Character", "&wscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xcap without a semi-colon" do
    input = "&xcap"
    output = [["Character", "&xcap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xcirc without a semi-colon" do
    input = "&xcirc"
    output = [["Character", "&xcirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xcup without a semi-colon" do
    input = "&xcup"
    output = [["Character", "&xcup"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xdtri without a semi-colon" do
    input = "&xdtri"
    output = [["Character", "&xdtri"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xfr without a semi-colon" do
    input = "&xfr"
    output = [["Character", "&xfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xhArr without a semi-colon" do
    input = "&xhArr"
    output = [["Character", "&xhArr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xharr without a semi-colon" do
    input = "&xharr"
    output = [["Character", "&xharr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xi without a semi-colon" do
    input = "&xi"
    output = [["Character", "&xi"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xlArr without a semi-colon" do
    input = "&xlArr"
    output = [["Character", "&xlArr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xlarr without a semi-colon" do
    input = "&xlarr"
    output = [["Character", "&xlarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xmap without a semi-colon" do
    input = "&xmap"
    output = [["Character", "&xmap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xnis without a semi-colon" do
    input = "&xnis"
    output = [["Character", "&xnis"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xodot without a semi-colon" do
    input = "&xodot"
    output = [["Character", "&xodot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xopf without a semi-colon" do
    input = "&xopf"
    output = [["Character", "&xopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xoplus without a semi-colon" do
    input = "&xoplus"
    output = [["Character", "&xoplus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xotime without a semi-colon" do
    input = "&xotime"
    output = [["Character", "&xotime"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xrArr without a semi-colon" do
    input = "&xrArr"
    output = [["Character", "&xrArr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xrarr without a semi-colon" do
    input = "&xrarr"
    output = [["Character", "&xrarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xscr without a semi-colon" do
    input = "&xscr"
    output = [["Character", "&xscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xsqcup without a semi-colon" do
    input = "&xsqcup"
    output = [["Character", "&xsqcup"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xuplus without a semi-colon" do
    input = "&xuplus"
    output = [["Character", "&xuplus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xutri without a semi-colon" do
    input = "&xutri"
    output = [["Character", "&xutri"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xvee without a semi-colon" do
    input = "&xvee"
    output = [["Character", "&xvee"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: xwedge without a semi-colon" do
    input = "&xwedge"
    output = [["Character", "&xwedge"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: yacy without a semi-colon" do
    input = "&yacy"
    output = [["Character", "&yacy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ycirc without a semi-colon" do
    input = "&ycirc"
    output = [["Character", "&ycirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ycy without a semi-colon" do
    input = "&ycy"
    output = [["Character", "&ycy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: yfr without a semi-colon" do
    input = "&yfr"
    output = [["Character", "&yfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: yicy without a semi-colon" do
    input = "&yicy"
    output = [["Character", "&yicy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: yopf without a semi-colon" do
    input = "&yopf"
    output = [["Character", "&yopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: yscr without a semi-colon" do
    input = "&yscr"
    output = [["Character", "&yscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: yucy without a semi-colon" do
    input = "&yucy"
    output = [["Character", "&yucy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zacute without a semi-colon" do
    input = "&zacute"
    output = [["Character", "&zacute"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zcaron without a semi-colon" do
    input = "&zcaron"
    output = [["Character", "&zcaron"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zcy without a semi-colon" do
    input = "&zcy"
    output = [["Character", "&zcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zdot without a semi-colon" do
    input = "&zdot"
    output = [["Character", "&zdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zeetrf without a semi-colon" do
    input = "&zeetrf"
    output = [["Character", "&zeetrf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zeta without a semi-colon" do
    input = "&zeta"
    output = [["Character", "&zeta"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zfr without a semi-colon" do
    input = "&zfr"
    output = [["Character", "&zfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zhcy without a semi-colon" do
    input = "&zhcy"
    output = [["Character", "&zhcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zigrarr without a semi-colon" do
    input = "&zigrarr"
    output = [["Character", "&zigrarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zopf without a semi-colon" do
    input = "&zopf"
    output = [["Character", "&zopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zscr without a semi-colon" do
    input = "&zscr"
    output = [["Character", "&zscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zwj without a semi-colon" do
    input = "&zwj"
    output = [["Character", "&zwj"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: zwnj without a semi-colon" do
    input = "&zwnj"
    output = [["Character", "&zwnj"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: AElig without a semi-colon" do
    input = "&AElig"
    output = [["Character", "Ã†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: AElig; with a semi-colon" do
    input = "&AElig;"
    output = [["Character", "Ã†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: AMP without a semi-colon" do
    input = "&AMP"
    output = [["Character", "&"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: AMP; with a semi-colon" do
    input = "&AMP;"
    output = [["Character", "&"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Aacute without a semi-colon" do
    input = "&Aacute"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Aacute; with a semi-colon" do
    input = "&Aacute;"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Abreve; with a semi-colon" do
    input = "&Abreve;"
    output = [["Character", "Ä‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Acirc without a semi-colon" do
    input = "&Acirc"
    output = [["Character", "Ã‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Acirc; with a semi-colon" do
    input = "&Acirc;"
    output = [["Character", "Ã‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Acy; with a semi-colon" do
    input = "&Acy;"
    output = [["Character", "Ð"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Afr; with a semi-colon" do
    input = "&Afr;"
    output = [["Character", "ð”„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Agrave without a semi-colon" do
    input = "&Agrave"
    output = [["Character", "Ã€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Agrave; with a semi-colon" do
    input = "&Agrave;"
    output = [["Character", "Ã€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Alpha; with a semi-colon" do
    input = "&Alpha;"
    output = [["Character", "Î‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Amacr; with a semi-colon" do
    input = "&Amacr;"
    output = [["Character", "Ä€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: And; with a semi-colon" do
    input = "&And;"
    output = [["Character", "â©“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Aogon; with a semi-colon" do
    input = "&Aogon;"
    output = [["Character", "Ä„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Aopf; with a semi-colon" do
    input = "&Aopf;"
    output = [["Character", "ð”¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ApplyFunction; with a semi-colon" do
    input = "&ApplyFunction;"
    output = [["Character", "â¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Aring without a semi-colon" do
    input = "&Aring"
    output = [["Character", "Ã…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Aring; with a semi-colon" do
    input = "&Aring;"
    output = [["Character", "Ã…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
