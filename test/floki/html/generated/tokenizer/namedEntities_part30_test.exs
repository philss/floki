defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart30Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: daleth; with a semi-colon" do
    input = "&daleth;"
    output = [["Character", "ℸ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: darr; with a semi-colon" do
    input = "&darr;"
    output = [["Character", "↓"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dash; with a semi-colon" do
    input = "&dash;"
    output = [["Character", "‐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dashv; with a semi-colon" do
    input = "&dashv;"
    output = [["Character", "⊣"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dbkarow; with a semi-colon" do
    input = "&dbkarow;"
    output = [["Character", "⤏"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dblac; with a semi-colon" do
    input = "&dblac;"
    output = [["Character", "˝"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dcaron; with a semi-colon" do
    input = "&dcaron;"
    output = [["Character", "ď"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dcy; with a semi-colon" do
    input = "&dcy;"
    output = [["Character", "д"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dd; with a semi-colon" do
    input = "&dd;"
    output = [["Character", "ⅆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ddagger; with a semi-colon" do
    input = "&ddagger;"
    output = [["Character", "‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ddarr; with a semi-colon" do
    input = "&ddarr;"
    output = [["Character", "⇊"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ddotseq; with a semi-colon" do
    input = "&ddotseq;"
    output = [["Character", "⩷"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: deg without a semi-colon" do
    input = "&deg"
    output = [["Character", "°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: deg; with a semi-colon" do
    input = "&deg;"
    output = [["Character", "°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: delta; with a semi-colon" do
    input = "&delta;"
    output = [["Character", "δ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: demptyv; with a semi-colon" do
    input = "&demptyv;"
    output = [["Character", "⦱"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dfisht; with a semi-colon" do
    input = "&dfisht;"
    output = [["Character", "⥿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dfr; with a semi-colon" do
    input = "&dfr;"
    output = [["Character", "𝔡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dharl; with a semi-colon" do
    input = "&dharl;"
    output = [["Character", "⇃"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dharr; with a semi-colon" do
    input = "&dharr;"
    output = [["Character", "⇂"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: diam; with a semi-colon" do
    input = "&diam;"
    output = [["Character", "⋄"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: diamond; with a semi-colon" do
    input = "&diamond;"
    output = [["Character", "⋄"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: diamondsuit; with a semi-colon" do
    input = "&diamondsuit;"
    output = [["Character", "♦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: diams; with a semi-colon" do
    input = "&diams;"
    output = [["Character", "♦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: die; with a semi-colon" do
    input = "&die;"
    output = [["Character", "¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: digamma; with a semi-colon" do
    input = "&digamma;"
    output = [["Character", "ϝ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: disin; with a semi-colon" do
    input = "&disin;"
    output = [["Character", "⋲"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: div; with a semi-colon" do
    input = "&div;"
    output = [["Character", "÷"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: divide without a semi-colon" do
    input = "&divide"
    output = [["Character", "÷"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: divide; with a semi-colon" do
    input = "&divide;"
    output = [["Character", "÷"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: divideontimes; with a semi-colon" do
    input = "&divideontimes;"
    output = [["Character", "⋇"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: divonx; with a semi-colon" do
    input = "&divonx;"
    output = [["Character", "⋇"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: djcy; with a semi-colon" do
    input = "&djcy;"
    output = [["Character", "ђ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dlcorn; with a semi-colon" do
    input = "&dlcorn;"
    output = [["Character", "⌞"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dlcrop; with a semi-colon" do
    input = "&dlcrop;"
    output = [["Character", "⌍"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dollar; with a semi-colon" do
    input = "&dollar;"
    output = [["Character", "$"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dopf; with a semi-colon" do
    input = "&dopf;"
    output = [["Character", "𝕕"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dot; with a semi-colon" do
    input = "&dot;"
    output = [["Character", "˙"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: doteq; with a semi-colon" do
    input = "&doteq;"
    output = [["Character", "≐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: doteqdot; with a semi-colon" do
    input = "&doteqdot;"
    output = [["Character", "≑"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dotminus; with a semi-colon" do
    input = "&dotminus;"
    output = [["Character", "∸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dotplus; with a semi-colon" do
    input = "&dotplus;"
    output = [["Character", "∔"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dotsquare; with a semi-colon" do
    input = "&dotsquare;"
    output = [["Character", "⊡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: doublebarwedge; with a semi-colon" do
    input = "&doublebarwedge;"
    output = [["Character", "⌆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: downarrow; with a semi-colon" do
    input = "&downarrow;"
    output = [["Character", "↓"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: downdownarrows; with a semi-colon" do
    input = "&downdownarrows;"
    output = [["Character", "⇊"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: downharpoonleft; with a semi-colon" do
    input = "&downharpoonleft;"
    output = [["Character", "⇃"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: downharpoonright; with a semi-colon" do
    input = "&downharpoonright;"
    output = [["Character", "⇂"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: drbkarow; with a semi-colon" do
    input = "&drbkarow;"
    output = [["Character", "⤐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: drcorn; with a semi-colon" do
    input = "&drcorn;"
    output = [["Character", "⌟"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: drcrop; with a semi-colon" do
    input = "&drcrop;"
    output = [["Character", "⌌"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dscr; with a semi-colon" do
    input = "&dscr;"
    output = [["Character", "𝒹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dscy; with a semi-colon" do
    input = "&dscy;"
    output = [["Character", "ѕ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dsol; with a semi-colon" do
    input = "&dsol;"
    output = [["Character", "⧶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dstrok; with a semi-colon" do
    input = "&dstrok;"
    output = [["Character", "đ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dtdot; with a semi-colon" do
    input = "&dtdot;"
    output = [["Character", "⋱"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dtri; with a semi-colon" do
    input = "&dtri;"
    output = [["Character", "▿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dtrif; with a semi-colon" do
    input = "&dtrif;"
    output = [["Character", "▾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: duarr; with a semi-colon" do
    input = "&duarr;"
    output = [["Character", "⇵"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: duhar; with a semi-colon" do
    input = "&duhar;"
    output = [["Character", "⥯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dwangle; with a semi-colon" do
    input = "&dwangle;"
    output = [["Character", "⦦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dzcy; with a semi-colon" do
    input = "&dzcy;"
    output = [["Character", "џ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dzigrarr; with a semi-colon" do
    input = "&dzigrarr;"
    output = [["Character", "⟿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eDDot; with a semi-colon" do
    input = "&eDDot;"
    output = [["Character", "⩷"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eDot; with a semi-colon" do
    input = "&eDot;"
    output = [["Character", "≑"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eacute without a semi-colon" do
    input = "&eacute"
    output = [["Character", "é"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eacute; with a semi-colon" do
    input = "&eacute;"
    output = [["Character", "é"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: easter; with a semi-colon" do
    input = "&easter;"
    output = [["Character", "⩮"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecaron; with a semi-colon" do
    input = "&ecaron;"
    output = [["Character", "ě"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecir; with a semi-colon" do
    input = "&ecir;"
    output = [["Character", "≖"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecirc without a semi-colon" do
    input = "&ecirc"
    output = [["Character", "ê"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecirc; with a semi-colon" do
    input = "&ecirc;"
    output = [["Character", "ê"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecolon; with a semi-colon" do
    input = "&ecolon;"
    output = [["Character", "≕"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecy; with a semi-colon" do
    input = "&ecy;"
    output = [["Character", "э"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: edot; with a semi-colon" do
    input = "&edot;"
    output = [["Character", "ė"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ee; with a semi-colon" do
    input = "&ee;"
    output = [["Character", "ⅇ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: efDot; with a semi-colon" do
    input = "&efDot;"
    output = [["Character", "≒"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: efr; with a semi-colon" do
    input = "&efr;"
    output = [["Character", "𝔢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eg; with a semi-colon" do
    input = "&eg;"
    output = [["Character", "⪚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: egrave without a semi-colon" do
    input = "&egrave"
    output = [["Character", "è"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: egrave; with a semi-colon" do
    input = "&egrave;"
    output = [["Character", "è"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: egs; with a semi-colon" do
    input = "&egs;"
    output = [["Character", "⪖"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: egsdot; with a semi-colon" do
    input = "&egsdot;"
    output = [["Character", "⪘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: el; with a semi-colon" do
    input = "&el;"
    output = [["Character", "⪙"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: elinters; with a semi-colon" do
    input = "&elinters;"
    output = [["Character", "⏧"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ell; with a semi-colon" do
    input = "&ell;"
    output = [["Character", "ℓ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: els; with a semi-colon" do
    input = "&els;"
    output = [["Character", "⪕"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: elsdot; with a semi-colon" do
    input = "&elsdot;"
    output = [["Character", "⪗"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emacr; with a semi-colon" do
    input = "&emacr;"
    output = [["Character", "ē"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: empty; with a semi-colon" do
    input = "&empty;"
    output = [["Character", "∅"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emptyset; with a semi-colon" do
    input = "&emptyset;"
    output = [["Character", "∅"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emptyv; with a semi-colon" do
    input = "&emptyv;"
    output = [["Character", "∅"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emsp13; with a semi-colon" do
    input = "&emsp13;"
    output = [["Character", " "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emsp14; with a semi-colon" do
    input = "&emsp14;"
    output = [["Character", " "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emsp; with a semi-colon" do
    input = "&emsp;"
    output = [["Character", " "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eng; with a semi-colon" do
    input = "&eng;"
    output = [["Character", "ŋ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ensp; with a semi-colon" do
    input = "&ensp;"
    output = [["Character", " "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eogon; with a semi-colon" do
    input = "&eogon;"
    output = [["Character", "ę"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eopf; with a semi-colon" do
    input = "&eopf;"
    output = [["Character", "𝕖"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: epar; with a semi-colon" do
    input = "&epar;"
    output = [["Character", "⋕"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end