defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart30Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: daleth; with a semi-colon" do
    input = "&daleth;"
    output = [["Character", "â„¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: darr; with a semi-colon" do
    input = "&darr;"
    output = [["Character", "â†“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dash; with a semi-colon" do
    input = "&dash;"
    output = [["Character", "â€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dashv; with a semi-colon" do
    input = "&dashv;"
    output = [["Character", "âŠ£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dbkarow; with a semi-colon" do
    input = "&dbkarow;"
    output = [["Character", "â¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dblac; with a semi-colon" do
    input = "&dblac;"
    output = [["Character", "Ë"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dcaron; with a semi-colon" do
    input = "&dcaron;"
    output = [["Character", "Ä"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dcy; with a semi-colon" do
    input = "&dcy;"
    output = [["Character", "Ð´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dd; with a semi-colon" do
    input = "&dd;"
    output = [["Character", "â…†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ddagger; with a semi-colon" do
    input = "&ddagger;"
    output = [["Character", "â€¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ddarr; with a semi-colon" do
    input = "&ddarr;"
    output = [["Character", "â‡Š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ddotseq; with a semi-colon" do
    input = "&ddotseq;"
    output = [["Character", "â©·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: deg without a semi-colon" do
    input = "&deg"
    output = [["Character", "Â°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: deg; with a semi-colon" do
    input = "&deg;"
    output = [["Character", "Â°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: delta; with a semi-colon" do
    input = "&delta;"
    output = [["Character", "Î´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: demptyv; with a semi-colon" do
    input = "&demptyv;"
    output = [["Character", "â¦±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dfisht; with a semi-colon" do
    input = "&dfisht;"
    output = [["Character", "â¥¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dfr; with a semi-colon" do
    input = "&dfr;"
    output = [["Character", "ð”¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dharl; with a semi-colon" do
    input = "&dharl;"
    output = [["Character", "â‡ƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dharr; with a semi-colon" do
    input = "&dharr;"
    output = [["Character", "â‡‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: diam; with a semi-colon" do
    input = "&diam;"
    output = [["Character", "â‹„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: diamond; with a semi-colon" do
    input = "&diamond;"
    output = [["Character", "â‹„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: diamondsuit; with a semi-colon" do
    input = "&diamondsuit;"
    output = [["Character", "â™¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: diams; with a semi-colon" do
    input = "&diams;"
    output = [["Character", "â™¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: die; with a semi-colon" do
    input = "&die;"
    output = [["Character", "Â¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: digamma; with a semi-colon" do
    input = "&digamma;"
    output = [["Character", "Ï"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: disin; with a semi-colon" do
    input = "&disin;"
    output = [["Character", "â‹²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: div; with a semi-colon" do
    input = "&div;"
    output = [["Character", "Ã·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: divide without a semi-colon" do
    input = "&divide"
    output = [["Character", "Ã·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: divide; with a semi-colon" do
    input = "&divide;"
    output = [["Character", "Ã·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: divideontimes; with a semi-colon" do
    input = "&divideontimes;"
    output = [["Character", "â‹‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: divonx; with a semi-colon" do
    input = "&divonx;"
    output = [["Character", "â‹‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: djcy; with a semi-colon" do
    input = "&djcy;"
    output = [["Character", "Ñ’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dlcorn; with a semi-colon" do
    input = "&dlcorn;"
    output = [["Character", "âŒž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dlcrop; with a semi-colon" do
    input = "&dlcrop;"
    output = [["Character", "âŒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dollar; with a semi-colon" do
    input = "&dollar;"
    output = [["Character", "$"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dopf; with a semi-colon" do
    input = "&dopf;"
    output = [["Character", "ð••"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dot; with a semi-colon" do
    input = "&dot;"
    output = [["Character", "Ë™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: doteq; with a semi-colon" do
    input = "&doteq;"
    output = [["Character", "â‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: doteqdot; with a semi-colon" do
    input = "&doteqdot;"
    output = [["Character", "â‰‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dotminus; with a semi-colon" do
    input = "&dotminus;"
    output = [["Character", "âˆ¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dotplus; with a semi-colon" do
    input = "&dotplus;"
    output = [["Character", "âˆ”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dotsquare; with a semi-colon" do
    input = "&dotsquare;"
    output = [["Character", "âŠ¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: doublebarwedge; with a semi-colon" do
    input = "&doublebarwedge;"
    output = [["Character", "âŒ†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: downarrow; with a semi-colon" do
    input = "&downarrow;"
    output = [["Character", "â†“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: downdownarrows; with a semi-colon" do
    input = "&downdownarrows;"
    output = [["Character", "â‡Š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: downharpoonleft; with a semi-colon" do
    input = "&downharpoonleft;"
    output = [["Character", "â‡ƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: downharpoonright; with a semi-colon" do
    input = "&downharpoonright;"
    output = [["Character", "â‡‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: drbkarow; with a semi-colon" do
    input = "&drbkarow;"
    output = [["Character", "â¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: drcorn; with a semi-colon" do
    input = "&drcorn;"
    output = [["Character", "âŒŸ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: drcrop; with a semi-colon" do
    input = "&drcrop;"
    output = [["Character", "âŒŒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dscr; with a semi-colon" do
    input = "&dscr;"
    output = [["Character", "ð’¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dscy; with a semi-colon" do
    input = "&dscy;"
    output = [["Character", "Ñ•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dsol; with a semi-colon" do
    input = "&dsol;"
    output = [["Character", "â§¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dstrok; with a semi-colon" do
    input = "&dstrok;"
    output = [["Character", "Ä‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dtdot; with a semi-colon" do
    input = "&dtdot;"
    output = [["Character", "â‹±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dtri; with a semi-colon" do
    input = "&dtri;"
    output = [["Character", "â–¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dtrif; with a semi-colon" do
    input = "&dtrif;"
    output = [["Character", "â–¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: duarr; with a semi-colon" do
    input = "&duarr;"
    output = [["Character", "â‡µ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: duhar; with a semi-colon" do
    input = "&duhar;"
    output = [["Character", "â¥¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dwangle; with a semi-colon" do
    input = "&dwangle;"
    output = [["Character", "â¦¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dzcy; with a semi-colon" do
    input = "&dzcy;"
    output = [["Character", "ÑŸ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dzigrarr; with a semi-colon" do
    input = "&dzigrarr;"
    output = [["Character", "âŸ¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eDDot; with a semi-colon" do
    input = "&eDDot;"
    output = [["Character", "â©·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eDot; with a semi-colon" do
    input = "&eDot;"
    output = [["Character", "â‰‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eacute without a semi-colon" do
    input = "&eacute"
    output = [["Character", "Ã©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eacute; with a semi-colon" do
    input = "&eacute;"
    output = [["Character", "Ã©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: easter; with a semi-colon" do
    input = "&easter;"
    output = [["Character", "â©®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecaron; with a semi-colon" do
    input = "&ecaron;"
    output = [["Character", "Ä›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecir; with a semi-colon" do
    input = "&ecir;"
    output = [["Character", "â‰–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecirc without a semi-colon" do
    input = "&ecirc"
    output = [["Character", "Ãª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecirc; with a semi-colon" do
    input = "&ecirc;"
    output = [["Character", "Ãª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecolon; with a semi-colon" do
    input = "&ecolon;"
    output = [["Character", "â‰•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ecy; with a semi-colon" do
    input = "&ecy;"
    output = [["Character", "Ñ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: edot; with a semi-colon" do
    input = "&edot;"
    output = [["Character", "Ä—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ee; with a semi-colon" do
    input = "&ee;"
    output = [["Character", "â…‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: efDot; with a semi-colon" do
    input = "&efDot;"
    output = [["Character", "â‰’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: efr; with a semi-colon" do
    input = "&efr;"
    output = [["Character", "ð”¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eg; with a semi-colon" do
    input = "&eg;"
    output = [["Character", "âªš"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: egrave without a semi-colon" do
    input = "&egrave"
    output = [["Character", "Ã¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: egrave; with a semi-colon" do
    input = "&egrave;"
    output = [["Character", "Ã¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: egs; with a semi-colon" do
    input = "&egs;"
    output = [["Character", "âª–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: egsdot; with a semi-colon" do
    input = "&egsdot;"
    output = [["Character", "âª˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: el; with a semi-colon" do
    input = "&el;"
    output = [["Character", "âª™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: elinters; with a semi-colon" do
    input = "&elinters;"
    output = [["Character", "â§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ell; with a semi-colon" do
    input = "&ell;"
    output = [["Character", "â„“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: els; with a semi-colon" do
    input = "&els;"
    output = [["Character", "âª•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: elsdot; with a semi-colon" do
    input = "&elsdot;"
    output = [["Character", "âª—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emacr; with a semi-colon" do
    input = "&emacr;"
    output = [["Character", "Ä“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: empty; with a semi-colon" do
    input = "&empty;"
    output = [["Character", "âˆ…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emptyset; with a semi-colon" do
    input = "&emptyset;"
    output = [["Character", "âˆ…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emptyv; with a semi-colon" do
    input = "&emptyv;"
    output = [["Character", "âˆ…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emsp13; with a semi-colon" do
    input = "&emsp13;"
    output = [["Character", "â€„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emsp14; with a semi-colon" do
    input = "&emsp14;"
    output = [["Character", "â€…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: emsp; with a semi-colon" do
    input = "&emsp;"
    output = [["Character", "â€ƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eng; with a semi-colon" do
    input = "&eng;"
    output = [["Character", "Å‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ensp; with a semi-colon" do
    input = "&ensp;"
    output = [["Character", "â€‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eogon; with a semi-colon" do
    input = "&eogon;"
    output = [["Character", "Ä™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eopf; with a semi-colon" do
    input = "&eopf;"
    output = [["Character", "ð•–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: epar; with a semi-colon" do
    input = "&epar;"
    output = [["Character", "â‹•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end