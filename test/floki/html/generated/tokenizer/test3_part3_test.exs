defmodule Floki.HTML.Generated.Tokenizer.Test3Part3Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests test3.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 <!--a" do
    input = "<!--a"
    output = [["Comment", "a"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!--b" do
    input = "<!--b"
    output = [["Comment", "b"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!--y" do
    input = "<!--y"
    output = [["Comment", "y"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!--z" do
    input = "<!--z"
    output = [["Comment", "z"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!--{" do
    input = "<!--{"
    output = [["Comment", "{"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!/" do
    input = "<!/"
    output = [["Comment", "/"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!0" do
    input = "<!0"
    output = [["Comment", "0"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!1" do
    input = "<!1"
    output = [["Comment", "1"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!9" do
    input = "<!9"
    output = [["Comment", "9"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!<" do
    input = "<!<"
    output = [["Comment", "<"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!=" do
    input = "<!="
    output = [["Comment", "="]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!>" do
    input = "<!>"
    output = [["Comment", ""]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!?" do
    input = "<!?"
    output = [["Comment", "?"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!@" do
    input = "<!@"
    output = [["Comment", "@"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!A" do
    input = "<!A"
    output = [["Comment", "A"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!B" do
    input = "<!B"
    output = [["Comment", "B"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE" do
    input = "<!DOCTYPE"
    output = [["DOCTYPE", nil, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE " do
    input = "<!DOCTYPE "
    output = [["DOCTYPE", nil, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE  " do
    input = "<!DOCTYPE  "
    output = [["DOCTYPE", nil, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE !" do
    input = "<!DOCTYPE !"
    output = [["DOCTYPE", "!", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \"" do
    input = "<!DOCTYPE \""
    output = [["DOCTYPE", "\"", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE &" do
    input = "<!DOCTYPE &"
    output = [["DOCTYPE", "&", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE '" do
    input = "<!DOCTYPE '"
    output = [["DOCTYPE", "'", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE -" do
    input = "<!DOCTYPE -"
    output = [["DOCTYPE", "-", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE /" do
    input = "<!DOCTYPE /"
    output = [["DOCTYPE", "/", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE 0" do
    input = "<!DOCTYPE 0"
    output = [["DOCTYPE", "0", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE 1" do
    input = "<!DOCTYPE 1"
    output = [["DOCTYPE", "1", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE 9" do
    input = "<!DOCTYPE 9"
    output = [["DOCTYPE", "9", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE <" do
    input = "<!DOCTYPE <"
    output = [["DOCTYPE", "<", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE =" do
    input = "<!DOCTYPE ="
    output = [["DOCTYPE", "=", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE >" do
    input = "<!DOCTYPE >"
    output = [["DOCTYPE", nil, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE ?" do
    input = "<!DOCTYPE ?"
    output = [["DOCTYPE", "?", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE @" do
    input = "<!DOCTYPE @"
    output = [["DOCTYPE", "@", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE A" do
    input = "<!DOCTYPE A"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE B" do
    input = "<!DOCTYPE B"
    output = [["DOCTYPE", "b", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE Y" do
    input = "<!DOCTYPE Y"
    output = [["DOCTYPE", "y", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE Z" do
    input = "<!DOCTYPE Z"
    output = [["DOCTYPE", "z", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE [" do
    input = "<!DOCTYPE ["
    output = [["DOCTYPE", "[", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \\u0000" do
    input = <<60, 33, 68, 79, 67, 84, 89, 80, 69, 32, 0>>
    output = [["DOCTYPE", "�", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \\u0008" do
    input = "<!DOCTYPE \b"
    output = [["DOCTYPE", "\b", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \\u0009" do
    input = "<!DOCTYPE \t"
    output = [["DOCTYPE", nil, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \\u000A" do
    input = "<!DOCTYPE \n"
    output = [["DOCTYPE", nil, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \\u000B" do
    input = "<!DOCTYPE \v"
    output = [["DOCTYPE", "\v", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \\u000C" do
    input = "<!DOCTYPE \f"
    output = [["DOCTYPE", nil, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \\u000D" do
    input = "<!DOCTYPE \r"
    output = [["DOCTYPE", nil, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \\u001F" do
    input = <<60, 33, 68, 79, 67, 84, 89, 80, 69, 32, 31>>
    output = [["DOCTYPE", <<31>>, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE \\uDBC0\\uDC00" do
    input = "<!DOCTYPE 􀀀"
    output = [["DOCTYPE", "􀀀", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE `" do
    input = "<!DOCTYPE `"
    output = [["DOCTYPE", "`", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a" do
    input = "<!DOCTYPE a"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a " do
    input = "<!DOCTYPE a "
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a  " do
    input = "<!DOCTYPE a  "
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a !" do
    input = "<!DOCTYPE a !"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a \"" do
    input = "<!DOCTYPE a \""
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a &" do
    input = "<!DOCTYPE a &"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a '" do
    input = "<!DOCTYPE a '"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a -" do
    input = "<!DOCTYPE a -"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a /" do
    input = "<!DOCTYPE a /"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a 0" do
    input = "<!DOCTYPE a 0"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a 1" do
    input = "<!DOCTYPE a 1"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a 9" do
    input = "<!DOCTYPE a 9"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a <" do
    input = "<!DOCTYPE a <"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a =" do
    input = "<!DOCTYPE a ="
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a >" do
    input = "<!DOCTYPE a >"
    output = [["DOCTYPE", "a", nil, nil, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a ?" do
    input = "<!DOCTYPE a ?"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a @" do
    input = "<!DOCTYPE a @"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a A" do
    input = "<!DOCTYPE a A"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a B" do
    input = "<!DOCTYPE a B"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC" do
    input = "<!DOCTYPE a PUBLIC"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC " do
    input = "<!DOCTYPE a PUBLIC "
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC!" do
    input = "<!DOCTYPE a PUBLIC!"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"" do
    input = "<!DOCTYPE a PUBLIC\""
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\" " do
    input = "<!DOCTYPE a PUBLIC\" "
    output = [["DOCTYPE", "a", " ", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"!" do
    input = "<!DOCTYPE a PUBLIC\"!"
    output = [["DOCTYPE", "a", "!", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"\"" do
    input = "<!DOCTYPE a PUBLIC\"\""
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"\" \\u0000" do
    input =
      <<60, 33, 68, 79, 67, 84, 89, 80, 69, 32, 97, 32, 80, 85, 66, 76, 73, 67, 34, 34, 32, 0>>

    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"\"\\u0000" do
    input = <<60, 33, 68, 79, 67, 84, 89, 80, 69, 32, 97, 32, 80, 85, 66, 76, 73, 67, 34, 34, 0>>
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"#" do
    input = "<!DOCTYPE a PUBLIC\"#"
    output = [["DOCTYPE", "a", "#", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"&" do
    input = "<!DOCTYPE a PUBLIC\"&"
    output = [["DOCTYPE", "a", "&", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"'" do
    input = "<!DOCTYPE a PUBLIC\"'"
    output = [["DOCTYPE", "a", "'", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"-" do
    input = "<!DOCTYPE a PUBLIC\"-"
    output = [["DOCTYPE", "a", "-", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"/" do
    input = "<!DOCTYPE a PUBLIC\"/"
    output = [["DOCTYPE", "a", "/", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"0" do
    input = "<!DOCTYPE a PUBLIC\"0"
    output = [["DOCTYPE", "a", "0", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"1" do
    input = "<!DOCTYPE a PUBLIC\"1"
    output = [["DOCTYPE", "a", "1", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"9" do
    input = "<!DOCTYPE a PUBLIC\"9"
    output = [["DOCTYPE", "a", "9", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"<" do
    input = "<!DOCTYPE a PUBLIC\"<"
    output = [["DOCTYPE", "a", "<", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"=" do
    input = "<!DOCTYPE a PUBLIC\"="
    output = [["DOCTYPE", "a", "=", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\">" do
    input = "<!DOCTYPE a PUBLIC\">"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"?" do
    input = "<!DOCTYPE a PUBLIC\"?"
    output = [["DOCTYPE", "a", "?", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"@" do
    input = "<!DOCTYPE a PUBLIC\"@"
    output = [["DOCTYPE", "a", "@", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"A" do
    input = "<!DOCTYPE a PUBLIC\"A"
    output = [["DOCTYPE", "a", "A", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"B" do
    input = "<!DOCTYPE a PUBLIC\"B"
    output = [["DOCTYPE", "a", "B", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"Y" do
    input = "<!DOCTYPE a PUBLIC\"Y"
    output = [["DOCTYPE", "a", "Y", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"Z" do
    input = "<!DOCTYPE a PUBLIC\"Z"
    output = [["DOCTYPE", "a", "Z", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"\\u0000" do
    input = <<60, 33, 68, 79, 67, 84, 89, 80, 69, 32, 97, 32, 80, 85, 66, 76, 73, 67, 34, 0>>
    output = [["DOCTYPE", "a", "�", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"\\u0009" do
    input = "<!DOCTYPE a PUBLIC\"\t"
    output = [["DOCTYPE", "a", "\t", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"\\u000A" do
    input = "<!DOCTYPE a PUBLIC\"\n"
    output = [["DOCTYPE", "a", "\n", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"\\u000B" do
    input = "<!DOCTYPE a PUBLIC\"\v"
    output = [["DOCTYPE", "a", "\v", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"\\u000C" do
    input = "<!DOCTYPE a PUBLIC\"\f"
    output = [["DOCTYPE", "a", "\f", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"\\uDBC0\\uDC00" do
    input = "<!DOCTYPE a PUBLIC\"􀀀"
    output = [["DOCTYPE", "a", "􀀀", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPE a PUBLIC\"`" do
    input = "<!DOCTYPE a PUBLIC\"`"
    output = [["DOCTYPE", "a", "`", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end