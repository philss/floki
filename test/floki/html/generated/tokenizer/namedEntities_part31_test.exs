defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart31Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: eparsl; with a semi-colon" do
    input = "&eparsl;"
    output = [["Character", "â§£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eplus; with a semi-colon" do
    input = "&eplus;"
    output = [["Character", "â©±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: epsi; with a semi-colon" do
    input = "&epsi;"
    output = [["Character", "Îµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: epsilon; with a semi-colon" do
    input = "&epsilon;"
    output = [["Character", "Îµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: epsiv; with a semi-colon" do
    input = "&epsiv;"
    output = [["Character", "Ïµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eqcirc; with a semi-colon" do
    input = "&eqcirc;"
    output = [["Character", "â‰–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eqcolon; with a semi-colon" do
    input = "&eqcolon;"
    output = [["Character", "â‰•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eqsim; with a semi-colon" do
    input = "&eqsim;"
    output = [["Character", "â‰‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eqslantgtr; with a semi-colon" do
    input = "&eqslantgtr;"
    output = [["Character", "âª–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eqslantless; with a semi-colon" do
    input = "&eqslantless;"
    output = [["Character", "âª•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: equals; with a semi-colon" do
    input = "&equals;"
    output = [["Character", "="]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: equest; with a semi-colon" do
    input = "&equest;"
    output = [["Character", "â‰Ÿ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: equiv; with a semi-colon" do
    input = "&equiv;"
    output = [["Character", "â‰¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: equivDD; with a semi-colon" do
    input = "&equivDD;"
    output = [["Character", "â©¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eqvparsl; with a semi-colon" do
    input = "&eqvparsl;"
    output = [["Character", "â§¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: erDot; with a semi-colon" do
    input = "&erDot;"
    output = [["Character", "â‰“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: erarr; with a semi-colon" do
    input = "&erarr;"
    output = [["Character", "â¥±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: escr; with a semi-colon" do
    input = "&escr;"
    output = [["Character", "â„¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: esdot; with a semi-colon" do
    input = "&esdot;"
    output = [["Character", "â‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: esim; with a semi-colon" do
    input = "&esim;"
    output = [["Character", "â‰‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eta; with a semi-colon" do
    input = "&eta;"
    output = [["Character", "Î·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eth without a semi-colon" do
    input = "&eth"
    output = [["Character", "Ã°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: eth; with a semi-colon" do
    input = "&eth;"
    output = [["Character", "Ã°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: euml without a semi-colon" do
    input = "&euml"
    output = [["Character", "Ã«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: euml; with a semi-colon" do
    input = "&euml;"
    output = [["Character", "Ã«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: euro; with a semi-colon" do
    input = "&euro;"
    output = [["Character", "â‚¬"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: excl; with a semi-colon" do
    input = "&excl;"
    output = [["Character", "!"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: exist; with a semi-colon" do
    input = "&exist;"
    output = [["Character", "âˆƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: expectation; with a semi-colon" do
    input = "&expectation;"
    output = [["Character", "â„°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: exponentiale; with a semi-colon" do
    input = "&exponentiale;"
    output = [["Character", "â…‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fallingdotseq; with a semi-colon" do
    input = "&fallingdotseq;"
    output = [["Character", "â‰’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fcy; with a semi-colon" do
    input = "&fcy;"
    output = [["Character", "Ñ„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: female; with a semi-colon" do
    input = "&female;"
    output = [["Character", "â™€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ffilig; with a semi-colon" do
    input = "&ffilig;"
    output = [["Character", "ï¬ƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fflig; with a semi-colon" do
    input = "&fflig;"
    output = [["Character", "ï¬€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ffllig; with a semi-colon" do
    input = "&ffllig;"
    output = [["Character", "ï¬„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ffr; with a semi-colon" do
    input = "&ffr;"
    output = [["Character", "ð”£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: filig; with a semi-colon" do
    input = "&filig;"
    output = [["Character", "ï¬"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fjlig; with a semi-colon" do
    input = "&fjlig;"
    output = [["Character", "fj"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: flat; with a semi-colon" do
    input = "&flat;"
    output = [["Character", "â™­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fllig; with a semi-colon" do
    input = "&fllig;"
    output = [["Character", "ï¬‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fltns; with a semi-colon" do
    input = "&fltns;"
    output = [["Character", "â–±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fnof; with a semi-colon" do
    input = "&fnof;"
    output = [["Character", "Æ’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fopf; with a semi-colon" do
    input = "&fopf;"
    output = [["Character", "ð•—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: forall; with a semi-colon" do
    input = "&forall;"
    output = [["Character", "âˆ€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fork; with a semi-colon" do
    input = "&fork;"
    output = [["Character", "â‹”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: forkv; with a semi-colon" do
    input = "&forkv;"
    output = [["Character", "â«™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fpartint; with a semi-colon" do
    input = "&fpartint;"
    output = [["Character", "â¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac12 without a semi-colon" do
    input = "&frac12"
    output = [["Character", "Â½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac12; with a semi-colon" do
    input = "&frac12;"
    output = [["Character", "Â½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac13; with a semi-colon" do
    input = "&frac13;"
    output = [["Character", "â…“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac14 without a semi-colon" do
    input = "&frac14"
    output = [["Character", "Â¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac14; with a semi-colon" do
    input = "&frac14;"
    output = [["Character", "Â¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac15; with a semi-colon" do
    input = "&frac15;"
    output = [["Character", "â…•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac16; with a semi-colon" do
    input = "&frac16;"
    output = [["Character", "â…™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac18; with a semi-colon" do
    input = "&frac18;"
    output = [["Character", "â…›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac23; with a semi-colon" do
    input = "&frac23;"
    output = [["Character", "â…”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac25; with a semi-colon" do
    input = "&frac25;"
    output = [["Character", "â…–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac34 without a semi-colon" do
    input = "&frac34"
    output = [["Character", "Â¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac34; with a semi-colon" do
    input = "&frac34;"
    output = [["Character", "Â¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac35; with a semi-colon" do
    input = "&frac35;"
    output = [["Character", "â…—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac38; with a semi-colon" do
    input = "&frac38;"
    output = [["Character", "â…œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac45; with a semi-colon" do
    input = "&frac45;"
    output = [["Character", "â…˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac56; with a semi-colon" do
    input = "&frac56;"
    output = [["Character", "â…š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac58; with a semi-colon" do
    input = "&frac58;"
    output = [["Character", "â…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frac78; with a semi-colon" do
    input = "&frac78;"
    output = [["Character", "â…ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frasl; with a semi-colon" do
    input = "&frasl;"
    output = [["Character", "â„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: frown; with a semi-colon" do
    input = "&frown;"
    output = [["Character", "âŒ¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: fscr; with a semi-colon" do
    input = "&fscr;"
    output = [["Character", "ð’»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gE; with a semi-colon" do
    input = "&gE;"
    output = [["Character", "â‰§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gEl; with a semi-colon" do
    input = "&gEl;"
    output = [["Character", "âªŒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gacute; with a semi-colon" do
    input = "&gacute;"
    output = [["Character", "Çµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gamma; with a semi-colon" do
    input = "&gamma;"
    output = [["Character", "Î³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gammad; with a semi-colon" do
    input = "&gammad;"
    output = [["Character", "Ï"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gap; with a semi-colon" do
    input = "&gap;"
    output = [["Character", "âª†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gbreve; with a semi-colon" do
    input = "&gbreve;"
    output = [["Character", "ÄŸ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gcirc; with a semi-colon" do
    input = "&gcirc;"
    output = [["Character", "Ä"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gcy; with a semi-colon" do
    input = "&gcy;"
    output = [["Character", "Ð³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gdot; with a semi-colon" do
    input = "&gdot;"
    output = [["Character", "Ä¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ge; with a semi-colon" do
    input = "&ge;"
    output = [["Character", "â‰¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gel; with a semi-colon" do
    input = "&gel;"
    output = [["Character", "â‹›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: geq; with a semi-colon" do
    input = "&geq;"
    output = [["Character", "â‰¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: geqq; with a semi-colon" do
    input = "&geqq;"
    output = [["Character", "â‰§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: geqslant; with a semi-colon" do
    input = "&geqslant;"
    output = [["Character", "â©¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ges; with a semi-colon" do
    input = "&ges;"
    output = [["Character", "â©¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gescc; with a semi-colon" do
    input = "&gescc;"
    output = [["Character", "âª©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gesdot; with a semi-colon" do
    input = "&gesdot;"
    output = [["Character", "âª€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gesdoto; with a semi-colon" do
    input = "&gesdoto;"
    output = [["Character", "âª‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gesdotol; with a semi-colon" do
    input = "&gesdotol;"
    output = [["Character", "âª„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gesl; with a semi-colon" do
    input = "&gesl;"
    output = [["Character", "â‹›ï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gesles; with a semi-colon" do
    input = "&gesles;"
    output = [["Character", "âª”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gfr; with a semi-colon" do
    input = "&gfr;"
    output = [["Character", "ð”¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gg; with a semi-colon" do
    input = "&gg;"
    output = [["Character", "â‰«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ggg; with a semi-colon" do
    input = "&ggg;"
    output = [["Character", "â‹™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gimel; with a semi-colon" do
    input = "&gimel;"
    output = [["Character", "â„·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gjcy; with a semi-colon" do
    input = "&gjcy;"
    output = [["Character", "Ñ“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gl; with a semi-colon" do
    input = "&gl;"
    output = [["Character", "â‰·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: glE; with a semi-colon" do
    input = "&glE;"
    output = [["Character", "âª’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: gla; with a semi-colon" do
    input = "&gla;"
    output = [["Character", "âª¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: glj; with a semi-colon" do
    input = "&glj;"
    output = [["Character", "âª¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
