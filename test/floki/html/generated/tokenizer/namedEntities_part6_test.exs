defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart6Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Bad named entity: Uopf without a semi-colon" do
    input = "&Uopf"
    output = [["Character", "&Uopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: UpArrow without a semi-colon" do
    input = "&UpArrow"
    output = [["Character", "&UpArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: UpArrowBar without a semi-colon" do
    input = "&UpArrowBar"
    output = [["Character", "&UpArrowBar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: UpArrowDownArrow without a semi-colon" do
    input = "&UpArrowDownArrow"
    output = [["Character", "&UpArrowDownArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: UpDownArrow without a semi-colon" do
    input = "&UpDownArrow"
    output = [["Character", "&UpDownArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: UpEquilibrium without a semi-colon" do
    input = "&UpEquilibrium"
    output = [["Character", "&UpEquilibrium"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: UpTee without a semi-colon" do
    input = "&UpTee"
    output = [["Character", "&UpTee"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: UpTeeArrow without a semi-colon" do
    input = "&UpTeeArrow"
    output = [["Character", "&UpTeeArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Uparrow without a semi-colon" do
    input = "&Uparrow"
    output = [["Character", "&Uparrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Updownarrow without a semi-colon" do
    input = "&Updownarrow"
    output = [["Character", "&Updownarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: UpperLeftArrow without a semi-colon" do
    input = "&UpperLeftArrow"
    output = [["Character", "&UpperLeftArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: UpperRightArrow without a semi-colon" do
    input = "&UpperRightArrow"
    output = [["Character", "&UpperRightArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Upsi without a semi-colon" do
    input = "&Upsi"
    output = [["Character", "&Upsi"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Upsilon without a semi-colon" do
    input = "&Upsilon"
    output = [["Character", "&Upsilon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Uring without a semi-colon" do
    input = "&Uring"
    output = [["Character", "&Uring"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Uscr without a semi-colon" do
    input = "&Uscr"
    output = [["Character", "&Uscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Utilde without a semi-colon" do
    input = "&Utilde"
    output = [["Character", "&Utilde"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: VDash without a semi-colon" do
    input = "&VDash"
    output = [["Character", "&VDash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vbar without a semi-colon" do
    input = "&Vbar"
    output = [["Character", "&Vbar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vcy without a semi-colon" do
    input = "&Vcy"
    output = [["Character", "&Vcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vdash without a semi-colon" do
    input = "&Vdash"
    output = [["Character", "&Vdash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vdashl without a semi-colon" do
    input = "&Vdashl"
    output = [["Character", "&Vdashl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vee without a semi-colon" do
    input = "&Vee"
    output = [["Character", "&Vee"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Verbar without a semi-colon" do
    input = "&Verbar"
    output = [["Character", "&Verbar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vert without a semi-colon" do
    input = "&Vert"
    output = [["Character", "&Vert"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: VerticalBar without a semi-colon" do
    input = "&VerticalBar"
    output = [["Character", "&VerticalBar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: VerticalLine without a semi-colon" do
    input = "&VerticalLine"
    output = [["Character", "&VerticalLine"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: VerticalSeparator without a semi-colon" do
    input = "&VerticalSeparator"
    output = [["Character", "&VerticalSeparator"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: VerticalTilde without a semi-colon" do
    input = "&VerticalTilde"
    output = [["Character", "&VerticalTilde"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: VeryThinSpace without a semi-colon" do
    input = "&VeryThinSpace"
    output = [["Character", "&VeryThinSpace"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vfr without a semi-colon" do
    input = "&Vfr"
    output = [["Character", "&Vfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vopf without a semi-colon" do
    input = "&Vopf"
    output = [["Character", "&Vopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vscr without a semi-colon" do
    input = "&Vscr"
    output = [["Character", "&Vscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Vvdash without a semi-colon" do
    input = "&Vvdash"
    output = [["Character", "&Vvdash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Wcirc without a semi-colon" do
    input = "&Wcirc"
    output = [["Character", "&Wcirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Wedge without a semi-colon" do
    input = "&Wedge"
    output = [["Character", "&Wedge"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Wfr without a semi-colon" do
    input = "&Wfr"
    output = [["Character", "&Wfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Wopf without a semi-colon" do
    input = "&Wopf"
    output = [["Character", "&Wopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Wscr without a semi-colon" do
    input = "&Wscr"
    output = [["Character", "&Wscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Xfr without a semi-colon" do
    input = "&Xfr"
    output = [["Character", "&Xfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Xi without a semi-colon" do
    input = "&Xi"
    output = [["Character", "&Xi"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Xopf without a semi-colon" do
    input = "&Xopf"
    output = [["Character", "&Xopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Xscr without a semi-colon" do
    input = "&Xscr"
    output = [["Character", "&Xscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: YAcy without a semi-colon" do
    input = "&YAcy"
    output = [["Character", "&YAcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: YIcy without a semi-colon" do
    input = "&YIcy"
    output = [["Character", "&YIcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: YUcy without a semi-colon" do
    input = "&YUcy"
    output = [["Character", "&YUcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Ycirc without a semi-colon" do
    input = "&Ycirc"
    output = [["Character", "&Ycirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Ycy without a semi-colon" do
    input = "&Ycy"
    output = [["Character", "&Ycy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Yfr without a semi-colon" do
    input = "&Yfr"
    output = [["Character", "&Yfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Yopf without a semi-colon" do
    input = "&Yopf"
    output = [["Character", "&Yopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Yscr without a semi-colon" do
    input = "&Yscr"
    output = [["Character", "&Yscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Yuml without a semi-colon" do
    input = "&Yuml"
    output = [["Character", "&Yuml"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ZHcy without a semi-colon" do
    input = "&ZHcy"
    output = [["Character", "&ZHcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Zacute without a semi-colon" do
    input = "&Zacute"
    output = [["Character", "&Zacute"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Zcaron without a semi-colon" do
    input = "&Zcaron"
    output = [["Character", "&Zcaron"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Zcy without a semi-colon" do
    input = "&Zcy"
    output = [["Character", "&Zcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Zdot without a semi-colon" do
    input = "&Zdot"
    output = [["Character", "&Zdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ZeroWidthSpace without a semi-colon" do
    input = "&ZeroWidthSpace"
    output = [["Character", "&ZeroWidthSpace"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Zeta without a semi-colon" do
    input = "&Zeta"
    output = [["Character", "&Zeta"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Zfr without a semi-colon" do
    input = "&Zfr"
    output = [["Character", "&Zfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Zopf without a semi-colon" do
    input = "&Zopf"
    output = [["Character", "&Zopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Zscr without a semi-colon" do
    input = "&Zscr"
    output = [["Character", "&Zscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: abreve without a semi-colon" do
    input = "&abreve"
    output = [["Character", "&abreve"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ac without a semi-colon" do
    input = "&ac"
    output = [["Character", "&ac"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: acE without a semi-colon" do
    input = "&acE"
    output = [["Character", "&acE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: acd without a semi-colon" do
    input = "&acd"
    output = [["Character", "&acd"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: acy without a semi-colon" do
    input = "&acy"
    output = [["Character", "&acy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: af without a semi-colon" do
    input = "&af"
    output = [["Character", "&af"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: afr without a semi-colon" do
    input = "&afr"
    output = [["Character", "&afr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: alefsym without a semi-colon" do
    input = "&alefsym"
    output = [["Character", "&alefsym"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: aleph without a semi-colon" do
    input = "&aleph"
    output = [["Character", "&aleph"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: alpha without a semi-colon" do
    input = "&alpha"
    output = [["Character", "&alpha"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: amacr without a semi-colon" do
    input = "&amacr"
    output = [["Character", "&amacr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: amalg without a semi-colon" do
    input = "&amalg"
    output = [["Character", "&amalg"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: and without a semi-colon" do
    input = "&and"
    output = [["Character", "&and"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: andand without a semi-colon" do
    input = "&andand"
    output = [["Character", "&andand"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: andd without a semi-colon" do
    input = "&andd"
    output = [["Character", "&andd"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: andslope without a semi-colon" do
    input = "&andslope"
    output = [["Character", "&andslope"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: andv without a semi-colon" do
    input = "&andv"
    output = [["Character", "&andv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ang without a semi-colon" do
    input = "&ang"
    output = [["Character", "&ang"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ange without a semi-colon" do
    input = "&ange"
    output = [["Character", "&ange"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angle without a semi-colon" do
    input = "&angle"
    output = [["Character", "&angle"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angmsd without a semi-colon" do
    input = "&angmsd"
    output = [["Character", "&angmsd"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angmsdaa without a semi-colon" do
    input = "&angmsdaa"
    output = [["Character", "&angmsdaa"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angmsdab without a semi-colon" do
    input = "&angmsdab"
    output = [["Character", "&angmsdab"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angmsdac without a semi-colon" do
    input = "&angmsdac"
    output = [["Character", "&angmsdac"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angmsdad without a semi-colon" do
    input = "&angmsdad"
    output = [["Character", "&angmsdad"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angmsdae without a semi-colon" do
    input = "&angmsdae"
    output = [["Character", "&angmsdae"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angmsdaf without a semi-colon" do
    input = "&angmsdaf"
    output = [["Character", "&angmsdaf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angmsdag without a semi-colon" do
    input = "&angmsdag"
    output = [["Character", "&angmsdag"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angmsdah without a semi-colon" do
    input = "&angmsdah"
    output = [["Character", "&angmsdah"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angrt without a semi-colon" do
    input = "&angrt"
    output = [["Character", "&angrt"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angrtvb without a semi-colon" do
    input = "&angrtvb"
    output = [["Character", "&angrtvb"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angrtvbd without a semi-colon" do
    input = "&angrtvbd"
    output = [["Character", "&angrtvbd"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angsph without a semi-colon" do
    input = "&angsph"
    output = [["Character", "&angsph"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angst without a semi-colon" do
    input = "&angst"
    output = [["Character", "&angst"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: angzarr without a semi-colon" do
    input = "&angzarr"
    output = [["Character", "&angzarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: aogon without a semi-colon" do
    input = "&aogon"
    output = [["Character", "&aogon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: aopf without a semi-colon" do
    input = "&aopf"
    output = [["Character", "&aopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ap without a semi-colon" do
    input = "&ap"
    output = [["Character", "&ap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
