defmodule Floki.HTML.Generated.Tokenizer.Test3Part15Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests test3.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 <a a>" do
    input = "<a a>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a?>" do
    input = "<a a?>"
    output = [["StartTag", "a", %{"a?" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a@>" do
    input = "<a a@>"
    output = [["StartTag", "a", %{"a@" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a aA>" do
    input = "<a aA>"
    output = [["StartTag", "a", %{"aa" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a aB>" do
    input = "<a aB>"
    output = [["StartTag", "a", %{"ab" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a aY>" do
    input = "<a aY>"
    output = [["StartTag", "a", %{"ay" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a aZ>" do
    input = "<a aZ>"
    output = [["StartTag", "a", %{"az" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a[>" do
    input = "<a a[>"
    output = [["StartTag", "a", %{"a[" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\\u0000>" do
    input = <<60, 97, 32, 97, 0, 62>>
    output = [["StartTag", "a", %{"a�" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\\u0008>" do
    input = "<a a\b>"
    output = [["StartTag", "a", %{"a\b" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\\u0009>" do
    input = "<a a\t>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\\u000A>" do
    input = "<a a\n>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\\u000B>" do
    input = "<a a\v>"
    output = [["StartTag", "a", %{"a\v" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\\u000C>" do
    input = "<a a\f>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\\u000D>" do
    input = "<a a\r>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\\u001F>" do
    input = <<60, 97, 32, 97, 31, 62>>
    output = [["StartTag", "a", %{<<97, 31>> => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\\uDBC0\\uDC00>" do
    input = "<a a􀀀>"
    output = [["StartTag", "a", %{"a􀀀" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a`>" do
    input = "<a a`>"
    output = [["StartTag", "a", %{"a`" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a aa>" do
    input = "<a aa>"
    output = [["StartTag", "a", %{"aa" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a ab>" do
    input = "<a ab>"
    output = [["StartTag", "a", %{"ab" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a ay>" do
    input = "<a ay>"
    output = [["StartTag", "a", %{"ay" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a az>" do
    input = "<a az>"
    output = [["StartTag", "a", %{"az" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a{>" do
    input = "<a a{>"
    output = [["StartTag", "a", %{"a{" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a b>" do
    input = "<a b>"
    output = [["StartTag", "a", %{"b" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a y>" do
    input = "<a y>"
    output = [["StartTag", "a", %{"y" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a z>" do
    input = "<a z>"
    output = [["StartTag", "a", %{"z" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a {>" do
    input = "<a {>"
    output = [["StartTag", "a", %{"{" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a!>" do
    input = "<a!>"
    output = [["StartTag", "a!", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\">" do
    input = "<a\">"
    output = [["StartTag", "a\"", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a&>" do
    input = "<a&>"
    output = [["StartTag", "a&", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a'>" do
    input = "<a'>"
    output = [["StartTag", "a'", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a->" do
    input = "<a->"
    output = [["StartTag", "a-", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a.>" do
    input = "<a.>"
    output = [["StartTag", "a.", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/ >" do
    input = "<a/ >"
    output = [["StartTag", "a", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/!>" do
    input = "<a/!>"
    output = [["StartTag", "a", %{"!" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/\">" do
    input = "<a/\">"
    output = [["StartTag", "a", %{"\"" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/&>" do
    input = "<a/&>"
    output = [["StartTag", "a", %{"&" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/'>" do
    input = "<a/'>"
    output = [["StartTag", "a", %{"'" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/->" do
    input = "<a/->"
    output = [["StartTag", "a", %{"-" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a//>" do
    input = "<a//>"
    output = [["StartTag", "a", %{}, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/0>" do
    input = "<a/0>"
    output = [["StartTag", "a", %{"0" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/1>" do
    input = "<a/1>"
    output = [["StartTag", "a", %{"1" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/9>" do
    input = "<a/9>"
    output = [["StartTag", "a", %{"9" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/<>" do
    input = "<a/<>"
    output = [["StartTag", "a", %{"<" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/=>" do
    input = "<a/=>"
    output = [["StartTag", "a", %{"=" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/>" do
    input = "<a/>"
    output = [["StartTag", "a", %{}, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/?>" do
    input = "<a/?>"
    output = [["StartTag", "a", %{"?" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/@>" do
    input = "<a/@>"
    output = [["StartTag", "a", %{"@" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/A>" do
    input = "<a/A>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/B>" do
    input = "<a/B>"
    output = [["StartTag", "a", %{"b" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/Y>" do
    input = "<a/Y>"
    output = [["StartTag", "a", %{"y" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/Z>" do
    input = "<a/Z>"
    output = [["StartTag", "a", %{"z" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/\\u0000>" do
    input = <<60, 97, 47, 0, 62>>
    output = [["StartTag", "a", %{"�" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/\\u0009>" do
    input = "<a/\t>"
    output = [["StartTag", "a", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/\\u000A>" do
    input = "<a/\n>"
    output = [["StartTag", "a", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/\\u000B>" do
    input = "<a/\v>"
    output = [["StartTag", "a", %{"\v" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/\\u000C>" do
    input = "<a/\f>"
    output = [["StartTag", "a", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/\\uDBC0\\uDC00>" do
    input = "<a/􀀀>"
    output = [["StartTag", "a", %{"􀀀" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/`>" do
    input = "<a/`>"
    output = [["StartTag", "a", %{"`" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/a>" do
    input = "<a/a>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/b>" do
    input = "<a/b>"
    output = [["StartTag", "a", %{"b" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/y>" do
    input = "<a/y>"
    output = [["StartTag", "a", %{"y" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/z>" do
    input = "<a/z>"
    output = [["StartTag", "a", %{"z" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a/{>" do
    input = "<a/{>"
    output = [["StartTag", "a", %{"{" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a0>" do
    input = "<a0>"
    output = [["StartTag", "a0", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a1>" do
    input = "<a1>"
    output = [["StartTag", "a1", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a9>" do
    input = "<a9>"
    output = [["StartTag", "a9", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a<>" do
    input = "<a<>"
    output = [["StartTag", "a<", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a=>" do
    input = "<a=>"
    output = [["StartTag", "a=", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a>" do
    input = "<a>"
    output = [["StartTag", "a", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a?>" do
    input = "<a?>"
    output = [["StartTag", "a?", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a@>" do
    input = "<a@>"
    output = [["StartTag", "a@", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <aA>" do
    input = "<aA>"
    output = [["StartTag", "aa", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <aB>" do
    input = "<aB>"
    output = [["StartTag", "ab", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <aY>" do
    input = "<aY>"
    output = [["StartTag", "ay", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <aZ>" do
    input = "<aZ>"
    output = [["StartTag", "az", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a[>" do
    input = "<a[>"
    output = [["StartTag", "a[", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\\u0000>" do
    input = <<60, 97, 0, 62>>
    output = [["StartTag", "a�", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\\u0008>" do
    input = "<a\b>"
    output = [["StartTag", "a\b", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\\u0009>" do
    input = "<a\t>"
    output = [["StartTag", "a", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\\u000A>" do
    input = "<a\n>"
    output = [["StartTag", "a", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\\u000B>" do
    input = "<a\v>"
    output = [["StartTag", "a\v", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\\u000C>" do
    input = "<a\f>"
    output = [["StartTag", "a", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\\u000D>" do
    input = "<a\r>"
    output = [["StartTag", "a", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\\u001F>" do
    input = <<60, 97, 31, 62>>
    output = [["StartTag", <<97, 31>>, %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a\\uDBC0\\uDC00>" do
    input = "<a􀀀>"
    output = [["StartTag", "a􀀀", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a`>" do
    input = "<a`>"
    output = [["StartTag", "a`", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <aa>" do
    input = "<aa>"
    output = [["StartTag", "aa", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <ab>" do
    input = "<ab>"
    output = [["StartTag", "ab", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <ay>" do
    input = "<ay>"
    output = [["StartTag", "ay", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <az>" do
    input = "<az>"
    output = [["StartTag", "az", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a{>" do
    input = "<a{>"
    output = [["StartTag", "a{", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <b>" do
    input = "<b>"
    output = [["StartTag", "b", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <y>" do
    input = "<y>"
    output = [["StartTag", "y", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <z>" do
    input = "<z>"
    output = [["StartTag", "z", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <{" do
    input = "<{"
    output = [["Character", "<{"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 =" do
    input = "="
    output = [["Character", "="]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 >" do
    input = ">"
    output = [["Character", ">"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 ?" do
    input = "?"
    output = [["Character", "?"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 @" do
    input = "@"
    output = [["Character", "@"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end