defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart11Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Bad named entity: gneq without a semi-colon" do
    input = "&gneq"
    output = [["Character", "&gneq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gneqq without a semi-colon" do
    input = "&gneqq"
    output = [["Character", "&gneqq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gnsim without a semi-colon" do
    input = "&gnsim"
    output = [["Character", "&gnsim"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gopf without a semi-colon" do
    input = "&gopf"
    output = [["Character", "&gopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: grave without a semi-colon" do
    input = "&grave"
    output = [["Character", "&grave"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gscr without a semi-colon" do
    input = "&gscr"
    output = [["Character", "&gscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gsim without a semi-colon" do
    input = "&gsim"
    output = [["Character", "&gsim"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gsime without a semi-colon" do
    input = "&gsime"
    output = [["Character", "&gsime"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gsiml without a semi-colon" do
    input = "&gsiml"
    output = [["Character", "&gsiml"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gvertneqq without a semi-colon" do
    input = "&gvertneqq"
    output = [["Character", "&gvertneqq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gvnE without a semi-colon" do
    input = "&gvnE"
    output = [["Character", "&gvnE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hArr without a semi-colon" do
    input = "&hArr"
    output = [["Character", "&hArr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hairsp without a semi-colon" do
    input = "&hairsp"
    output = [["Character", "&hairsp"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: half without a semi-colon" do
    input = "&half"
    output = [["Character", "&half"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hamilt without a semi-colon" do
    input = "&hamilt"
    output = [["Character", "&hamilt"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hardcy without a semi-colon" do
    input = "&hardcy"
    output = [["Character", "&hardcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: harr without a semi-colon" do
    input = "&harr"
    output = [["Character", "&harr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: harrcir without a semi-colon" do
    input = "&harrcir"
    output = [["Character", "&harrcir"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: harrw without a semi-colon" do
    input = "&harrw"
    output = [["Character", "&harrw"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hbar without a semi-colon" do
    input = "&hbar"
    output = [["Character", "&hbar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hcirc without a semi-colon" do
    input = "&hcirc"
    output = [["Character", "&hcirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hearts without a semi-colon" do
    input = "&hearts"
    output = [["Character", "&hearts"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: heartsuit without a semi-colon" do
    input = "&heartsuit"
    output = [["Character", "&heartsuit"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hellip without a semi-colon" do
    input = "&hellip"
    output = [["Character", "&hellip"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hercon without a semi-colon" do
    input = "&hercon"
    output = [["Character", "&hercon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hfr without a semi-colon" do
    input = "&hfr"
    output = [["Character", "&hfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hksearow without a semi-colon" do
    input = "&hksearow"
    output = [["Character", "&hksearow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hkswarow without a semi-colon" do
    input = "&hkswarow"
    output = [["Character", "&hkswarow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hoarr without a semi-colon" do
    input = "&hoarr"
    output = [["Character", "&hoarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: homtht without a semi-colon" do
    input = "&homtht"
    output = [["Character", "&homtht"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hookleftarrow without a semi-colon" do
    input = "&hookleftarrow"
    output = [["Character", "&hookleftarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hookrightarrow without a semi-colon" do
    input = "&hookrightarrow"
    output = [["Character", "&hookrightarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hopf without a semi-colon" do
    input = "&hopf"
    output = [["Character", "&hopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: horbar without a semi-colon" do
    input = "&horbar"
    output = [["Character", "&horbar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hscr without a semi-colon" do
    input = "&hscr"
    output = [["Character", "&hscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hslash without a semi-colon" do
    input = "&hslash"
    output = [["Character", "&hslash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hstrok without a semi-colon" do
    input = "&hstrok"
    output = [["Character", "&hstrok"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hybull without a semi-colon" do
    input = "&hybull"
    output = [["Character", "&hybull"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: hyphen without a semi-colon" do
    input = "&hyphen"
    output = [["Character", "&hyphen"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ic without a semi-colon" do
    input = "&ic"
    output = [["Character", "&ic"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: icy without a semi-colon" do
    input = "&icy"
    output = [["Character", "&icy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iecy without a semi-colon" do
    input = "&iecy"
    output = [["Character", "&iecy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iff without a semi-colon" do
    input = "&iff"
    output = [["Character", "&iff"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ifr without a semi-colon" do
    input = "&ifr"
    output = [["Character", "&ifr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ii without a semi-colon" do
    input = "&ii"
    output = [["Character", "&ii"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iiiint without a semi-colon" do
    input = "&iiiint"
    output = [["Character", "&iiiint"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iiint without a semi-colon" do
    input = "&iiint"
    output = [["Character", "&iiint"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iinfin without a semi-colon" do
    input = "&iinfin"
    output = [["Character", "&iinfin"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iiota without a semi-colon" do
    input = "&iiota"
    output = [["Character", "&iiota"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ijlig without a semi-colon" do
    input = "&ijlig"
    output = [["Character", "&ijlig"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: imacr without a semi-colon" do
    input = "&imacr"
    output = [["Character", "&imacr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: image without a semi-colon" do
    input = "&image"
    output = [["Character", "&image"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: imagline without a semi-colon" do
    input = "&imagline"
    output = [["Character", "&imagline"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: imagpart without a semi-colon" do
    input = "&imagpart"
    output = [["Character", "&imagpart"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: imath without a semi-colon" do
    input = "&imath"
    output = [["Character", "&imath"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: imof without a semi-colon" do
    input = "&imof"
    output = [["Character", "&imof"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: imped without a semi-colon" do
    input = "&imped"
    output = [["Character", "&imped"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: in without a semi-colon" do
    input = "&in"
    output = [["Character", "&in"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: incare without a semi-colon" do
    input = "&incare"
    output = [["Character", "&incare"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: infin without a semi-colon" do
    input = "&infin"
    output = [["Character", "&infin"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: infintie without a semi-colon" do
    input = "&infintie"
    output = [["Character", "&infintie"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: inodot without a semi-colon" do
    input = "&inodot"
    output = [["Character", "&inodot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: int without a semi-colon" do
    input = "&int"
    output = [["Character", "&int"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: intcal without a semi-colon" do
    input = "&intcal"
    output = [["Character", "&intcal"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: integers without a semi-colon" do
    input = "&integers"
    output = [["Character", "&integers"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: intercal without a semi-colon" do
    input = "&intercal"
    output = [["Character", "&intercal"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: intlarhk without a semi-colon" do
    input = "&intlarhk"
    output = [["Character", "&intlarhk"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: intprod without a semi-colon" do
    input = "&intprod"
    output = [["Character", "&intprod"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iocy without a semi-colon" do
    input = "&iocy"
    output = [["Character", "&iocy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iogon without a semi-colon" do
    input = "&iogon"
    output = [["Character", "&iogon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iopf without a semi-colon" do
    input = "&iopf"
    output = [["Character", "&iopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iota without a semi-colon" do
    input = "&iota"
    output = [["Character", "&iota"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iprod without a semi-colon" do
    input = "&iprod"
    output = [["Character", "&iprod"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iscr without a semi-colon" do
    input = "&iscr"
    output = [["Character", "&iscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: isin without a semi-colon" do
    input = "&isin"
    output = [["Character", "&isin"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: isinE without a semi-colon" do
    input = "&isinE"
    output = [["Character", "&isinE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: isindot without a semi-colon" do
    input = "&isindot"
    output = [["Character", "&isindot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: isins without a semi-colon" do
    input = "&isins"
    output = [["Character", "&isins"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: isinsv without a semi-colon" do
    input = "&isinsv"
    output = [["Character", "&isinsv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: isinv without a semi-colon" do
    input = "&isinv"
    output = [["Character", "&isinv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: it without a semi-colon" do
    input = "&it"
    output = [["Character", "&it"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: itilde without a semi-colon" do
    input = "&itilde"
    output = [["Character", "&itilde"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: iukcy without a semi-colon" do
    input = "&iukcy"
    output = [["Character", "&iukcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: jcirc without a semi-colon" do
    input = "&jcirc"
    output = [["Character", "&jcirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: jcy without a semi-colon" do
    input = "&jcy"
    output = [["Character", "&jcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: jfr without a semi-colon" do
    input = "&jfr"
    output = [["Character", "&jfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: jmath without a semi-colon" do
    input = "&jmath"
    output = [["Character", "&jmath"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: jopf without a semi-colon" do
    input = "&jopf"
    output = [["Character", "&jopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: jscr without a semi-colon" do
    input = "&jscr"
    output = [["Character", "&jscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: jsercy without a semi-colon" do
    input = "&jsercy"
    output = [["Character", "&jsercy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: jukcy without a semi-colon" do
    input = "&jukcy"
    output = [["Character", "&jukcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: kappa without a semi-colon" do
    input = "&kappa"
    output = [["Character", "&kappa"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: kappav without a semi-colon" do
    input = "&kappav"
    output = [["Character", "&kappav"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: kcedil without a semi-colon" do
    input = "&kcedil"
    output = [["Character", "&kcedil"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: kcy without a semi-colon" do
    input = "&kcy"
    output = [["Character", "&kcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: kfr without a semi-colon" do
    input = "&kfr"
    output = [["Character", "&kfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: kgreen without a semi-colon" do
    input = "&kgreen"
    output = [["Character", "&kgreen"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: khcy without a semi-colon" do
    input = "&khcy"
    output = [["Character", "&khcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: kjcy without a semi-colon" do
    input = "&kjcy"
    output = [["Character", "&kjcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: kopf without a semi-colon" do
    input = "&kopf"
    output = [["Character", "&kopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end