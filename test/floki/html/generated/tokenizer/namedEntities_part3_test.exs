defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart3Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Bad named entity: Kcedil without a semi-colon" do
    input = "&Kcedil"
    output = [["Character", "&Kcedil"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Kcy without a semi-colon" do
    input = "&Kcy"
    output = [["Character", "&Kcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Kfr without a semi-colon" do
    input = "&Kfr"
    output = [["Character", "&Kfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Kopf without a semi-colon" do
    input = "&Kopf"
    output = [["Character", "&Kopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Kscr without a semi-colon" do
    input = "&Kscr"
    output = [["Character", "&Kscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LJcy without a semi-colon" do
    input = "&LJcy"
    output = [["Character", "&LJcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lacute without a semi-colon" do
    input = "&Lacute"
    output = [["Character", "&Lacute"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lambda without a semi-colon" do
    input = "&Lambda"
    output = [["Character", "&Lambda"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lang without a semi-colon" do
    input = "&Lang"
    output = [["Character", "&Lang"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Laplacetrf without a semi-colon" do
    input = "&Laplacetrf"
    output = [["Character", "&Laplacetrf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Larr without a semi-colon" do
    input = "&Larr"
    output = [["Character", "&Larr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lcaron without a semi-colon" do
    input = "&Lcaron"
    output = [["Character", "&Lcaron"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lcedil without a semi-colon" do
    input = "&Lcedil"
    output = [["Character", "&Lcedil"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lcy without a semi-colon" do
    input = "&Lcy"
    output = [["Character", "&Lcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftAngleBracket without a semi-colon" do
    input = "&LeftAngleBracket"
    output = [["Character", "&LeftAngleBracket"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftArrow without a semi-colon" do
    input = "&LeftArrow"
    output = [["Character", "&LeftArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftArrowBar without a semi-colon" do
    input = "&LeftArrowBar"
    output = [["Character", "&LeftArrowBar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftArrowRightArrow without a semi-colon" do
    input = "&LeftArrowRightArrow"
    output = [["Character", "&LeftArrowRightArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftCeiling without a semi-colon" do
    input = "&LeftCeiling"
    output = [["Character", "&LeftCeiling"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftDoubleBracket without a semi-colon" do
    input = "&LeftDoubleBracket"
    output = [["Character", "&LeftDoubleBracket"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftDownTeeVector without a semi-colon" do
    input = "&LeftDownTeeVector"
    output = [["Character", "&LeftDownTeeVector"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftDownVector without a semi-colon" do
    input = "&LeftDownVector"
    output = [["Character", "&LeftDownVector"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftDownVectorBar without a semi-colon" do
    input = "&LeftDownVectorBar"
    output = [["Character", "&LeftDownVectorBar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftFloor without a semi-colon" do
    input = "&LeftFloor"
    output = [["Character", "&LeftFloor"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftRightArrow without a semi-colon" do
    input = "&LeftRightArrow"
    output = [["Character", "&LeftRightArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftRightVector without a semi-colon" do
    input = "&LeftRightVector"
    output = [["Character", "&LeftRightVector"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftTee without a semi-colon" do
    input = "&LeftTee"
    output = [["Character", "&LeftTee"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftTeeArrow without a semi-colon" do
    input = "&LeftTeeArrow"
    output = [["Character", "&LeftTeeArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftTeeVector without a semi-colon" do
    input = "&LeftTeeVector"
    output = [["Character", "&LeftTeeVector"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftTriangle without a semi-colon" do
    input = "&LeftTriangle"
    output = [["Character", "&LeftTriangle"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftTriangleBar without a semi-colon" do
    input = "&LeftTriangleBar"
    output = [["Character", "&LeftTriangleBar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftTriangleEqual without a semi-colon" do
    input = "&LeftTriangleEqual"
    output = [["Character", "&LeftTriangleEqual"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftUpDownVector without a semi-colon" do
    input = "&LeftUpDownVector"
    output = [["Character", "&LeftUpDownVector"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftUpTeeVector without a semi-colon" do
    input = "&LeftUpTeeVector"
    output = [["Character", "&LeftUpTeeVector"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftUpVector without a semi-colon" do
    input = "&LeftUpVector"
    output = [["Character", "&LeftUpVector"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftUpVectorBar without a semi-colon" do
    input = "&LeftUpVectorBar"
    output = [["Character", "&LeftUpVectorBar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftVector without a semi-colon" do
    input = "&LeftVector"
    output = [["Character", "&LeftVector"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LeftVectorBar without a semi-colon" do
    input = "&LeftVectorBar"
    output = [["Character", "&LeftVectorBar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Leftarrow without a semi-colon" do
    input = "&Leftarrow"
    output = [["Character", "&Leftarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Leftrightarrow without a semi-colon" do
    input = "&Leftrightarrow"
    output = [["Character", "&Leftrightarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LessEqualGreater without a semi-colon" do
    input = "&LessEqualGreater"
    output = [["Character", "&LessEqualGreater"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LessFullEqual without a semi-colon" do
    input = "&LessFullEqual"
    output = [["Character", "&LessFullEqual"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LessGreater without a semi-colon" do
    input = "&LessGreater"
    output = [["Character", "&LessGreater"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LessLess without a semi-colon" do
    input = "&LessLess"
    output = [["Character", "&LessLess"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LessSlantEqual without a semi-colon" do
    input = "&LessSlantEqual"
    output = [["Character", "&LessSlantEqual"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LessTilde without a semi-colon" do
    input = "&LessTilde"
    output = [["Character", "&LessTilde"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lfr without a semi-colon" do
    input = "&Lfr"
    output = [["Character", "&Lfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Ll without a semi-colon" do
    input = "&Ll"
    output = [["Character", "&Ll"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lleftarrow without a semi-colon" do
    input = "&Lleftarrow"
    output = [["Character", "&Lleftarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lmidot without a semi-colon" do
    input = "&Lmidot"
    output = [["Character", "&Lmidot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LongLeftArrow without a semi-colon" do
    input = "&LongLeftArrow"
    output = [["Character", "&LongLeftArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LongLeftRightArrow without a semi-colon" do
    input = "&LongLeftRightArrow"
    output = [["Character", "&LongLeftRightArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LongRightArrow without a semi-colon" do
    input = "&LongRightArrow"
    output = [["Character", "&LongRightArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Longleftarrow without a semi-colon" do
    input = "&Longleftarrow"
    output = [["Character", "&Longleftarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Longleftrightarrow without a semi-colon" do
    input = "&Longleftrightarrow"
    output = [["Character", "&Longleftrightarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Longrightarrow without a semi-colon" do
    input = "&Longrightarrow"
    output = [["Character", "&Longrightarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lopf without a semi-colon" do
    input = "&Lopf"
    output = [["Character", "&Lopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LowerLeftArrow without a semi-colon" do
    input = "&LowerLeftArrow"
    output = [["Character", "&LowerLeftArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: LowerRightArrow without a semi-colon" do
    input = "&LowerRightArrow"
    output = [["Character", "&LowerRightArrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lscr without a semi-colon" do
    input = "&Lscr"
    output = [["Character", "&Lscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lsh without a semi-colon" do
    input = "&Lsh"
    output = [["Character", "&Lsh"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lstrok without a semi-colon" do
    input = "&Lstrok"
    output = [["Character", "&Lstrok"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Lt without a semi-colon" do
    input = "&Lt"
    output = [["Character", "&Lt"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Map without a semi-colon" do
    input = "&Map"
    output = [["Character", "&Map"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Mcy without a semi-colon" do
    input = "&Mcy"
    output = [["Character", "&Mcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: MediumSpace without a semi-colon" do
    input = "&MediumSpace"
    output = [["Character", "&MediumSpace"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Mellintrf without a semi-colon" do
    input = "&Mellintrf"
    output = [["Character", "&Mellintrf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Mfr without a semi-colon" do
    input = "&Mfr"
    output = [["Character", "&Mfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: MinusPlus without a semi-colon" do
    input = "&MinusPlus"
    output = [["Character", "&MinusPlus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Mopf without a semi-colon" do
    input = "&Mopf"
    output = [["Character", "&Mopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Mscr without a semi-colon" do
    input = "&Mscr"
    output = [["Character", "&Mscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Mu without a semi-colon" do
    input = "&Mu"
    output = [["Character", "&Mu"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NJcy without a semi-colon" do
    input = "&NJcy"
    output = [["Character", "&NJcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Nacute without a semi-colon" do
    input = "&Nacute"
    output = [["Character", "&Nacute"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Ncaron without a semi-colon" do
    input = "&Ncaron"
    output = [["Character", "&Ncaron"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Ncedil without a semi-colon" do
    input = "&Ncedil"
    output = [["Character", "&Ncedil"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Ncy without a semi-colon" do
    input = "&Ncy"
    output = [["Character", "&Ncy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NegativeMediumSpace without a semi-colon" do
    input = "&NegativeMediumSpace"
    output = [["Character", "&NegativeMediumSpace"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NegativeThickSpace without a semi-colon" do
    input = "&NegativeThickSpace"
    output = [["Character", "&NegativeThickSpace"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NegativeThinSpace without a semi-colon" do
    input = "&NegativeThinSpace"
    output = [["Character", "&NegativeThinSpace"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NegativeVeryThinSpace without a semi-colon" do
    input = "&NegativeVeryThinSpace"
    output = [["Character", "&NegativeVeryThinSpace"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NestedGreaterGreater without a semi-colon" do
    input = "&NestedGreaterGreater"
    output = [["Character", "&NestedGreaterGreater"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NestedLessLess without a semi-colon" do
    input = "&NestedLessLess"
    output = [["Character", "&NestedLessLess"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NewLine without a semi-colon" do
    input = "&NewLine"
    output = [["Character", "&NewLine"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Nfr without a semi-colon" do
    input = "&Nfr"
    output = [["Character", "&Nfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NoBreak without a semi-colon" do
    input = "&NoBreak"
    output = [["Character", "&NoBreak"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NonBreakingSpace without a semi-colon" do
    input = "&NonBreakingSpace"
    output = [["Character", "&NonBreakingSpace"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Nopf without a semi-colon" do
    input = "&Nopf"
    output = [["Character", "&Nopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: Not without a semi-colon" do
    input = "&Not"
    output = [["Character", "&Not"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotCongruent without a semi-colon" do
    input = "&NotCongruent"
    output = [["Character", "&NotCongruent"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotCupCap without a semi-colon" do
    input = "&NotCupCap"
    output = [["Character", "&NotCupCap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotDoubleVerticalBar without a semi-colon" do
    input = "&NotDoubleVerticalBar"
    output = [["Character", "&NotDoubleVerticalBar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotElement without a semi-colon" do
    input = "&NotElement"
    output = [["Character", "&NotElement"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotEqual without a semi-colon" do
    input = "&NotEqual"
    output = [["Character", "&NotEqual"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotEqualTilde without a semi-colon" do
    input = "&NotEqualTilde"
    output = [["Character", "&NotEqualTilde"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotExists without a semi-colon" do
    input = "&NotExists"
    output = [["Character", "&NotExists"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotGreater without a semi-colon" do
    input = "&NotGreater"
    output = [["Character", "&NotGreater"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotGreaterEqual without a semi-colon" do
    input = "&NotGreaterEqual"
    output = [["Character", "&NotGreaterEqual"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotGreaterFullEqual without a semi-colon" do
    input = "&NotGreaterFullEqual"
    output = [["Character", "&NotGreaterFullEqual"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: NotGreaterGreater without a semi-colon" do
    input = "&NotGreaterGreater"
    output = [["Character", "&NotGreaterGreater"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
