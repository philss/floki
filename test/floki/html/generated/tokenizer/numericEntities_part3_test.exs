defmodule Floki.HTML.Generated.Tokenizer.NumericentitiesPart3Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests numericEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Valid numeric entity character U+00A3" do
    input = "&#x00a3;"
    output = [["Character", "Â£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A4" do
    input = "&#x00a4;"
    output = [["Character", "Â¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A5" do
    input = "&#x00a5;"
    output = [["Character", "Â¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A6" do
    input = "&#x00a6;"
    output = [["Character", "Â¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A7" do
    input = "&#x00a7;"
    output = [["Character", "Â§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A8" do
    input = "&#x00a8;"
    output = [["Character", "Â¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A9" do
    input = "&#x00a9;"
    output = [["Character", "Â©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AA" do
    input = "&#x00aa;"
    output = [["Character", "Âª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AB" do
    input = "&#x00ab;"
    output = [["Character", "Â«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AC" do
    input = "&#x00ac;"
    output = [["Character", "Â¬"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AD" do
    input = "&#x00ad;"
    output = [["Character", "Â­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AE" do
    input = "&#x00ae;"
    output = [["Character", "Â®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AF" do
    input = "&#x00af;"
    output = [["Character", "Â¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B0" do
    input = "&#x00b0;"
    output = [["Character", "Â°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B1" do
    input = "&#x00b1;"
    output = [["Character", "Â±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B2" do
    input = "&#x00b2;"
    output = [["Character", "Â²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B3" do
    input = "&#x00b3;"
    output = [["Character", "Â³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B4" do
    input = "&#x00b4;"
    output = [["Character", "Â´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B5" do
    input = "&#x00b5;"
    output = [["Character", "Âµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B6" do
    input = "&#x00b6;"
    output = [["Character", "Â¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B7" do
    input = "&#x00b7;"
    output = [["Character", "Â·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B8" do
    input = "&#x00b8;"
    output = [["Character", "Â¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B9" do
    input = "&#x00b9;"
    output = [["Character", "Â¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BA" do
    input = "&#x00ba;"
    output = [["Character", "Âº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BB" do
    input = "&#x00bb;"
    output = [["Character", "Â»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BC" do
    input = "&#x00bc;"
    output = [["Character", "Â¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BD" do
    input = "&#x00bd;"
    output = [["Character", "Â½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BE" do
    input = "&#x00be;"
    output = [["Character", "Â¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BF" do
    input = "&#x00bf;"
    output = [["Character", "Â¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C0" do
    input = "&#x00c0;"
    output = [["Character", "Ã€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C1" do
    input = "&#x00c1;"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C2" do
    input = "&#x00c2;"
    output = [["Character", "Ã‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C3" do
    input = "&#x00c3;"
    output = [["Character", "Ãƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C4" do
    input = "&#x00c4;"
    output = [["Character", "Ã„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C5" do
    input = "&#x00c5;"
    output = [["Character", "Ã…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C6" do
    input = "&#x00c6;"
    output = [["Character", "Ã†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C7" do
    input = "&#x00c7;"
    output = [["Character", "Ã‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C8" do
    input = "&#x00c8;"
    output = [["Character", "Ãˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C9" do
    input = "&#x00c9;"
    output = [["Character", "Ã‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CA" do
    input = "&#x00ca;"
    output = [["Character", "ÃŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CB" do
    input = "&#x00cb;"
    output = [["Character", "Ã‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CC" do
    input = "&#x00cc;"
    output = [["Character", "ÃŒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CD" do
    input = "&#x00cd;"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CE" do
    input = "&#x00ce;"
    output = [["Character", "ÃŽ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CF" do
    input = "&#x00cf;"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D0" do
    input = "&#x00d0;"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D1" do
    input = "&#x00d1;"
    output = [["Character", "Ã‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D2" do
    input = "&#x00d2;"
    output = [["Character", "Ã’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D3" do
    input = "&#x00d3;"
    output = [["Character", "Ã“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D4" do
    input = "&#x00d4;"
    output = [["Character", "Ã”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D5" do
    input = "&#x00d5;"
    output = [["Character", "Ã•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D6" do
    input = "&#x00d6;"
    output = [["Character", "Ã–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D7" do
    input = "&#x00d7;"
    output = [["Character", "Ã—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D8" do
    input = "&#x00d8;"
    output = [["Character", "Ã˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D9" do
    input = "&#x00d9;"
    output = [["Character", "Ã™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DA" do
    input = "&#x00da;"
    output = [["Character", "Ãš"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DB" do
    input = "&#x00db;"
    output = [["Character", "Ã›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DC" do
    input = "&#x00dc;"
    output = [["Character", "Ãœ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DD" do
    input = "&#x00dd;"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DE" do
    input = "&#x00de;"
    output = [["Character", "Ãž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DF" do
    input = "&#x00df;"
    output = [["Character", "ÃŸ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E0" do
    input = "&#x00e0;"
    output = [["Character", "Ã "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E1" do
    input = "&#x00e1;"
    output = [["Character", "Ã¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E2" do
    input = "&#x00e2;"
    output = [["Character", "Ã¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E3" do
    input = "&#x00e3;"
    output = [["Character", "Ã£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E4" do
    input = "&#x00e4;"
    output = [["Character", "Ã¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E5" do
    input = "&#x00e5;"
    output = [["Character", "Ã¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E6" do
    input = "&#x00e6;"
    output = [["Character", "Ã¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E7" do
    input = "&#x00e7;"
    output = [["Character", "Ã§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E8" do
    input = "&#x00e8;"
    output = [["Character", "Ã¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E9" do
    input = "&#x00e9;"
    output = [["Character", "Ã©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EA" do
    input = "&#x00ea;"
    output = [["Character", "Ãª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EB" do
    input = "&#x00eb;"
    output = [["Character", "Ã«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EC" do
    input = "&#x00ec;"
    output = [["Character", "Ã¬"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00ED" do
    input = "&#x00ed;"
    output = [["Character", "Ã­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EE" do
    input = "&#x00ee;"
    output = [["Character", "Ã®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EF" do
    input = "&#x00ef;"
    output = [["Character", "Ã¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F0" do
    input = "&#x00f0;"
    output = [["Character", "Ã°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F1" do
    input = "&#x00f1;"
    output = [["Character", "Ã±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F2" do
    input = "&#x00f2;"
    output = [["Character", "Ã²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F3" do
    input = "&#x00f3;"
    output = [["Character", "Ã³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F4" do
    input = "&#x00f4;"
    output = [["Character", "Ã´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F5" do
    input = "&#x00f5;"
    output = [["Character", "Ãµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F6" do
    input = "&#x00f6;"
    output = [["Character", "Ã¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F7" do
    input = "&#x00f7;"
    output = [["Character", "Ã·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F8" do
    input = "&#x00f8;"
    output = [["Character", "Ã¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F9" do
    input = "&#x00f9;"
    output = [["Character", "Ã¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FA" do
    input = "&#x00fa;"
    output = [["Character", "Ãº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FB" do
    input = "&#x00fb;"
    output = [["Character", "Ã»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FC" do
    input = "&#x00fc;"
    output = [["Character", "Ã¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FD" do
    input = "&#x00fd;"
    output = [["Character", "Ã½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FE" do
    input = "&#x00fe;"
    output = [["Character", "Ã¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FF" do
    input = "&#x00ff;"
    output = [["Character", "Ã¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+10000" do
    input = "&#x10000;"
    output = [["Character", "ð€€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+100000" do
    input = "&#x100000;"
    output = [["Character", "ô€€€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+10FFFD" do
    input = "&#x10fffd;"
    output = [["Character", "ô¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+1FFFD" do
    input = "&#x1fffd;"
    output = [["Character", "ðŸ¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+20000" do
    input = "&#x20000;"
    output = [["Character", "ð €€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+2FFFD" do
    input = "&#x2fffd;"
    output = [["Character", "ð¯¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+30000" do
    input = "&#x30000;"
    output = [["Character", "ð°€€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end