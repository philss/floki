defmodule Floki.HTML.Generated.Tokenizer.NumericentitiesPart3Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests numericEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Valid numeric entity character U+00A3" do
    input = "&#x00a3;"
    output = [["Character", "£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A4" do
    input = "&#x00a4;"
    output = [["Character", "¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A5" do
    input = "&#x00a5;"
    output = [["Character", "¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A6" do
    input = "&#x00a6;"
    output = [["Character", "¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A7" do
    input = "&#x00a7;"
    output = [["Character", "§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A8" do
    input = "&#x00a8;"
    output = [["Character", "¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00A9" do
    input = "&#x00a9;"
    output = [["Character", "©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AA" do
    input = "&#x00aa;"
    output = [["Character", "ª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AB" do
    input = "&#x00ab;"
    output = [["Character", "«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AC" do
    input = "&#x00ac;"
    output = [["Character", "¬"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AD" do
    input = "&#x00ad;"
    output = [["Character", "­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AE" do
    input = "&#x00ae;"
    output = [["Character", "®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00AF" do
    input = "&#x00af;"
    output = [["Character", "¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B0" do
    input = "&#x00b0;"
    output = [["Character", "°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B1" do
    input = "&#x00b1;"
    output = [["Character", "±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B2" do
    input = "&#x00b2;"
    output = [["Character", "²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B3" do
    input = "&#x00b3;"
    output = [["Character", "³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B4" do
    input = "&#x00b4;"
    output = [["Character", "´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B5" do
    input = "&#x00b5;"
    output = [["Character", "µ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B6" do
    input = "&#x00b6;"
    output = [["Character", "¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B7" do
    input = "&#x00b7;"
    output = [["Character", "·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B8" do
    input = "&#x00b8;"
    output = [["Character", "¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00B9" do
    input = "&#x00b9;"
    output = [["Character", "¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BA" do
    input = "&#x00ba;"
    output = [["Character", "º"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BB" do
    input = "&#x00bb;"
    output = [["Character", "»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BC" do
    input = "&#x00bc;"
    output = [["Character", "¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BD" do
    input = "&#x00bd;"
    output = [["Character", "½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BE" do
    input = "&#x00be;"
    output = [["Character", "¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00BF" do
    input = "&#x00bf;"
    output = [["Character", "¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C0" do
    input = "&#x00c0;"
    output = [["Character", "À"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C1" do
    input = "&#x00c1;"
    output = [["Character", "Á"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C2" do
    input = "&#x00c2;"
    output = [["Character", "Â"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C3" do
    input = "&#x00c3;"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C4" do
    input = "&#x00c4;"
    output = [["Character", "Ä"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C5" do
    input = "&#x00c5;"
    output = [["Character", "Å"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C6" do
    input = "&#x00c6;"
    output = [["Character", "Æ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C7" do
    input = "&#x00c7;"
    output = [["Character", "Ç"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C8" do
    input = "&#x00c8;"
    output = [["Character", "È"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00C9" do
    input = "&#x00c9;"
    output = [["Character", "É"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CA" do
    input = "&#x00ca;"
    output = [["Character", "Ê"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CB" do
    input = "&#x00cb;"
    output = [["Character", "Ë"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CC" do
    input = "&#x00cc;"
    output = [["Character", "Ì"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CD" do
    input = "&#x00cd;"
    output = [["Character", "Í"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CE" do
    input = "&#x00ce;"
    output = [["Character", "Î"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00CF" do
    input = "&#x00cf;"
    output = [["Character", "Ï"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D0" do
    input = "&#x00d0;"
    output = [["Character", "Ð"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D1" do
    input = "&#x00d1;"
    output = [["Character", "Ñ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D2" do
    input = "&#x00d2;"
    output = [["Character", "Ò"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D3" do
    input = "&#x00d3;"
    output = [["Character", "Ó"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D4" do
    input = "&#x00d4;"
    output = [["Character", "Ô"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D5" do
    input = "&#x00d5;"
    output = [["Character", "Õ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D6" do
    input = "&#x00d6;"
    output = [["Character", "Ö"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D7" do
    input = "&#x00d7;"
    output = [["Character", "×"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D8" do
    input = "&#x00d8;"
    output = [["Character", "Ø"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00D9" do
    input = "&#x00d9;"
    output = [["Character", "Ù"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DA" do
    input = "&#x00da;"
    output = [["Character", "Ú"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DB" do
    input = "&#x00db;"
    output = [["Character", "Û"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DC" do
    input = "&#x00dc;"
    output = [["Character", "Ü"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DD" do
    input = "&#x00dd;"
    output = [["Character", "Ý"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DE" do
    input = "&#x00de;"
    output = [["Character", "Þ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00DF" do
    input = "&#x00df;"
    output = [["Character", "ß"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E0" do
    input = "&#x00e0;"
    output = [["Character", "à"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E1" do
    input = "&#x00e1;"
    output = [["Character", "á"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E2" do
    input = "&#x00e2;"
    output = [["Character", "â"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E3" do
    input = "&#x00e3;"
    output = [["Character", "ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E4" do
    input = "&#x00e4;"
    output = [["Character", "ä"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E5" do
    input = "&#x00e5;"
    output = [["Character", "å"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E6" do
    input = "&#x00e6;"
    output = [["Character", "æ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E7" do
    input = "&#x00e7;"
    output = [["Character", "ç"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E8" do
    input = "&#x00e8;"
    output = [["Character", "è"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00E9" do
    input = "&#x00e9;"
    output = [["Character", "é"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EA" do
    input = "&#x00ea;"
    output = [["Character", "ê"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EB" do
    input = "&#x00eb;"
    output = [["Character", "ë"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EC" do
    input = "&#x00ec;"
    output = [["Character", "ì"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00ED" do
    input = "&#x00ed;"
    output = [["Character", "í"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EE" do
    input = "&#x00ee;"
    output = [["Character", "î"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00EF" do
    input = "&#x00ef;"
    output = [["Character", "ï"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F0" do
    input = "&#x00f0;"
    output = [["Character", "ð"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F1" do
    input = "&#x00f1;"
    output = [["Character", "ñ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F2" do
    input = "&#x00f2;"
    output = [["Character", "ò"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F3" do
    input = "&#x00f3;"
    output = [["Character", "ó"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F4" do
    input = "&#x00f4;"
    output = [["Character", "ô"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F5" do
    input = "&#x00f5;"
    output = [["Character", "õ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F6" do
    input = "&#x00f6;"
    output = [["Character", "ö"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F7" do
    input = "&#x00f7;"
    output = [["Character", "÷"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F8" do
    input = "&#x00f8;"
    output = [["Character", "ø"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00F9" do
    input = "&#x00f9;"
    output = [["Character", "ù"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FA" do
    input = "&#x00fa;"
    output = [["Character", "ú"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FB" do
    input = "&#x00fb;"
    output = [["Character", "û"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FC" do
    input = "&#x00fc;"
    output = [["Character", "ü"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FD" do
    input = "&#x00fd;"
    output = [["Character", "ý"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FE" do
    input = "&#x00fe;"
    output = [["Character", "þ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+00FF" do
    input = "&#x00ff;"
    output = [["Character", "ÿ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+10000" do
    input = "&#x10000;"
    output = [["Character", "𐀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+100000" do
    input = "&#x100000;"
    output = [["Character", "􀀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+10FFFD" do
    input = "&#x10fffd;"
    output = [["Character", "􏿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+1FFFD" do
    input = "&#x1fffd;"
    output = [["Character", "🿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+20000" do
    input = "&#x20000;"
    output = [["Character", "𠀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+2FFFD" do
    input = "&#x2fffd;"
    output = [["Character", "𯿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+30000" do
    input = "&#x30000;"
    output = [["Character", "𰀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end