defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart19Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Bad named entity: thetasym without a semi-colon" do
    input = "&thetasym"
    output = [["Character", "&thetasym"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: thetav without a semi-colon" do
    input = "&thetav"
    output = [["Character", "&thetav"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: thickapprox without a semi-colon" do
    input = "&thickapprox"
    output = [["Character", "&thickapprox"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: thicksim without a semi-colon" do
    input = "&thicksim"
    output = [["Character", "&thicksim"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: thinsp without a semi-colon" do
    input = "&thinsp"
    output = [["Character", "&thinsp"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: thkap without a semi-colon" do
    input = "&thkap"
    output = [["Character", "&thkap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: thksim without a semi-colon" do
    input = "&thksim"
    output = [["Character", "&thksim"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tilde without a semi-colon" do
    input = "&tilde"
    output = [["Character", "&tilde"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tint without a semi-colon" do
    input = "&tint"
    output = [["Character", "&tint"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: toea without a semi-colon" do
    input = "&toea"
    output = [["Character", "&toea"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: top without a semi-colon" do
    input = "&top"
    output = [["Character", "&top"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: topbot without a semi-colon" do
    input = "&topbot"
    output = [["Character", "&topbot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: topcir without a semi-colon" do
    input = "&topcir"
    output = [["Character", "&topcir"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: topf without a semi-colon" do
    input = "&topf"
    output = [["Character", "&topf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: topfork without a semi-colon" do
    input = "&topfork"
    output = [["Character", "&topfork"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tosa without a semi-colon" do
    input = "&tosa"
    output = [["Character", "&tosa"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tprime without a semi-colon" do
    input = "&tprime"
    output = [["Character", "&tprime"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: trade without a semi-colon" do
    input = "&trade"
    output = [["Character", "&trade"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: triangle without a semi-colon" do
    input = "&triangle"
    output = [["Character", "&triangle"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: triangledown without a semi-colon" do
    input = "&triangledown"
    output = [["Character", "&triangledown"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: triangleleft without a semi-colon" do
    input = "&triangleleft"
    output = [["Character", "&triangleleft"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: trianglelefteq without a semi-colon" do
    input = "&trianglelefteq"
    output = [["Character", "&trianglelefteq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: triangleq without a semi-colon" do
    input = "&triangleq"
    output = [["Character", "&triangleq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: triangleright without a semi-colon" do
    input = "&triangleright"
    output = [["Character", "&triangleright"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: trianglerighteq without a semi-colon" do
    input = "&trianglerighteq"
    output = [["Character", "&trianglerighteq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tridot without a semi-colon" do
    input = "&tridot"
    output = [["Character", "&tridot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: trie without a semi-colon" do
    input = "&trie"
    output = [["Character", "&trie"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: triminus without a semi-colon" do
    input = "&triminus"
    output = [["Character", "&triminus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: triplus without a semi-colon" do
    input = "&triplus"
    output = [["Character", "&triplus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: trisb without a semi-colon" do
    input = "&trisb"
    output = [["Character", "&trisb"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tritime without a semi-colon" do
    input = "&tritime"
    output = [["Character", "&tritime"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: trpezium without a semi-colon" do
    input = "&trpezium"
    output = [["Character", "&trpezium"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tscr without a semi-colon" do
    input = "&tscr"
    output = [["Character", "&tscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tscy without a semi-colon" do
    input = "&tscy"
    output = [["Character", "&tscy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tshcy without a semi-colon" do
    input = "&tshcy"
    output = [["Character", "&tshcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: tstrok without a semi-colon" do
    input = "&tstrok"
    output = [["Character", "&tstrok"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: twixt without a semi-colon" do
    input = "&twixt"
    output = [["Character", "&twixt"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: twoheadleftarrow without a semi-colon" do
    input = "&twoheadleftarrow"
    output = [["Character", "&twoheadleftarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: twoheadrightarrow without a semi-colon" do
    input = "&twoheadrightarrow"
    output = [["Character", "&twoheadrightarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uArr without a semi-colon" do
    input = "&uArr"
    output = [["Character", "&uArr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uHar without a semi-colon" do
    input = "&uHar"
    output = [["Character", "&uHar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uarr without a semi-colon" do
    input = "&uarr"
    output = [["Character", "&uarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ubrcy without a semi-colon" do
    input = "&ubrcy"
    output = [["Character", "&ubrcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ubreve without a semi-colon" do
    input = "&ubreve"
    output = [["Character", "&ubreve"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ucy without a semi-colon" do
    input = "&ucy"
    output = [["Character", "&ucy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: udarr without a semi-colon" do
    input = "&udarr"
    output = [["Character", "&udarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: udblac without a semi-colon" do
    input = "&udblac"
    output = [["Character", "&udblac"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: udhar without a semi-colon" do
    input = "&udhar"
    output = [["Character", "&udhar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ufisht without a semi-colon" do
    input = "&ufisht"
    output = [["Character", "&ufisht"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ufr without a semi-colon" do
    input = "&ufr"
    output = [["Character", "&ufr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uharl without a semi-colon" do
    input = "&uharl"
    output = [["Character", "&uharl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uharr without a semi-colon" do
    input = "&uharr"
    output = [["Character", "&uharr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uhblk without a semi-colon" do
    input = "&uhblk"
    output = [["Character", "&uhblk"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ulcorn without a semi-colon" do
    input = "&ulcorn"
    output = [["Character", "&ulcorn"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ulcorner without a semi-colon" do
    input = "&ulcorner"
    output = [["Character", "&ulcorner"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ulcrop without a semi-colon" do
    input = "&ulcrop"
    output = [["Character", "&ulcrop"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ultri without a semi-colon" do
    input = "&ultri"
    output = [["Character", "&ultri"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: umacr without a semi-colon" do
    input = "&umacr"
    output = [["Character", "&umacr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uogon without a semi-colon" do
    input = "&uogon"
    output = [["Character", "&uogon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uopf without a semi-colon" do
    input = "&uopf"
    output = [["Character", "&uopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uparrow without a semi-colon" do
    input = "&uparrow"
    output = [["Character", "&uparrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: updownarrow without a semi-colon" do
    input = "&updownarrow"
    output = [["Character", "&updownarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: upharpoonleft without a semi-colon" do
    input = "&upharpoonleft"
    output = [["Character", "&upharpoonleft"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: upharpoonright without a semi-colon" do
    input = "&upharpoonright"
    output = [["Character", "&upharpoonright"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uplus without a semi-colon" do
    input = "&uplus"
    output = [["Character", "&uplus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: upsi without a semi-colon" do
    input = "&upsi"
    output = [["Character", "&upsi"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: upsih without a semi-colon" do
    input = "&upsih"
    output = [["Character", "&upsih"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: upsilon without a semi-colon" do
    input = "&upsilon"
    output = [["Character", "&upsilon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: upuparrows without a semi-colon" do
    input = "&upuparrows"
    output = [["Character", "&upuparrows"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: urcorn without a semi-colon" do
    input = "&urcorn"
    output = [["Character", "&urcorn"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: urcorner without a semi-colon" do
    input = "&urcorner"
    output = [["Character", "&urcorner"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: urcrop without a semi-colon" do
    input = "&urcrop"
    output = [["Character", "&urcrop"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uring without a semi-colon" do
    input = "&uring"
    output = [["Character", "&uring"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: urtri without a semi-colon" do
    input = "&urtri"
    output = [["Character", "&urtri"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uscr without a semi-colon" do
    input = "&uscr"
    output = [["Character", "&uscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: utdot without a semi-colon" do
    input = "&utdot"
    output = [["Character", "&utdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: utilde without a semi-colon" do
    input = "&utilde"
    output = [["Character", "&utilde"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: utri without a semi-colon" do
    input = "&utri"
    output = [["Character", "&utri"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: utrif without a semi-colon" do
    input = "&utrif"
    output = [["Character", "&utrif"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uuarr without a semi-colon" do
    input = "&uuarr"
    output = [["Character", "&uuarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: uwangle without a semi-colon" do
    input = "&uwangle"
    output = [["Character", "&uwangle"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vArr without a semi-colon" do
    input = "&vArr"
    output = [["Character", "&vArr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vBar without a semi-colon" do
    input = "&vBar"
    output = [["Character", "&vBar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vBarv without a semi-colon" do
    input = "&vBarv"
    output = [["Character", "&vBarv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vDash without a semi-colon" do
    input = "&vDash"
    output = [["Character", "&vDash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vangrt without a semi-colon" do
    input = "&vangrt"
    output = [["Character", "&vangrt"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varepsilon without a semi-colon" do
    input = "&varepsilon"
    output = [["Character", "&varepsilon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varkappa without a semi-colon" do
    input = "&varkappa"
    output = [["Character", "&varkappa"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varnothing without a semi-colon" do
    input = "&varnothing"
    output = [["Character", "&varnothing"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varphi without a semi-colon" do
    input = "&varphi"
    output = [["Character", "&varphi"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varpi without a semi-colon" do
    input = "&varpi"
    output = [["Character", "&varpi"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varpropto without a semi-colon" do
    input = "&varpropto"
    output = [["Character", "&varpropto"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varr without a semi-colon" do
    input = "&varr"
    output = [["Character", "&varr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varrho without a semi-colon" do
    input = "&varrho"
    output = [["Character", "&varrho"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varsigma without a semi-colon" do
    input = "&varsigma"
    output = [["Character", "&varsigma"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varsubsetneq without a semi-colon" do
    input = "&varsubsetneq"
    output = [["Character", "&varsubsetneq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varsubsetneqq without a semi-colon" do
    input = "&varsubsetneqq"
    output = [["Character", "&varsubsetneqq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varsupsetneq without a semi-colon" do
    input = "&varsupsetneq"
    output = [["Character", "&varsupsetneq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: varsupsetneqq without a semi-colon" do
    input = "&varsupsetneqq"
    output = [["Character", "&varsupsetneqq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: vartheta without a semi-colon" do
    input = "&vartheta"
    output = [["Character", "&vartheta"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end