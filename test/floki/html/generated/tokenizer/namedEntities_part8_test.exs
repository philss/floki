defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart8Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Bad named entity: boxtimes without a semi-colon" do
    input = "&boxtimes"
    output = [["Character", "&boxtimes"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxuL without a semi-colon" do
    input = "&boxuL"
    output = [["Character", "&boxuL"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxuR without a semi-colon" do
    input = "&boxuR"
    output = [["Character", "&boxuR"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxul without a semi-colon" do
    input = "&boxul"
    output = [["Character", "&boxul"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxur without a semi-colon" do
    input = "&boxur"
    output = [["Character", "&boxur"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxv without a semi-colon" do
    input = "&boxv"
    output = [["Character", "&boxv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxvH without a semi-colon" do
    input = "&boxvH"
    output = [["Character", "&boxvH"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxvL without a semi-colon" do
    input = "&boxvL"
    output = [["Character", "&boxvL"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxvR without a semi-colon" do
    input = "&boxvR"
    output = [["Character", "&boxvR"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxvh without a semi-colon" do
    input = "&boxvh"
    output = [["Character", "&boxvh"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxvl without a semi-colon" do
    input = "&boxvl"
    output = [["Character", "&boxvl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: boxvr without a semi-colon" do
    input = "&boxvr"
    output = [["Character", "&boxvr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bprime without a semi-colon" do
    input = "&bprime"
    output = [["Character", "&bprime"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: breve without a semi-colon" do
    input = "&breve"
    output = [["Character", "&breve"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bscr without a semi-colon" do
    input = "&bscr"
    output = [["Character", "&bscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bsemi without a semi-colon" do
    input = "&bsemi"
    output = [["Character", "&bsemi"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bsim without a semi-colon" do
    input = "&bsim"
    output = [["Character", "&bsim"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bsime without a semi-colon" do
    input = "&bsime"
    output = [["Character", "&bsime"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bsol without a semi-colon" do
    input = "&bsol"
    output = [["Character", "&bsol"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bsolb without a semi-colon" do
    input = "&bsolb"
    output = [["Character", "&bsolb"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bsolhsub without a semi-colon" do
    input = "&bsolhsub"
    output = [["Character", "&bsolhsub"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bull without a semi-colon" do
    input = "&bull"
    output = [["Character", "&bull"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bullet without a semi-colon" do
    input = "&bullet"
    output = [["Character", "&bullet"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bump without a semi-colon" do
    input = "&bump"
    output = [["Character", "&bump"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bumpE without a semi-colon" do
    input = "&bumpE"
    output = [["Character", "&bumpE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bumpe without a semi-colon" do
    input = "&bumpe"
    output = [["Character", "&bumpe"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: bumpeq without a semi-colon" do
    input = "&bumpeq"
    output = [["Character", "&bumpeq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cacute without a semi-colon" do
    input = "&cacute"
    output = [["Character", "&cacute"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cap without a semi-colon" do
    input = "&cap"
    output = [["Character", "&cap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: capand without a semi-colon" do
    input = "&capand"
    output = [["Character", "&capand"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: capbrcup without a semi-colon" do
    input = "&capbrcup"
    output = [["Character", "&capbrcup"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: capcap without a semi-colon" do
    input = "&capcap"
    output = [["Character", "&capcap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: capcup without a semi-colon" do
    input = "&capcup"
    output = [["Character", "&capcup"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: capdot without a semi-colon" do
    input = "&capdot"
    output = [["Character", "&capdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: caps without a semi-colon" do
    input = "&caps"
    output = [["Character", "&caps"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: caret without a semi-colon" do
    input = "&caret"
    output = [["Character", "&caret"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: caron without a semi-colon" do
    input = "&caron"
    output = [["Character", "&caron"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ccaps without a semi-colon" do
    input = "&ccaps"
    output = [["Character", "&ccaps"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ccaron without a semi-colon" do
    input = "&ccaron"
    output = [["Character", "&ccaron"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ccirc without a semi-colon" do
    input = "&ccirc"
    output = [["Character", "&ccirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ccups without a semi-colon" do
    input = "&ccups"
    output = [["Character", "&ccups"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ccupssm without a semi-colon" do
    input = "&ccupssm"
    output = [["Character", "&ccupssm"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cdot without a semi-colon" do
    input = "&cdot"
    output = [["Character", "&cdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cemptyv without a semi-colon" do
    input = "&cemptyv"
    output = [["Character", "&cemptyv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cfr without a semi-colon" do
    input = "&cfr"
    output = [["Character", "&cfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: chcy without a semi-colon" do
    input = "&chcy"
    output = [["Character", "&chcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: check without a semi-colon" do
    input = "&check"
    output = [["Character", "&check"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: checkmark without a semi-colon" do
    input = "&checkmark"
    output = [["Character", "&checkmark"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: chi without a semi-colon" do
    input = "&chi"
    output = [["Character", "&chi"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cir without a semi-colon" do
    input = "&cir"
    output = [["Character", "&cir"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cirE without a semi-colon" do
    input = "&cirE"
    output = [["Character", "&cirE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: circ without a semi-colon" do
    input = "&circ"
    output = [["Character", "&circ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: circeq without a semi-colon" do
    input = "&circeq"
    output = [["Character", "&circeq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: circlearrowleft without a semi-colon" do
    input = "&circlearrowleft"
    output = [["Character", "&circlearrowleft"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: circlearrowright without a semi-colon" do
    input = "&circlearrowright"
    output = [["Character", "&circlearrowright"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: circledR without a semi-colon" do
    input = "&circledR"
    output = [["Character", "&circledR"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: circledS without a semi-colon" do
    input = "&circledS"
    output = [["Character", "&circledS"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: circledast without a semi-colon" do
    input = "&circledast"
    output = [["Character", "&circledast"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: circledcirc without a semi-colon" do
    input = "&circledcirc"
    output = [["Character", "&circledcirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: circleddash without a semi-colon" do
    input = "&circleddash"
    output = [["Character", "&circleddash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cire without a semi-colon" do
    input = "&cire"
    output = [["Character", "&cire"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cirfnint without a semi-colon" do
    input = "&cirfnint"
    output = [["Character", "&cirfnint"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cirmid without a semi-colon" do
    input = "&cirmid"
    output = [["Character", "&cirmid"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cirscir without a semi-colon" do
    input = "&cirscir"
    output = [["Character", "&cirscir"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: clubs without a semi-colon" do
    input = "&clubs"
    output = [["Character", "&clubs"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: clubsuit without a semi-colon" do
    input = "&clubsuit"
    output = [["Character", "&clubsuit"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: colon without a semi-colon" do
    input = "&colon"
    output = [["Character", "&colon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: colone without a semi-colon" do
    input = "&colone"
    output = [["Character", "&colone"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: coloneq without a semi-colon" do
    input = "&coloneq"
    output = [["Character", "&coloneq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: comma without a semi-colon" do
    input = "&comma"
    output = [["Character", "&comma"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: commat without a semi-colon" do
    input = "&commat"
    output = [["Character", "&commat"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: comp without a semi-colon" do
    input = "&comp"
    output = [["Character", "&comp"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: compfn without a semi-colon" do
    input = "&compfn"
    output = [["Character", "&compfn"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: complement without a semi-colon" do
    input = "&complement"
    output = [["Character", "&complement"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: complexes without a semi-colon" do
    input = "&complexes"
    output = [["Character", "&complexes"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cong without a semi-colon" do
    input = "&cong"
    output = [["Character", "&cong"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: congdot without a semi-colon" do
    input = "&congdot"
    output = [["Character", "&congdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: conint without a semi-colon" do
    input = "&conint"
    output = [["Character", "&conint"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: copf without a semi-colon" do
    input = "&copf"
    output = [["Character", "&copf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: coprod without a semi-colon" do
    input = "&coprod"
    output = [["Character", "&coprod"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: crarr without a semi-colon" do
    input = "&crarr"
    output = [["Character", "&crarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cross without a semi-colon" do
    input = "&cross"
    output = [["Character", "&cross"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cscr without a semi-colon" do
    input = "&cscr"
    output = [["Character", "&cscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: csub without a semi-colon" do
    input = "&csub"
    output = [["Character", "&csub"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: csube without a semi-colon" do
    input = "&csube"
    output = [["Character", "&csube"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: csup without a semi-colon" do
    input = "&csup"
    output = [["Character", "&csup"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: csupe without a semi-colon" do
    input = "&csupe"
    output = [["Character", "&csupe"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ctdot without a semi-colon" do
    input = "&ctdot"
    output = [["Character", "&ctdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cudarrl without a semi-colon" do
    input = "&cudarrl"
    output = [["Character", "&cudarrl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cudarrr without a semi-colon" do
    input = "&cudarrr"
    output = [["Character", "&cudarrr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cuepr without a semi-colon" do
    input = "&cuepr"
    output = [["Character", "&cuepr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cuesc without a semi-colon" do
    input = "&cuesc"
    output = [["Character", "&cuesc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cularr without a semi-colon" do
    input = "&cularr"
    output = [["Character", "&cularr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cularrp without a semi-colon" do
    input = "&cularrp"
    output = [["Character", "&cularrp"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cup without a semi-colon" do
    input = "&cup"
    output = [["Character", "&cup"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cupbrcap without a semi-colon" do
    input = "&cupbrcap"
    output = [["Character", "&cupbrcap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cupcap without a semi-colon" do
    input = "&cupcap"
    output = [["Character", "&cupcap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cupcup without a semi-colon" do
    input = "&cupcup"
    output = [["Character", "&cupcup"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cupdot without a semi-colon" do
    input = "&cupdot"
    output = [["Character", "&cupdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: cupor without a semi-colon" do
    input = "&cupor"
    output = [["Character", "&cupor"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end