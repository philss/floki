defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart21Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: Ascr; with a semi-colon" do
    input = "&Ascr;"
    output = [["Character", "ð’œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Assign; with a semi-colon" do
    input = "&Assign;"
    output = [["Character", "â‰”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Atilde without a semi-colon" do
    input = "&Atilde"
    output = [["Character", "Ãƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Atilde; with a semi-colon" do
    input = "&Atilde;"
    output = [["Character", "Ãƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Auml without a semi-colon" do
    input = "&Auml"
    output = [["Character", "Ã„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Auml; with a semi-colon" do
    input = "&Auml;"
    output = [["Character", "Ã„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Backslash; with a semi-colon" do
    input = "&Backslash;"
    output = [["Character", "âˆ–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Barv; with a semi-colon" do
    input = "&Barv;"
    output = [["Character", "â«§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Barwed; with a semi-colon" do
    input = "&Barwed;"
    output = [["Character", "âŒ†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Bcy; with a semi-colon" do
    input = "&Bcy;"
    output = [["Character", "Ð‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Because; with a semi-colon" do
    input = "&Because;"
    output = [["Character", "âˆµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Bernoullis; with a semi-colon" do
    input = "&Bernoullis;"
    output = [["Character", "â„¬"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Beta; with a semi-colon" do
    input = "&Beta;"
    output = [["Character", "Î’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Bfr; with a semi-colon" do
    input = "&Bfr;"
    output = [["Character", "ð”…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Bopf; with a semi-colon" do
    input = "&Bopf;"
    output = [["Character", "ð”¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Breve; with a semi-colon" do
    input = "&Breve;"
    output = [["Character", "Ë˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Bscr; with a semi-colon" do
    input = "&Bscr;"
    output = [["Character", "â„¬"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Bumpeq; with a semi-colon" do
    input = "&Bumpeq;"
    output = [["Character", "â‰Ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CHcy; with a semi-colon" do
    input = "&CHcy;"
    output = [["Character", "Ð§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: COPY without a semi-colon" do
    input = "&COPY"
    output = [["Character", "Â©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: COPY; with a semi-colon" do
    input = "&COPY;"
    output = [["Character", "Â©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cacute; with a semi-colon" do
    input = "&Cacute;"
    output = [["Character", "Ä†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cap; with a semi-colon" do
    input = "&Cap;"
    output = [["Character", "â‹’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CapitalDifferentialD; with a semi-colon" do
    input = "&CapitalDifferentialD;"
    output = [["Character", "â……"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cayleys; with a semi-colon" do
    input = "&Cayleys;"
    output = [["Character", "â„­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Ccaron; with a semi-colon" do
    input = "&Ccaron;"
    output = [["Character", "ÄŒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Ccedil without a semi-colon" do
    input = "&Ccedil"
    output = [["Character", "Ã‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Ccedil; with a semi-colon" do
    input = "&Ccedil;"
    output = [["Character", "Ã‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Ccirc; with a semi-colon" do
    input = "&Ccirc;"
    output = [["Character", "Äˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cconint; with a semi-colon" do
    input = "&Cconint;"
    output = [["Character", "âˆ°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cdot; with a semi-colon" do
    input = "&Cdot;"
    output = [["Character", "ÄŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cedilla; with a semi-colon" do
    input = "&Cedilla;"
    output = [["Character", "Â¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CenterDot; with a semi-colon" do
    input = "&CenterDot;"
    output = [["Character", "Â·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cfr; with a semi-colon" do
    input = "&Cfr;"
    output = [["Character", "â„­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Chi; with a semi-colon" do
    input = "&Chi;"
    output = [["Character", "Î§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CircleDot; with a semi-colon" do
    input = "&CircleDot;"
    output = [["Character", "âŠ™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CircleMinus; with a semi-colon" do
    input = "&CircleMinus;"
    output = [["Character", "âŠ–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CirclePlus; with a semi-colon" do
    input = "&CirclePlus;"
    output = [["Character", "âŠ•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CircleTimes; with a semi-colon" do
    input = "&CircleTimes;"
    output = [["Character", "âŠ—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ClockwiseContourIntegral; with a semi-colon" do
    input = "&ClockwiseContourIntegral;"
    output = [["Character", "âˆ²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CloseCurlyDoubleQuote; with a semi-colon" do
    input = "&CloseCurlyDoubleQuote;"
    output = [["Character", "â€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CloseCurlyQuote; with a semi-colon" do
    input = "&CloseCurlyQuote;"
    output = [["Character", "â€™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Colon; with a semi-colon" do
    input = "&Colon;"
    output = [["Character", "âˆ·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Colone; with a semi-colon" do
    input = "&Colone;"
    output = [["Character", "â©´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Congruent; with a semi-colon" do
    input = "&Congruent;"
    output = [["Character", "â‰¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Conint; with a semi-colon" do
    input = "&Conint;"
    output = [["Character", "âˆ¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ContourIntegral; with a semi-colon" do
    input = "&ContourIntegral;"
    output = [["Character", "âˆ®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Copf; with a semi-colon" do
    input = "&Copf;"
    output = [["Character", "â„‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Coproduct; with a semi-colon" do
    input = "&Coproduct;"
    output = [["Character", "âˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CounterClockwiseContourIntegral; with a semi-colon" do
    input = "&CounterClockwiseContourIntegral;"
    output = [["Character", "âˆ³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cross; with a semi-colon" do
    input = "&Cross;"
    output = [["Character", "â¨¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cscr; with a semi-colon" do
    input = "&Cscr;"
    output = [["Character", "ð’ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Cup; with a semi-colon" do
    input = "&Cup;"
    output = [["Character", "â‹“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: CupCap; with a semi-colon" do
    input = "&CupCap;"
    output = [["Character", "â‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DD; with a semi-colon" do
    input = "&DD;"
    output = [["Character", "â……"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DDotrahd; with a semi-colon" do
    input = "&DDotrahd;"
    output = [["Character", "â¤‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DJcy; with a semi-colon" do
    input = "&DJcy;"
    output = [["Character", "Ð‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DScy; with a semi-colon" do
    input = "&DScy;"
    output = [["Character", "Ð…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DZcy; with a semi-colon" do
    input = "&DZcy;"
    output = [["Character", "Ð"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Dagger; with a semi-colon" do
    input = "&Dagger;"
    output = [["Character", "â€¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Darr; with a semi-colon" do
    input = "&Darr;"
    output = [["Character", "â†¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Dashv; with a semi-colon" do
    input = "&Dashv;"
    output = [["Character", "â«¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Dcaron; with a semi-colon" do
    input = "&Dcaron;"
    output = [["Character", "ÄŽ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Dcy; with a semi-colon" do
    input = "&Dcy;"
    output = [["Character", "Ð”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Del; with a semi-colon" do
    input = "&Del;"
    output = [["Character", "âˆ‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Delta; with a semi-colon" do
    input = "&Delta;"
    output = [["Character", "Î”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Dfr; with a semi-colon" do
    input = "&Dfr;"
    output = [["Character", "ð”‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DiacriticalAcute; with a semi-colon" do
    input = "&DiacriticalAcute;"
    output = [["Character", "Â´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DiacriticalDot; with a semi-colon" do
    input = "&DiacriticalDot;"
    output = [["Character", "Ë™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DiacriticalDoubleAcute; with a semi-colon" do
    input = "&DiacriticalDoubleAcute;"
    output = [["Character", "Ë"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DiacriticalGrave; with a semi-colon" do
    input = "&DiacriticalGrave;"
    output = [["Character", "`"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DiacriticalTilde; with a semi-colon" do
    input = "&DiacriticalTilde;"
    output = [["Character", "Ëœ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Diamond; with a semi-colon" do
    input = "&Diamond;"
    output = [["Character", "â‹„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DifferentialD; with a semi-colon" do
    input = "&DifferentialD;"
    output = [["Character", "â…†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Dopf; with a semi-colon" do
    input = "&Dopf;"
    output = [["Character", "ð”»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: Dot; with a semi-colon" do
    input = "&Dot;"
    output = [["Character", "Â¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DotDot; with a semi-colon" do
    input = "&DotDot;"
    output = [["Character", "âƒœ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DotEqual; with a semi-colon" do
    input = "&DotEqual;"
    output = [["Character", "â‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleContourIntegral; with a semi-colon" do
    input = "&DoubleContourIntegral;"
    output = [["Character", "âˆ¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleDot; with a semi-colon" do
    input = "&DoubleDot;"
    output = [["Character", "Â¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleDownArrow; with a semi-colon" do
    input = "&DoubleDownArrow;"
    output = [["Character", "â‡“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleLeftArrow; with a semi-colon" do
    input = "&DoubleLeftArrow;"
    output = [["Character", "â‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleLeftRightArrow; with a semi-colon" do
    input = "&DoubleLeftRightArrow;"
    output = [["Character", "â‡”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleLeftTee; with a semi-colon" do
    input = "&DoubleLeftTee;"
    output = [["Character", "â«¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleLongLeftArrow; with a semi-colon" do
    input = "&DoubleLongLeftArrow;"
    output = [["Character", "âŸ¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleLongLeftRightArrow; with a semi-colon" do
    input = "&DoubleLongLeftRightArrow;"
    output = [["Character", "âŸº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleLongRightArrow; with a semi-colon" do
    input = "&DoubleLongRightArrow;"
    output = [["Character", "âŸ¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleRightArrow; with a semi-colon" do
    input = "&DoubleRightArrow;"
    output = [["Character", "â‡’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleRightTee; with a semi-colon" do
    input = "&DoubleRightTee;"
    output = [["Character", "âŠ¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleUpArrow; with a semi-colon" do
    input = "&DoubleUpArrow;"
    output = [["Character", "â‡‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleUpDownArrow; with a semi-colon" do
    input = "&DoubleUpDownArrow;"
    output = [["Character", "â‡•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DoubleVerticalBar; with a semi-colon" do
    input = "&DoubleVerticalBar;"
    output = [["Character", "âˆ¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DownArrow; with a semi-colon" do
    input = "&DownArrow;"
    output = [["Character", "â†“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DownArrowBar; with a semi-colon" do
    input = "&DownArrowBar;"
    output = [["Character", "â¤“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DownArrowUpArrow; with a semi-colon" do
    input = "&DownArrowUpArrow;"
    output = [["Character", "â‡µ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DownBreve; with a semi-colon" do
    input = "&DownBreve;"
    output = [["Character", "Ì‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DownLeftRightVector; with a semi-colon" do
    input = "&DownLeftRightVector;"
    output = [["Character", "â¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DownLeftTeeVector; with a semi-colon" do
    input = "&DownLeftTeeVector;"
    output = [["Character", "â¥ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DownLeftVector; with a semi-colon" do
    input = "&DownLeftVector;"
    output = [["Character", "â†½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: DownLeftVectorBar; with a semi-colon" do
    input = "&DownLeftVectorBar;"
    output = [["Character", "â¥–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end