defmodule Floki.HTML.Generated.Tokenizer.Test3Part13Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests test3.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 <a a [>" do
    input = "<a a [>"
    output = [["StartTag", "a", %{"[" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a \\u0000>" do
    input = <<60, 97, 32, 97, 32, 0, 62>>
    output = [["StartTag", "a", %{"a" => "", "�" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a \\u0008>" do
    input = "<a a \b>"
    output = [["StartTag", "a", %{"\b" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a \\u0009>" do
    input = "<a a \t>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a \\u000A>" do
    input = "<a a \n>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a \\u000B>" do
    input = "<a a \v>"
    output = [["StartTag", "a", %{"\v" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a \\u000C>" do
    input = "<a a \f>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a \\u000D>" do
    input = "<a a \r>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a \\u001F>" do
    input = <<60, 97, 32, 97, 32, 31, 62>>
    output = [["StartTag", "a", %{<<31>> => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a \\uDBC0\\uDC00>" do
    input = "<a a 􀀀>"
    output = [["StartTag", "a", %{"a" => "", "􀀀" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a `>" do
    input = "<a a `>"
    output = [["StartTag", "a", %{"`" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a a>" do
    input = "<a a a>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a b>" do
    input = "<a a b>"
    output = [["StartTag", "a", %{"a" => "", "b" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a y>" do
    input = "<a a y>"
    output = [["StartTag", "a", %{"a" => "", "y" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a z>" do
    input = "<a a z>"
    output = [["StartTag", "a", %{"a" => "", "z" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a {>" do
    input = "<a a {>"
    output = [["StartTag", "a", %{"a" => "", "{" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a!>" do
    input = "<a a!>"
    output = [["StartTag", "a", %{"a!" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a\">" do
    input = "<a a\">"
    output = [["StartTag", "a", %{"a\"" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a#>" do
    input = "<a a#>"
    output = [["StartTag", "a", %{"a#" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a&>" do
    input = "<a a&>"
    output = [["StartTag", "a", %{"a&" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a'>" do
    input = "<a a'>"
    output = [["StartTag", "a", %{"a'" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a(>" do
    input = "<a a(>"
    output = [["StartTag", "a", %{"a(" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a->" do
    input = "<a a->"
    output = [["StartTag", "a", %{"a-" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a.>" do
    input = "<a a.>"
    output = [["StartTag", "a", %{"a." => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a/>" do
    input = "<a a/>"
    output = [["StartTag", "a", %{"a" => ""}, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a0>" do
    input = "<a a0>"
    output = [["StartTag", "a", %{"a0" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a1>" do
    input = "<a a1>"
    output = [["StartTag", "a", %{"a1" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a9>" do
    input = "<a a9>"
    output = [["StartTag", "a", %{"a9" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a<>" do
    input = "<a a<>"
    output = [["StartTag", "a", %{"a<" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a= >" do
    input = "<a a= >"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=!>" do
    input = "<a a=!>"
    output = [["StartTag", "a", %{"a" => "!"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\" \">" do
    input = "<a a=\" \">"
    output = [["StartTag", "a", %{"a" => " "}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"!\">" do
    input = "<a a=\"!\">"
    output = [["StartTag", "a", %{"a" => "!"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"\">" do
    input = "<a a=\"\">"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"#\">" do
    input = "<a a=\"#\">"
    output = [["StartTag", "a", %{"a" => "#"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"%\">" do
    input = "<a a=\"%\">"
    output = [["StartTag", "a", %{"a" => "%"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"&\">" do
    input = "<a a=\"&\">"
    output = [["StartTag", "a", %{"a" => "&"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"'\">" do
    input = "<a a=\"'\">"
    output = [["StartTag", "a", %{"a" => "'"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"-\">" do
    input = "<a a=\"-\">"
    output = [["StartTag", "a", %{"a" => "-"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"/\">" do
    input = "<a a=\"/\">"
    output = [["StartTag", "a", %{"a" => "/"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"0\">" do
    input = "<a a=\"0\">"
    output = [["StartTag", "a", %{"a" => "0"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"1\">" do
    input = "<a a=\"1\">"
    output = [["StartTag", "a", %{"a" => "1"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"9\">" do
    input = "<a a=\"9\">"
    output = [["StartTag", "a", %{"a" => "9"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"<\">" do
    input = "<a a=\"<\">"
    output = [["StartTag", "a", %{"a" => "<"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"=\">" do
    input = "<a a=\"=\">"
    output = [["StartTag", "a", %{"a" => "="}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\">\">" do
    input = "<a a=\">\">"
    output = [["StartTag", "a", %{"a" => ">"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"?\">" do
    input = "<a a=\"?\">"
    output = [["StartTag", "a", %{"a" => "?"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"@\">" do
    input = "<a a=\"@\">"
    output = [["StartTag", "a", %{"a" => "@"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"A\">" do
    input = "<a a=\"A\">"
    output = [["StartTag", "a", %{"a" => "A"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"B\">" do
    input = "<a a=\"B\">"
    output = [["StartTag", "a", %{"a" => "B"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"Y\">" do
    input = "<a a=\"Y\">"
    output = [["StartTag", "a", %{"a" => "Y"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"Z\">" do
    input = "<a a=\"Z\">"
    output = [["StartTag", "a", %{"a" => "Z"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"\\u0000\">" do
    input = <<60, 97, 32, 97, 61, 34, 0, 34, 62>>
    output = [["StartTag", "a", %{"a" => "�"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"\\u0009\">" do
    input = "<a a=\"\t\">"
    output = [["StartTag", "a", %{"a" => "\t"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"\\u000A\">" do
    input = "<a a=\"\n\">"
    output = [["StartTag", "a", %{"a" => "\n"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"\\u000B\">" do
    input = "<a a=\"\v\">"
    output = [["StartTag", "a", %{"a" => "\v"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"\\u000C\">" do
    input = "<a a=\"\f\">"
    output = [["StartTag", "a", %{"a" => "\f"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"\\uDBC0\\uDC00\">" do
    input = "<a a=\"􀀀\">"
    output = [["StartTag", "a", %{"a" => "􀀀"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"`\">" do
    input = "<a a=\"`\">"
    output = [["StartTag", "a", %{"a" => "`"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"a\">" do
    input = "<a a=\"a\">"
    output = [["StartTag", "a", %{"a" => "a"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"b\">" do
    input = "<a a=\"b\">"
    output = [["StartTag", "a", %{"a" => "b"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"y\">" do
    input = "<a a=\"y\">"
    output = [["StartTag", "a", %{"a" => "y"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"z\">" do
    input = "<a a=\"z\">"
    output = [["StartTag", "a", %{"a" => "z"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=\"{\">" do
    input = "<a a=\"{\">"
    output = [["StartTag", "a", %{"a" => "{"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=#>" do
    input = "<a a=#>"
    output = [["StartTag", "a", %{"a" => "#"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=%>" do
    input = "<a a=%>"
    output = [["StartTag", "a", %{"a" => "%"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=&>" do
    input = "<a a=&>"
    output = [["StartTag", "a", %{"a" => "&"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=' '>" do
    input = "<a a=' '>"
    output = [["StartTag", "a", %{"a" => " "}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a='!'>" do
    input = "<a a='!'>"
    output = [["StartTag", "a", %{"a" => "!"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a='\"'>" do
    input = "<a a='\"'>"
    output = [["StartTag", "a", %{"a" => "\""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a='%'>" do
    input = "<a a='%'>"
    output = [["StartTag", "a", %{"a" => "%"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a='&'>" do
    input = "<a a='&'>"
    output = [["StartTag", "a", %{"a" => "&"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a='' >" do
    input = "<a a='' >"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''!>" do
    input = "<a a=''!>"
    output = [["StartTag", "a", %{"!" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''\">" do
    input = "<a a=''\">"
    output = [["StartTag", "a", %{"\"" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''&>" do
    input = "<a a=''&>"
    output = [["StartTag", "a", %{"&" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a='''>" do
    input = "<a a='''>"
    output = [["StartTag", "a", %{"'" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''->" do
    input = "<a a=''->"
    output = [["StartTag", "a", %{"-" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''.>" do
    input = "<a a=''.>"
    output = [["StartTag", "a", %{"." => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''/>" do
    input = "<a a=''/>"
    output = [["StartTag", "a", %{"a" => ""}, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''0>" do
    input = "<a a=''0>"
    output = [["StartTag", "a", %{"0" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''1>" do
    input = "<a a=''1>"
    output = [["StartTag", "a", %{"1" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''9>" do
    input = "<a a=''9>"
    output = [["StartTag", "a", %{"9" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''<>" do
    input = "<a a=''<>"
    output = [["StartTag", "a", %{"<" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''=>" do
    input = "<a a=''=>"
    output = [["StartTag", "a", %{"=" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''>" do
    input = "<a a=''>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''?>" do
    input = "<a a=''?>"
    output = [["StartTag", "a", %{"?" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''@>" do
    input = "<a a=''@>"
    output = [["StartTag", "a", %{"@" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''A>" do
    input = "<a a=''A>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''B>" do
    input = "<a a=''B>"
    output = [["StartTag", "a", %{"a" => "", "b" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''Y>" do
    input = "<a a=''Y>"
    output = [["StartTag", "a", %{"a" => "", "y" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''Z>" do
    input = "<a a=''Z>"
    output = [["StartTag", "a", %{"a" => "", "z" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''\\u0000>" do
    input = <<60, 97, 32, 97, 61, 39, 39, 0, 62>>
    output = [["StartTag", "a", %{"a" => "", "�" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''\\u0008>" do
    input = "<a a=''\b>"
    output = [["StartTag", "a", %{"\b" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''\\u0009>" do
    input = "<a a=''\t>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''\\u000A>" do
    input = "<a a=''\n>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''\\u000B>" do
    input = "<a a=''\v>"
    output = [["StartTag", "a", %{"\v" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''\\u000C>" do
    input = "<a a=''\f>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''\\u000D>" do
    input = "<a a=''\r>"
    output = [["StartTag", "a", %{"a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <a a=''\\u001F>" do
    input = <<60, 97, 32, 97, 61, 39, 39, 31, 62>>
    output = [["StartTag", "a", %{<<31>> => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end