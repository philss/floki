defmodule Floki.HTML.Generated.Tokenizer.EntitiesTest do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests entities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Ambiguous ampersand." do
    input = "&rrrraannddom;"
    output = [["Character", "&rrrraannddom;"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 CR as hexadecimal numeric entity" do
    input = "&#x00D;"
    output = [["Character", "\r"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 CR as numeric entity" do
    input = "&#013;"
    output = [["Character", "\r"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Decimal numeric entity followed by hex character A." do
    input = "&#97A"
    output = [["Character", "aA"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Decimal numeric entity followed by hex character a." do
    input = "&#97a"
    output = [["Character", "aa"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Decimal numeric entity followed by hex character f." do
    input = "&#97f"
    output = [["Character", "af"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Entity name followed by the equals sign in an attribute value." do
    input = "<h a='&lang='>"
    output = [["StartTag", "h", %{"a" => "&lang="}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Semicolonless named entity 'not' followed by 'i;' in body" do
    input = "&noti;"
    output = [["Character", "¬i;"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Undefined named entity in attribute value ending in semicolon and whose name starts with a known entity name." do
    input = "<h a='&noti;'>"
    output = [["StartTag", "h", %{"a" => "&noti;"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Very long undefined named entity in body" do
    input =
      "&ammmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmp;"

    output = [
      [
        "Character",
        "&ammmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmp;"
      ]
    ]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 BULLET hexadecimal numeric entity." do
    input = "&#x095;"
    output = [["Character", "•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 BULLET numeric entity." do
    input = "&#0149;"
    output = [["Character", "•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 DAGGER hexadecimal numeric entity." do
    input = "&#x086;"
    output = [["Character", "†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 DAGGER numeric entity." do
    input = "&#0134;"
    output = [["Character", "†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 DOUBLE DAGGER hexadecimal numeric entity." do
    input = "&#x087;"
    output = [["Character", "‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 DOUBLE DAGGER numeric entity." do
    input = "&#0135;"
    output = [["Character", "‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 DOUBLE LOW-9 QUOTATION MARK hexadecimal numeric entity." do
    input = "&#x084;"
    output = [["Character", "„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 DOUBLE LOW-9 QUOTATION MARK numeric entity." do
    input = "&#0132;"
    output = [["Character", "„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 EM DASH hexadecimal numeric entity." do
    input = "&#x097;"
    output = [["Character", "—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 EM DASH numeric entity." do
    input = "&#0151;"
    output = [["Character", "—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 EN DASH hexadecimal numeric entity." do
    input = "&#x096;"
    output = [["Character", "–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 EN DASH numeric entity." do
    input = "&#0150;"
    output = [["Character", "–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 EURO SIGN hexadecimal numeric entity." do
    input = "&#x080;"
    output = [["Character", "€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 EURO SIGN numeric entity." do
    input = "&#0128;"
    output = [["Character", "€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 HORIZONTAL ELLIPSIS hexadecimal numeric entity." do
    input = "&#x085;"
    output = [["Character", "…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 HORIZONTAL ELLIPSIS numeric entity." do
    input = "&#0133;"
    output = [["Character", "…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN CAPITAL LETTER S WITH CARON hexadecimal numeric entity." do
    input = "&#x08A;"
    output = [["Character", "Š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN CAPITAL LETTER S WITH CARON numeric entity." do
    input = "&#0138;"
    output = [["Character", "Š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN CAPITAL LETTER Y WITH DIAERESIS hexadecimal numeric entity." do
    input = "&#x09F;"
    output = [["Character", "Ÿ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN CAPITAL LETTER Z WITH CARON hexadecimal numeric entity." do
    input = "&#x08E;"
    output = [["Character", "Ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN CAPITAL LETTER Z WITH CARON numeric entity." do
    input = "&#0142;"
    output = [["Character", "Ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN CAPITAL LIGATURE OE hexadecimal numeric entity." do
    input = "&#x08C;"
    output = [["Character", "Œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN CAPITAL LIGATURE OE numeric entity." do
    input = "&#0140;"
    output = [["Character", "Œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN SMALL LETTER F WITH HOOK hexadecimal numeric entity." do
    input = "&#x083;"
    output = [["Character", "ƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN SMALL LETTER F WITH HOOK numeric entity." do
    input = "&#0131;"
    output = [["Character", "ƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN SMALL LETTER S WITH CARON hexadecimal numeric entity." do
    input = "&#x09A;"
    output = [["Character", "š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN SMALL LETTER S WITH CARON numeric entity." do
    input = "&#0154;"
    output = [["Character", "š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN SMALL LETTER Z WITH CARON hexadecimal numeric entity." do
    input = "&#x09E;"
    output = [["Character", "ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN SMALL LIGATURE OE hexadecimal numeric entity." do
    input = "&#x09C;"
    output = [["Character", "œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LATIN SMALL LIGATURE OE numeric entity." do
    input = "&#0156;"
    output = [["Character", "œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LEFT DOUBLE QUOTATION MARK hexadecimal numeric entity." do
    input = "&#x093;"
    output = [["Character", "“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LEFT DOUBLE QUOTATION MARK numeric entity." do
    input = "&#0147;"
    output = [["Character", "“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LEFT SINGLE QUOTATION MARK hexadecimal numeric entity." do
    input = "&#x091;"
    output = [["Character", "‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 LEFT SINGLE QUOTATION MARK numeric entity." do
    input = "&#0145;"
    output = [["Character", "‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 MODIFIER LETTER CIRCUMFLEX ACCENT hexadecimal numeric entity." do
    input = "&#x088;"
    output = [["Character", "ˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 MODIFIER LETTER CIRCUMFLEX ACCENT numeric entity." do
    input = "&#0136;"
    output = [["Character", "ˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 PER MILLE SIGN hexadecimal numeric entity." do
    input = "&#x089;"
    output = [["Character", "‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 PER MILLE SIGN numeric entity." do
    input = "&#0137;"
    output = [["Character", "‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 REPLACEMENT CHAR hexadecimal numeric entity." do
    input = "&#x081;"
    output = [["Character", <<194, 129>>]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 REPLACEMENT CHAR numeric entity." do
    input = "&#0129;"
    output = [["Character", <<194, 129>>]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 RIGHT DOUBLE QUOTATION MARK hexadecimal numeric entity." do
    input = "&#x094;"
    output = [["Character", "”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 RIGHT DOUBLE QUOTATION MARK numeric entity." do
    input = "&#0148;"
    output = [["Character", "”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 RIGHT SINGLE QUOTATION MARK hexadecimal numeric entity." do
    input = "&#x092;"
    output = [["Character", "’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 RIGHT SINGLE QUOTATION MARK numeric entity." do
    input = "&#0146;"
    output = [["Character", "’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 SINGLE LEFT-POINTING ANGLE QUOTATION MARK hexadecimal numeric entity." do
    input = "&#x08B;"
    output = [["Character", "‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 SINGLE LEFT-POINTING ANGLE QUOTATION MARK numeric entity." do
    input = "&#0139;"
    output = [["Character", "‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 SINGLE LOW-9 QUOTATION MARK hexadecimal numeric entity." do
    input = "&#x082;"
    output = [["Character", "‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 SINGLE LOW-9 QUOTATION MARK numeric entity." do
    input = "&#0130;"
    output = [["Character", "‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 SINGLE RIGHT-POINTING ANGLE QUOTATION MARK hexadecimal numeric entity." do
    input = "&#x09B;"
    output = [["Character", "›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 SINGLE RIGHT-POINTING ANGLE QUOTATION MARK numeric entity." do
    input = "&#0155;"
    output = [["Character", "›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 SMALL TILDE hexadecimal numeric entity." do
    input = "&#x098;"
    output = [["Character", "˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 SMALL TILDE numeric entity." do
    input = "&#0152;"
    output = [["Character", "˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 TRADE MARK SIGN hexadecimal numeric entity." do
    input = "&#x099;"
    output = [["Character", "™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Windows-1252 TRADE MARK SIGN numeric entity." do
    input = "&#0153;"
    output = [["Character", "™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end