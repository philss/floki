defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart13Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Bad named entity: loang without a semi-colon" do
    input = "&loang"
    output = [["Character", "&loang"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: loarr without a semi-colon" do
    input = "&loarr"
    output = [["Character", "&loarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lobrk without a semi-colon" do
    input = "&lobrk"
    output = [["Character", "&lobrk"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: longleftarrow without a semi-colon" do
    input = "&longleftarrow"
    output = [["Character", "&longleftarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: longleftrightarrow without a semi-colon" do
    input = "&longleftrightarrow"
    output = [["Character", "&longleftrightarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: longmapsto without a semi-colon" do
    input = "&longmapsto"
    output = [["Character", "&longmapsto"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: longrightarrow without a semi-colon" do
    input = "&longrightarrow"
    output = [["Character", "&longrightarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: looparrowleft without a semi-colon" do
    input = "&looparrowleft"
    output = [["Character", "&looparrowleft"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: looparrowright without a semi-colon" do
    input = "&looparrowright"
    output = [["Character", "&looparrowright"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lopar without a semi-colon" do
    input = "&lopar"
    output = [["Character", "&lopar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lopf without a semi-colon" do
    input = "&lopf"
    output = [["Character", "&lopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: loplus without a semi-colon" do
    input = "&loplus"
    output = [["Character", "&loplus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lotimes without a semi-colon" do
    input = "&lotimes"
    output = [["Character", "&lotimes"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lowast without a semi-colon" do
    input = "&lowast"
    output = [["Character", "&lowast"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lowbar without a semi-colon" do
    input = "&lowbar"
    output = [["Character", "&lowbar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: loz without a semi-colon" do
    input = "&loz"
    output = [["Character", "&loz"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lozenge without a semi-colon" do
    input = "&lozenge"
    output = [["Character", "&lozenge"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lozf without a semi-colon" do
    input = "&lozf"
    output = [["Character", "&lozf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lpar without a semi-colon" do
    input = "&lpar"
    output = [["Character", "&lpar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lparlt without a semi-colon" do
    input = "&lparlt"
    output = [["Character", "&lparlt"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lrarr without a semi-colon" do
    input = "&lrarr"
    output = [["Character", "&lrarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lrcorner without a semi-colon" do
    input = "&lrcorner"
    output = [["Character", "&lrcorner"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lrhar without a semi-colon" do
    input = "&lrhar"
    output = [["Character", "&lrhar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lrhard without a semi-colon" do
    input = "&lrhard"
    output = [["Character", "&lrhard"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lrm without a semi-colon" do
    input = "&lrm"
    output = [["Character", "&lrm"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lrtri without a semi-colon" do
    input = "&lrtri"
    output = [["Character", "&lrtri"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lsaquo without a semi-colon" do
    input = "&lsaquo"
    output = [["Character", "&lsaquo"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lscr without a semi-colon" do
    input = "&lscr"
    output = [["Character", "&lscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lsh without a semi-colon" do
    input = "&lsh"
    output = [["Character", "&lsh"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lsim without a semi-colon" do
    input = "&lsim"
    output = [["Character", "&lsim"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lsime without a semi-colon" do
    input = "&lsime"
    output = [["Character", "&lsime"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lsimg without a semi-colon" do
    input = "&lsimg"
    output = [["Character", "&lsimg"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lsqb without a semi-colon" do
    input = "&lsqb"
    output = [["Character", "&lsqb"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lsquo without a semi-colon" do
    input = "&lsquo"
    output = [["Character", "&lsquo"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lsquor without a semi-colon" do
    input = "&lsquor"
    output = [["Character", "&lsquor"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lstrok without a semi-colon" do
    input = "&lstrok"
    output = [["Character", "&lstrok"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lurdshar without a semi-colon" do
    input = "&lurdshar"
    output = [["Character", "&lurdshar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: luruhar without a semi-colon" do
    input = "&luruhar"
    output = [["Character", "&luruhar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lvertneqq without a semi-colon" do
    input = "&lvertneqq"
    output = [["Character", "&lvertneqq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: lvnE without a semi-colon" do
    input = "&lvnE"
    output = [["Character", "&lvnE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mDDot without a semi-colon" do
    input = "&mDDot"
    output = [["Character", "&mDDot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: male without a semi-colon" do
    input = "&male"
    output = [["Character", "&male"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: malt without a semi-colon" do
    input = "&malt"
    output = [["Character", "&malt"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: maltese without a semi-colon" do
    input = "&maltese"
    output = [["Character", "&maltese"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: map without a semi-colon" do
    input = "&map"
    output = [["Character", "&map"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mapsto without a semi-colon" do
    input = "&mapsto"
    output = [["Character", "&mapsto"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mapstodown without a semi-colon" do
    input = "&mapstodown"
    output = [["Character", "&mapstodown"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mapstoleft without a semi-colon" do
    input = "&mapstoleft"
    output = [["Character", "&mapstoleft"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mapstoup without a semi-colon" do
    input = "&mapstoup"
    output = [["Character", "&mapstoup"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: marker without a semi-colon" do
    input = "&marker"
    output = [["Character", "&marker"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mcomma without a semi-colon" do
    input = "&mcomma"
    output = [["Character", "&mcomma"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mcy without a semi-colon" do
    input = "&mcy"
    output = [["Character", "&mcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mdash without a semi-colon" do
    input = "&mdash"
    output = [["Character", "&mdash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: measuredangle without a semi-colon" do
    input = "&measuredangle"
    output = [["Character", "&measuredangle"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mfr without a semi-colon" do
    input = "&mfr"
    output = [["Character", "&mfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mho without a semi-colon" do
    input = "&mho"
    output = [["Character", "&mho"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mid without a semi-colon" do
    input = "&mid"
    output = [["Character", "&mid"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: midast without a semi-colon" do
    input = "&midast"
    output = [["Character", "&midast"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: midcir without a semi-colon" do
    input = "&midcir"
    output = [["Character", "&midcir"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: minus without a semi-colon" do
    input = "&minus"
    output = [["Character", "&minus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: minusb without a semi-colon" do
    input = "&minusb"
    output = [["Character", "&minusb"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: minusd without a semi-colon" do
    input = "&minusd"
    output = [["Character", "&minusd"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: minusdu without a semi-colon" do
    input = "&minusdu"
    output = [["Character", "&minusdu"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mlcp without a semi-colon" do
    input = "&mlcp"
    output = [["Character", "&mlcp"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mldr without a semi-colon" do
    input = "&mldr"
    output = [["Character", "&mldr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mnplus without a semi-colon" do
    input = "&mnplus"
    output = [["Character", "&mnplus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: models without a semi-colon" do
    input = "&models"
    output = [["Character", "&models"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mopf without a semi-colon" do
    input = "&mopf"
    output = [["Character", "&mopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mp without a semi-colon" do
    input = "&mp"
    output = [["Character", "&mp"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mscr without a semi-colon" do
    input = "&mscr"
    output = [["Character", "&mscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mstpos without a semi-colon" do
    input = "&mstpos"
    output = [["Character", "&mstpos"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mu without a semi-colon" do
    input = "&mu"
    output = [["Character", "&mu"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: multimap without a semi-colon" do
    input = "&multimap"
    output = [["Character", "&multimap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: mumap without a semi-colon" do
    input = "&mumap"
    output = [["Character", "&mumap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nGg without a semi-colon" do
    input = "&nGg"
    output = [["Character", "&nGg"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nGt without a semi-colon" do
    input = "&nGt"
    output = [["Character", "&nGt"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nGtv without a semi-colon" do
    input = "&nGtv"
    output = [["Character", "&nGtv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nLeftarrow without a semi-colon" do
    input = "&nLeftarrow"
    output = [["Character", "&nLeftarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nLeftrightarrow without a semi-colon" do
    input = "&nLeftrightarrow"
    output = [["Character", "&nLeftrightarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nLl without a semi-colon" do
    input = "&nLl"
    output = [["Character", "&nLl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nLt without a semi-colon" do
    input = "&nLt"
    output = [["Character", "&nLt"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nLtv without a semi-colon" do
    input = "&nLtv"
    output = [["Character", "&nLtv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nRightarrow without a semi-colon" do
    input = "&nRightarrow"
    output = [["Character", "&nRightarrow"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nVDash without a semi-colon" do
    input = "&nVDash"
    output = [["Character", "&nVDash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nVdash without a semi-colon" do
    input = "&nVdash"
    output = [["Character", "&nVdash"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nabla without a semi-colon" do
    input = "&nabla"
    output = [["Character", "&nabla"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nacute without a semi-colon" do
    input = "&nacute"
    output = [["Character", "&nacute"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nang without a semi-colon" do
    input = "&nang"
    output = [["Character", "&nang"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nap without a semi-colon" do
    input = "&nap"
    output = [["Character", "&nap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: napE without a semi-colon" do
    input = "&napE"
    output = [["Character", "&napE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: napid without a semi-colon" do
    input = "&napid"
    output = [["Character", "&napid"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: napos without a semi-colon" do
    input = "&napos"
    output = [["Character", "&napos"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: napprox without a semi-colon" do
    input = "&napprox"
    output = [["Character", "&napprox"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: natur without a semi-colon" do
    input = "&natur"
    output = [["Character", "&natur"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: natural without a semi-colon" do
    input = "&natural"
    output = [["Character", "&natural"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: naturals without a semi-colon" do
    input = "&naturals"
    output = [["Character", "&naturals"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nbump without a semi-colon" do
    input = "&nbump"
    output = [["Character", "&nbump"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: nbumpe without a semi-colon" do
    input = "&nbumpe"
    output = [["Character", "&nbumpe"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ncap without a semi-colon" do
    input = "&ncap"
    output = [["Character", "&ncap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ncaron without a semi-colon" do
    input = "&ncaron"
    output = [["Character", "&ncaron"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
