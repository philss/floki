defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart29Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: capand; with a semi-colon" do
    input = "&capand;"
    output = [["Character", "â©„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: capbrcup; with a semi-colon" do
    input = "&capbrcup;"
    output = [["Character", "â©‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: capcap; with a semi-colon" do
    input = "&capcap;"
    output = [["Character", "â©‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: capcup; with a semi-colon" do
    input = "&capcup;"
    output = [["Character", "â©‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: capdot; with a semi-colon" do
    input = "&capdot;"
    output = [["Character", "â©€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: caps; with a semi-colon" do
    input = "&caps;"
    output = [["Character", "âˆ©ï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: caret; with a semi-colon" do
    input = "&caret;"
    output = [["Character", "â"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: caron; with a semi-colon" do
    input = "&caron;"
    output = [["Character", "Ë‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccaps; with a semi-colon" do
    input = "&ccaps;"
    output = [["Character", "â©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccaron; with a semi-colon" do
    input = "&ccaron;"
    output = [["Character", "Ä"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccedil without a semi-colon" do
    input = "&ccedil"
    output = [["Character", "Ã§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccedil; with a semi-colon" do
    input = "&ccedil;"
    output = [["Character", "Ã§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccirc; with a semi-colon" do
    input = "&ccirc;"
    output = [["Character", "Ä‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccups; with a semi-colon" do
    input = "&ccups;"
    output = [["Character", "â©Œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccupssm; with a semi-colon" do
    input = "&ccupssm;"
    output = [["Character", "â©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cdot; with a semi-colon" do
    input = "&cdot;"
    output = [["Character", "Ä‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cedil without a semi-colon" do
    input = "&cedil"
    output = [["Character", "Â¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cedil; with a semi-colon" do
    input = "&cedil;"
    output = [["Character", "Â¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cemptyv; with a semi-colon" do
    input = "&cemptyv;"
    output = [["Character", "â¦²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cent without a semi-colon" do
    input = "&cent"
    output = [["Character", "Â¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cent; with a semi-colon" do
    input = "&cent;"
    output = [["Character", "Â¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: centerdot; with a semi-colon" do
    input = "&centerdot;"
    output = [["Character", "Â·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cfr; with a semi-colon" do
    input = "&cfr;"
    output = [["Character", "ð” "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: chcy; with a semi-colon" do
    input = "&chcy;"
    output = [["Character", "Ñ‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: check; with a semi-colon" do
    input = "&check;"
    output = [["Character", "âœ“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: checkmark; with a semi-colon" do
    input = "&checkmark;"
    output = [["Character", "âœ“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: chi; with a semi-colon" do
    input = "&chi;"
    output = [["Character", "Ï‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cir; with a semi-colon" do
    input = "&cir;"
    output = [["Character", "â—‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cirE; with a semi-colon" do
    input = "&cirE;"
    output = [["Character", "â§ƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circ; with a semi-colon" do
    input = "&circ;"
    output = [["Character", "Ë†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circeq; with a semi-colon" do
    input = "&circeq;"
    output = [["Character", "â‰—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circlearrowleft; with a semi-colon" do
    input = "&circlearrowleft;"
    output = [["Character", "â†º"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circlearrowright; with a semi-colon" do
    input = "&circlearrowright;"
    output = [["Character", "â†»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circledR; with a semi-colon" do
    input = "&circledR;"
    output = [["Character", "Â®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circledS; with a semi-colon" do
    input = "&circledS;"
    output = [["Character", "â“ˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circledast; with a semi-colon" do
    input = "&circledast;"
    output = [["Character", "âŠ›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circledcirc; with a semi-colon" do
    input = "&circledcirc;"
    output = [["Character", "âŠš"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circleddash; with a semi-colon" do
    input = "&circleddash;"
    output = [["Character", "âŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cire; with a semi-colon" do
    input = "&cire;"
    output = [["Character", "â‰—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cirfnint; with a semi-colon" do
    input = "&cirfnint;"
    output = [["Character", "â¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cirmid; with a semi-colon" do
    input = "&cirmid;"
    output = [["Character", "â«¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cirscir; with a semi-colon" do
    input = "&cirscir;"
    output = [["Character", "â§‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: clubs; with a semi-colon" do
    input = "&clubs;"
    output = [["Character", "â™£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: clubsuit; with a semi-colon" do
    input = "&clubsuit;"
    output = [["Character", "â™£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: colon; with a semi-colon" do
    input = "&colon;"
    output = [["Character", ":"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: colone; with a semi-colon" do
    input = "&colone;"
    output = [["Character", "â‰”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: coloneq; with a semi-colon" do
    input = "&coloneq;"
    output = [["Character", "â‰”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: comma; with a semi-colon" do
    input = "&comma;"
    output = [["Character", ","]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: commat; with a semi-colon" do
    input = "&commat;"
    output = [["Character", "@"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: comp; with a semi-colon" do
    input = "&comp;"
    output = [["Character", "âˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: compfn; with a semi-colon" do
    input = "&compfn;"
    output = [["Character", "âˆ˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: complement; with a semi-colon" do
    input = "&complement;"
    output = [["Character", "âˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: complexes; with a semi-colon" do
    input = "&complexes;"
    output = [["Character", "â„‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cong; with a semi-colon" do
    input = "&cong;"
    output = [["Character", "â‰…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: congdot; with a semi-colon" do
    input = "&congdot;"
    output = [["Character", "â©­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: conint; with a semi-colon" do
    input = "&conint;"
    output = [["Character", "âˆ®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: copf; with a semi-colon" do
    input = "&copf;"
    output = [["Character", "ð•”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: coprod; with a semi-colon" do
    input = "&coprod;"
    output = [["Character", "âˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: copy without a semi-colon" do
    input = "&copy"
    output = [["Character", "Â©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: copy; with a semi-colon" do
    input = "&copy;"
    output = [["Character", "Â©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: copysr; with a semi-colon" do
    input = "&copysr;"
    output = [["Character", "â„—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: crarr; with a semi-colon" do
    input = "&crarr;"
    output = [["Character", "â†µ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cross; with a semi-colon" do
    input = "&cross;"
    output = [["Character", "âœ—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cscr; with a semi-colon" do
    input = "&cscr;"
    output = [["Character", "ð’¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: csub; with a semi-colon" do
    input = "&csub;"
    output = [["Character", "â«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: csube; with a semi-colon" do
    input = "&csube;"
    output = [["Character", "â«‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: csup; with a semi-colon" do
    input = "&csup;"
    output = [["Character", "â«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: csupe; with a semi-colon" do
    input = "&csupe;"
    output = [["Character", "â«’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ctdot; with a semi-colon" do
    input = "&ctdot;"
    output = [["Character", "â‹¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cudarrl; with a semi-colon" do
    input = "&cudarrl;"
    output = [["Character", "â¤¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cudarrr; with a semi-colon" do
    input = "&cudarrr;"
    output = [["Character", "â¤µ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cuepr; with a semi-colon" do
    input = "&cuepr;"
    output = [["Character", "â‹ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cuesc; with a semi-colon" do
    input = "&cuesc;"
    output = [["Character", "â‹Ÿ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cularr; with a semi-colon" do
    input = "&cularr;"
    output = [["Character", "â†¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cularrp; with a semi-colon" do
    input = "&cularrp;"
    output = [["Character", "â¤½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cup; with a semi-colon" do
    input = "&cup;"
    output = [["Character", "âˆª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupbrcap; with a semi-colon" do
    input = "&cupbrcap;"
    output = [["Character", "â©ˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupcap; with a semi-colon" do
    input = "&cupcap;"
    output = [["Character", "â©†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupcup; with a semi-colon" do
    input = "&cupcup;"
    output = [["Character", "â©Š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupdot; with a semi-colon" do
    input = "&cupdot;"
    output = [["Character", "âŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupor; with a semi-colon" do
    input = "&cupor;"
    output = [["Character", "â©…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cups; with a semi-colon" do
    input = "&cups;"
    output = [["Character", "âˆªï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curarr; with a semi-colon" do
    input = "&curarr;"
    output = [["Character", "â†·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curarrm; with a semi-colon" do
    input = "&curarrm;"
    output = [["Character", "â¤¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curlyeqprec; with a semi-colon" do
    input = "&curlyeqprec;"
    output = [["Character", "â‹ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curlyeqsucc; with a semi-colon" do
    input = "&curlyeqsucc;"
    output = [["Character", "â‹Ÿ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curlyvee; with a semi-colon" do
    input = "&curlyvee;"
    output = [["Character", "â‹Ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curlywedge; with a semi-colon" do
    input = "&curlywedge;"
    output = [["Character", "â‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curren without a semi-colon" do
    input = "&curren"
    output = [["Character", "Â¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curren; with a semi-colon" do
    input = "&curren;"
    output = [["Character", "Â¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curvearrowleft; with a semi-colon" do
    input = "&curvearrowleft;"
    output = [["Character", "â†¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curvearrowright; with a semi-colon" do
    input = "&curvearrowright;"
    output = [["Character", "â†·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cuvee; with a semi-colon" do
    input = "&cuvee;"
    output = [["Character", "â‹Ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cuwed; with a semi-colon" do
    input = "&cuwed;"
    output = [["Character", "â‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cwconint; with a semi-colon" do
    input = "&cwconint;"
    output = [["Character", "âˆ²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cwint; with a semi-colon" do
    input = "&cwint;"
    output = [["Character", "âˆ±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cylcty; with a semi-colon" do
    input = "&cylcty;"
    output = [["Character", "âŒ­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dArr; with a semi-colon" do
    input = "&dArr;"
    output = [["Character", "â‡“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dHar; with a semi-colon" do
    input = "&dHar;"
    output = [["Character", "â¥¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dagger; with a semi-colon" do
    input = "&dagger;"
    output = [["Character", "â€ "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
