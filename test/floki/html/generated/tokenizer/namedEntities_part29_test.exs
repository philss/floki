defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart29Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: capand; with a semi-colon" do
    input = "&capand;"
    output = [["Character", "⩄"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: capbrcup; with a semi-colon" do
    input = "&capbrcup;"
    output = [["Character", "⩉"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: capcap; with a semi-colon" do
    input = "&capcap;"
    output = [["Character", "⩋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: capcup; with a semi-colon" do
    input = "&capcup;"
    output = [["Character", "⩇"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: capdot; with a semi-colon" do
    input = "&capdot;"
    output = [["Character", "⩀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: caps; with a semi-colon" do
    input = "&caps;"
    output = [["Character", "∩︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: caret; with a semi-colon" do
    input = "&caret;"
    output = [["Character", "⁁"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: caron; with a semi-colon" do
    input = "&caron;"
    output = [["Character", "ˇ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccaps; with a semi-colon" do
    input = "&ccaps;"
    output = [["Character", "⩍"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccaron; with a semi-colon" do
    input = "&ccaron;"
    output = [["Character", "č"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccedil without a semi-colon" do
    input = "&ccedil"
    output = [["Character", "ç"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccedil; with a semi-colon" do
    input = "&ccedil;"
    output = [["Character", "ç"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccirc; with a semi-colon" do
    input = "&ccirc;"
    output = [["Character", "ĉ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccups; with a semi-colon" do
    input = "&ccups;"
    output = [["Character", "⩌"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ccupssm; with a semi-colon" do
    input = "&ccupssm;"
    output = [["Character", "⩐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cdot; with a semi-colon" do
    input = "&cdot;"
    output = [["Character", "ċ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cedil without a semi-colon" do
    input = "&cedil"
    output = [["Character", "¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cedil; with a semi-colon" do
    input = "&cedil;"
    output = [["Character", "¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cemptyv; with a semi-colon" do
    input = "&cemptyv;"
    output = [["Character", "⦲"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cent without a semi-colon" do
    input = "&cent"
    output = [["Character", "¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cent; with a semi-colon" do
    input = "&cent;"
    output = [["Character", "¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: centerdot; with a semi-colon" do
    input = "&centerdot;"
    output = [["Character", "·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cfr; with a semi-colon" do
    input = "&cfr;"
    output = [["Character", "𝔠"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: chcy; with a semi-colon" do
    input = "&chcy;"
    output = [["Character", "ч"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: check; with a semi-colon" do
    input = "&check;"
    output = [["Character", "✓"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: checkmark; with a semi-colon" do
    input = "&checkmark;"
    output = [["Character", "✓"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: chi; with a semi-colon" do
    input = "&chi;"
    output = [["Character", "χ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cir; with a semi-colon" do
    input = "&cir;"
    output = [["Character", "○"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cirE; with a semi-colon" do
    input = "&cirE;"
    output = [["Character", "⧃"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circ; with a semi-colon" do
    input = "&circ;"
    output = [["Character", "ˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circeq; with a semi-colon" do
    input = "&circeq;"
    output = [["Character", "≗"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circlearrowleft; with a semi-colon" do
    input = "&circlearrowleft;"
    output = [["Character", "↺"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circlearrowright; with a semi-colon" do
    input = "&circlearrowright;"
    output = [["Character", "↻"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circledR; with a semi-colon" do
    input = "&circledR;"
    output = [["Character", "®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circledS; with a semi-colon" do
    input = "&circledS;"
    output = [["Character", "Ⓢ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circledast; with a semi-colon" do
    input = "&circledast;"
    output = [["Character", "⊛"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circledcirc; with a semi-colon" do
    input = "&circledcirc;"
    output = [["Character", "⊚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: circleddash; with a semi-colon" do
    input = "&circleddash;"
    output = [["Character", "⊝"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cire; with a semi-colon" do
    input = "&cire;"
    output = [["Character", "≗"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cirfnint; with a semi-colon" do
    input = "&cirfnint;"
    output = [["Character", "⨐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cirmid; with a semi-colon" do
    input = "&cirmid;"
    output = [["Character", "⫯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cirscir; with a semi-colon" do
    input = "&cirscir;"
    output = [["Character", "⧂"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: clubs; with a semi-colon" do
    input = "&clubs;"
    output = [["Character", "♣"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: clubsuit; with a semi-colon" do
    input = "&clubsuit;"
    output = [["Character", "♣"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: colon; with a semi-colon" do
    input = "&colon;"
    output = [["Character", ":"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: colone; with a semi-colon" do
    input = "&colone;"
    output = [["Character", "≔"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: coloneq; with a semi-colon" do
    input = "&coloneq;"
    output = [["Character", "≔"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: comma; with a semi-colon" do
    input = "&comma;"
    output = [["Character", ","]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: commat; with a semi-colon" do
    input = "&commat;"
    output = [["Character", "@"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: comp; with a semi-colon" do
    input = "&comp;"
    output = [["Character", "∁"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: compfn; with a semi-colon" do
    input = "&compfn;"
    output = [["Character", "∘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: complement; with a semi-colon" do
    input = "&complement;"
    output = [["Character", "∁"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: complexes; with a semi-colon" do
    input = "&complexes;"
    output = [["Character", "ℂ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cong; with a semi-colon" do
    input = "&cong;"
    output = [["Character", "≅"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: congdot; with a semi-colon" do
    input = "&congdot;"
    output = [["Character", "⩭"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: conint; with a semi-colon" do
    input = "&conint;"
    output = [["Character", "∮"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: copf; with a semi-colon" do
    input = "&copf;"
    output = [["Character", "𝕔"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: coprod; with a semi-colon" do
    input = "&coprod;"
    output = [["Character", "∐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: copy without a semi-colon" do
    input = "&copy"
    output = [["Character", "©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: copy; with a semi-colon" do
    input = "&copy;"
    output = [["Character", "©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: copysr; with a semi-colon" do
    input = "&copysr;"
    output = [["Character", "℗"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: crarr; with a semi-colon" do
    input = "&crarr;"
    output = [["Character", "↵"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cross; with a semi-colon" do
    input = "&cross;"
    output = [["Character", "✗"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cscr; with a semi-colon" do
    input = "&cscr;"
    output = [["Character", "𝒸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: csub; with a semi-colon" do
    input = "&csub;"
    output = [["Character", "⫏"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: csube; with a semi-colon" do
    input = "&csube;"
    output = [["Character", "⫑"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: csup; with a semi-colon" do
    input = "&csup;"
    output = [["Character", "⫐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: csupe; with a semi-colon" do
    input = "&csupe;"
    output = [["Character", "⫒"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ctdot; with a semi-colon" do
    input = "&ctdot;"
    output = [["Character", "⋯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cudarrl; with a semi-colon" do
    input = "&cudarrl;"
    output = [["Character", "⤸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cudarrr; with a semi-colon" do
    input = "&cudarrr;"
    output = [["Character", "⤵"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cuepr; with a semi-colon" do
    input = "&cuepr;"
    output = [["Character", "⋞"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cuesc; with a semi-colon" do
    input = "&cuesc;"
    output = [["Character", "⋟"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cularr; with a semi-colon" do
    input = "&cularr;"
    output = [["Character", "↶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cularrp; with a semi-colon" do
    input = "&cularrp;"
    output = [["Character", "⤽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cup; with a semi-colon" do
    input = "&cup;"
    output = [["Character", "∪"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupbrcap; with a semi-colon" do
    input = "&cupbrcap;"
    output = [["Character", "⩈"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupcap; with a semi-colon" do
    input = "&cupcap;"
    output = [["Character", "⩆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupcup; with a semi-colon" do
    input = "&cupcup;"
    output = [["Character", "⩊"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupdot; with a semi-colon" do
    input = "&cupdot;"
    output = [["Character", "⊍"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cupor; with a semi-colon" do
    input = "&cupor;"
    output = [["Character", "⩅"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cups; with a semi-colon" do
    input = "&cups;"
    output = [["Character", "∪︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curarr; with a semi-colon" do
    input = "&curarr;"
    output = [["Character", "↷"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curarrm; with a semi-colon" do
    input = "&curarrm;"
    output = [["Character", "⤼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curlyeqprec; with a semi-colon" do
    input = "&curlyeqprec;"
    output = [["Character", "⋞"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curlyeqsucc; with a semi-colon" do
    input = "&curlyeqsucc;"
    output = [["Character", "⋟"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curlyvee; with a semi-colon" do
    input = "&curlyvee;"
    output = [["Character", "⋎"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curlywedge; with a semi-colon" do
    input = "&curlywedge;"
    output = [["Character", "⋏"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curren without a semi-colon" do
    input = "&curren"
    output = [["Character", "¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curren; with a semi-colon" do
    input = "&curren;"
    output = [["Character", "¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curvearrowleft; with a semi-colon" do
    input = "&curvearrowleft;"
    output = [["Character", "↶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: curvearrowright; with a semi-colon" do
    input = "&curvearrowright;"
    output = [["Character", "↷"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cuvee; with a semi-colon" do
    input = "&cuvee;"
    output = [["Character", "⋎"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cuwed; with a semi-colon" do
    input = "&cuwed;"
    output = [["Character", "⋏"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cwconint; with a semi-colon" do
    input = "&cwconint;"
    output = [["Character", "∲"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cwint; with a semi-colon" do
    input = "&cwint;"
    output = [["Character", "∱"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: cylcty; with a semi-colon" do
    input = "&cylcty;"
    output = [["Character", "⌭"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dArr; with a semi-colon" do
    input = "&dArr;"
    output = [["Character", "⇓"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dHar; with a semi-colon" do
    input = "&dHar;"
    output = [["Character", "⥥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: dagger; with a semi-colon" do
    input = "&dagger;"
    output = [["Character", "†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
