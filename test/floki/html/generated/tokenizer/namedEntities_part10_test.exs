defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart10Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Bad named entity: emsp14 without a semi-colon" do
    input = "&emsp14"
    output = [["Character", "&emsp14"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eng without a semi-colon" do
    input = "&eng"
    output = [["Character", "&eng"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ensp without a semi-colon" do
    input = "&ensp"
    output = [["Character", "&ensp"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eogon without a semi-colon" do
    input = "&eogon"
    output = [["Character", "&eogon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eopf without a semi-colon" do
    input = "&eopf"
    output = [["Character", "&eopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: epar without a semi-colon" do
    input = "&epar"
    output = [["Character", "&epar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eparsl without a semi-colon" do
    input = "&eparsl"
    output = [["Character", "&eparsl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eplus without a semi-colon" do
    input = "&eplus"
    output = [["Character", "&eplus"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: epsi without a semi-colon" do
    input = "&epsi"
    output = [["Character", "&epsi"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: epsilon without a semi-colon" do
    input = "&epsilon"
    output = [["Character", "&epsilon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: epsiv without a semi-colon" do
    input = "&epsiv"
    output = [["Character", "&epsiv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eqcirc without a semi-colon" do
    input = "&eqcirc"
    output = [["Character", "&eqcirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eqcolon without a semi-colon" do
    input = "&eqcolon"
    output = [["Character", "&eqcolon"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eqsim without a semi-colon" do
    input = "&eqsim"
    output = [["Character", "&eqsim"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eqslantgtr without a semi-colon" do
    input = "&eqslantgtr"
    output = [["Character", "&eqslantgtr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eqslantless without a semi-colon" do
    input = "&eqslantless"
    output = [["Character", "&eqslantless"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: equals without a semi-colon" do
    input = "&equals"
    output = [["Character", "&equals"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: equest without a semi-colon" do
    input = "&equest"
    output = [["Character", "&equest"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: equiv without a semi-colon" do
    input = "&equiv"
    output = [["Character", "&equiv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: equivDD without a semi-colon" do
    input = "&equivDD"
    output = [["Character", "&equivDD"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eqvparsl without a semi-colon" do
    input = "&eqvparsl"
    output = [["Character", "&eqvparsl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: erDot without a semi-colon" do
    input = "&erDot"
    output = [["Character", "&erDot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: erarr without a semi-colon" do
    input = "&erarr"
    output = [["Character", "&erarr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: escr without a semi-colon" do
    input = "&escr"
    output = [["Character", "&escr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: esdot without a semi-colon" do
    input = "&esdot"
    output = [["Character", "&esdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: esim without a semi-colon" do
    input = "&esim"
    output = [["Character", "&esim"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: eta without a semi-colon" do
    input = "&eta"
    output = [["Character", "&eta"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: euro without a semi-colon" do
    input = "&euro"
    output = [["Character", "&euro"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: excl without a semi-colon" do
    input = "&excl"
    output = [["Character", "&excl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: exist without a semi-colon" do
    input = "&exist"
    output = [["Character", "&exist"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: expectation without a semi-colon" do
    input = "&expectation"
    output = [["Character", "&expectation"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: exponentiale without a semi-colon" do
    input = "&exponentiale"
    output = [["Character", "&exponentiale"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fallingdotseq without a semi-colon" do
    input = "&fallingdotseq"
    output = [["Character", "&fallingdotseq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fcy without a semi-colon" do
    input = "&fcy"
    output = [["Character", "&fcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: female without a semi-colon" do
    input = "&female"
    output = [["Character", "&female"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ffilig without a semi-colon" do
    input = "&ffilig"
    output = [["Character", "&ffilig"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fflig without a semi-colon" do
    input = "&fflig"
    output = [["Character", "&fflig"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ffllig without a semi-colon" do
    input = "&ffllig"
    output = [["Character", "&ffllig"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ffr without a semi-colon" do
    input = "&ffr"
    output = [["Character", "&ffr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: filig without a semi-colon" do
    input = "&filig"
    output = [["Character", "&filig"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fjlig without a semi-colon" do
    input = "&fjlig"
    output = [["Character", "&fjlig"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: flat without a semi-colon" do
    input = "&flat"
    output = [["Character", "&flat"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fllig without a semi-colon" do
    input = "&fllig"
    output = [["Character", "&fllig"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fltns without a semi-colon" do
    input = "&fltns"
    output = [["Character", "&fltns"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fnof without a semi-colon" do
    input = "&fnof"
    output = [["Character", "&fnof"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fopf without a semi-colon" do
    input = "&fopf"
    output = [["Character", "&fopf"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: forall without a semi-colon" do
    input = "&forall"
    output = [["Character", "&forall"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fork without a semi-colon" do
    input = "&fork"
    output = [["Character", "&fork"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: forkv without a semi-colon" do
    input = "&forkv"
    output = [["Character", "&forkv"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fpartint without a semi-colon" do
    input = "&fpartint"
    output = [["Character", "&fpartint"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac13 without a semi-colon" do
    input = "&frac13"
    output = [["Character", "&frac13"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac15 without a semi-colon" do
    input = "&frac15"
    output = [["Character", "&frac15"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac16 without a semi-colon" do
    input = "&frac16"
    output = [["Character", "&frac16"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac18 without a semi-colon" do
    input = "&frac18"
    output = [["Character", "&frac18"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac23 without a semi-colon" do
    input = "&frac23"
    output = [["Character", "&frac23"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac25 without a semi-colon" do
    input = "&frac25"
    output = [["Character", "&frac25"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac35 without a semi-colon" do
    input = "&frac35"
    output = [["Character", "&frac35"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac38 without a semi-colon" do
    input = "&frac38"
    output = [["Character", "&frac38"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac45 without a semi-colon" do
    input = "&frac45"
    output = [["Character", "&frac45"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac56 without a semi-colon" do
    input = "&frac56"
    output = [["Character", "&frac56"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac58 without a semi-colon" do
    input = "&frac58"
    output = [["Character", "&frac58"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frac78 without a semi-colon" do
    input = "&frac78"
    output = [["Character", "&frac78"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frasl without a semi-colon" do
    input = "&frasl"
    output = [["Character", "&frasl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: frown without a semi-colon" do
    input = "&frown"
    output = [["Character", "&frown"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: fscr without a semi-colon" do
    input = "&fscr"
    output = [["Character", "&fscr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gE without a semi-colon" do
    input = "&gE"
    output = [["Character", "&gE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gEl without a semi-colon" do
    input = "&gEl"
    output = [["Character", "&gEl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gacute without a semi-colon" do
    input = "&gacute"
    output = [["Character", "&gacute"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gamma without a semi-colon" do
    input = "&gamma"
    output = [["Character", "&gamma"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gammad without a semi-colon" do
    input = "&gammad"
    output = [["Character", "&gammad"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gap without a semi-colon" do
    input = "&gap"
    output = [["Character", "&gap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gbreve without a semi-colon" do
    input = "&gbreve"
    output = [["Character", "&gbreve"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gcirc without a semi-colon" do
    input = "&gcirc"
    output = [["Character", "&gcirc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gcy without a semi-colon" do
    input = "&gcy"
    output = [["Character", "&gcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gdot without a semi-colon" do
    input = "&gdot"
    output = [["Character", "&gdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ge without a semi-colon" do
    input = "&ge"
    output = [["Character", "&ge"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gel without a semi-colon" do
    input = "&gel"
    output = [["Character", "&gel"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: geq without a semi-colon" do
    input = "&geq"
    output = [["Character", "&geq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: geqq without a semi-colon" do
    input = "&geqq"
    output = [["Character", "&geqq"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: geqslant without a semi-colon" do
    input = "&geqslant"
    output = [["Character", "&geqslant"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ges without a semi-colon" do
    input = "&ges"
    output = [["Character", "&ges"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gescc without a semi-colon" do
    input = "&gescc"
    output = [["Character", "&gescc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gesdot without a semi-colon" do
    input = "&gesdot"
    output = [["Character", "&gesdot"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gesdoto without a semi-colon" do
    input = "&gesdoto"
    output = [["Character", "&gesdoto"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gesdotol without a semi-colon" do
    input = "&gesdotol"
    output = [["Character", "&gesdotol"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gesl without a semi-colon" do
    input = "&gesl"
    output = [["Character", "&gesl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gesles without a semi-colon" do
    input = "&gesles"
    output = [["Character", "&gesles"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gfr without a semi-colon" do
    input = "&gfr"
    output = [["Character", "&gfr"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gg without a semi-colon" do
    input = "&gg"
    output = [["Character", "&gg"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: ggg without a semi-colon" do
    input = "&ggg"
    output = [["Character", "&ggg"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gimel without a semi-colon" do
    input = "&gimel"
    output = [["Character", "&gimel"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gjcy without a semi-colon" do
    input = "&gjcy"
    output = [["Character", "&gjcy"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gl without a semi-colon" do
    input = "&gl"
    output = [["Character", "&gl"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: glE without a semi-colon" do
    input = "&glE"
    output = [["Character", "&glE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gla without a semi-colon" do
    input = "&gla"
    output = [["Character", "&gla"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: glj without a semi-colon" do
    input = "&glj"
    output = [["Character", "&glj"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gnE without a semi-colon" do
    input = "&gnE"
    output = [["Character", "&gnE"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gnap without a semi-colon" do
    input = "&gnap"
    output = [["Character", "&gnap"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gnapprox without a semi-colon" do
    input = "&gnapprox"
    output = [["Character", "&gnapprox"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Bad named entity: gne without a semi-colon" do
    input = "&gne"
    output = [["Character", "&gne"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
