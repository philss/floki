defmodule Floki.HTML.Generated.Tokenizer.UnicodecharsPart3Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests unicodeChars.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Valid Unicode character U+00AA" do
    input = "Âª"
    output = [["Character", "Âª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00AB" do
    input = "Â«"
    output = [["Character", "Â«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00AC" do
    input = "Â¬"
    output = [["Character", "Â¬"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00AD" do
    input = "Â­"
    output = [["Character", "Â­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00AE" do
    input = "Â®"
    output = [["Character", "Â®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00AF" do
    input = "Â¯"
    output = [["Character", "Â¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B0" do
    input = "Â°"
    output = [["Character", "Â°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B1" do
    input = "Â±"
    output = [["Character", "Â±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B2" do
    input = "Â²"
    output = [["Character", "Â²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B3" do
    input = "Â³"
    output = [["Character", "Â³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B4" do
    input = "Â´"
    output = [["Character", "Â´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B5" do
    input = "Âµ"
    output = [["Character", "Âµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B6" do
    input = "Â¶"
    output = [["Character", "Â¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B7" do
    input = "Â·"
    output = [["Character", "Â·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B8" do
    input = "Â¸"
    output = [["Character", "Â¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00B9" do
    input = "Â¹"
    output = [["Character", "Â¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00BA" do
    input = "Âº"
    output = [["Character", "Âº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00BB" do
    input = "Â»"
    output = [["Character", "Â»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00BC" do
    input = "Â¼"
    output = [["Character", "Â¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00BD" do
    input = "Â½"
    output = [["Character", "Â½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00BE" do
    input = "Â¾"
    output = [["Character", "Â¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00BF" do
    input = "Â¿"
    output = [["Character", "Â¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C0" do
    input = "Ã€"
    output = [["Character", "Ã€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C1" do
    input = "Ã"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C2" do
    input = "Ã‚"
    output = [["Character", "Ã‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C3" do
    input = "Ãƒ"
    output = [["Character", "Ãƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C4" do
    input = "Ã„"
    output = [["Character", "Ã„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C5" do
    input = "Ã…"
    output = [["Character", "Ã…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C6" do
    input = "Ã†"
    output = [["Character", "Ã†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C7" do
    input = "Ã‡"
    output = [["Character", "Ã‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C8" do
    input = "Ãˆ"
    output = [["Character", "Ãˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00C9" do
    input = "Ã‰"
    output = [["Character", "Ã‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00CA" do
    input = "ÃŠ"
    output = [["Character", "ÃŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00CB" do
    input = "Ã‹"
    output = [["Character", "Ã‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00CC" do
    input = "ÃŒ"
    output = [["Character", "ÃŒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00CD" do
    input = "Ã"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00CE" do
    input = "ÃŽ"
    output = [["Character", "ÃŽ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00CF" do
    input = "Ã"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D0" do
    input = "Ã"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D1" do
    input = "Ã‘"
    output = [["Character", "Ã‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D2" do
    input = "Ã’"
    output = [["Character", "Ã’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D3" do
    input = "Ã“"
    output = [["Character", "Ã“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D4" do
    input = "Ã”"
    output = [["Character", "Ã”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D5" do
    input = "Ã•"
    output = [["Character", "Ã•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D6" do
    input = "Ã–"
    output = [["Character", "Ã–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D7" do
    input = "Ã—"
    output = [["Character", "Ã—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D8" do
    input = "Ã˜"
    output = [["Character", "Ã˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00D9" do
    input = "Ã™"
    output = [["Character", "Ã™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00DA" do
    input = "Ãš"
    output = [["Character", "Ãš"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00DB" do
    input = "Ã›"
    output = [["Character", "Ã›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00DC" do
    input = "Ãœ"
    output = [["Character", "Ãœ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00DD" do
    input = "Ã"
    output = [["Character", "Ã"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00DE" do
    input = "Ãž"
    output = [["Character", "Ãž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00DF" do
    input = "ÃŸ"
    output = [["Character", "ÃŸ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E0" do
    input = "Ã "
    output = [["Character", "Ã "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E1" do
    input = "Ã¡"
    output = [["Character", "Ã¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E2" do
    input = "Ã¢"
    output = [["Character", "Ã¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E3" do
    input = "Ã£"
    output = [["Character", "Ã£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E4" do
    input = "Ã¤"
    output = [["Character", "Ã¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E5" do
    input = "Ã¥"
    output = [["Character", "Ã¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E6" do
    input = "Ã¦"
    output = [["Character", "Ã¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E7" do
    input = "Ã§"
    output = [["Character", "Ã§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E8" do
    input = "Ã¨"
    output = [["Character", "Ã¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00E9" do
    input = "Ã©"
    output = [["Character", "Ã©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00EA" do
    input = "Ãª"
    output = [["Character", "Ãª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00EB" do
    input = "Ã«"
    output = [["Character", "Ã«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00EC" do
    input = "Ã¬"
    output = [["Character", "Ã¬"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00ED" do
    input = "Ã­"
    output = [["Character", "Ã­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00EE" do
    input = "Ã®"
    output = [["Character", "Ã®"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00EF" do
    input = "Ã¯"
    output = [["Character", "Ã¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F0" do
    input = "Ã°"
    output = [["Character", "Ã°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F1" do
    input = "Ã±"
    output = [["Character", "Ã±"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F2" do
    input = "Ã²"
    output = [["Character", "Ã²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F3" do
    input = "Ã³"
    output = [["Character", "Ã³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F4" do
    input = "Ã´"
    output = [["Character", "Ã´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F5" do
    input = "Ãµ"
    output = [["Character", "Ãµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F6" do
    input = "Ã¶"
    output = [["Character", "Ã¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F7" do
    input = "Ã·"
    output = [["Character", "Ã·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F8" do
    input = "Ã¸"
    output = [["Character", "Ã¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00F9" do
    input = "Ã¹"
    output = [["Character", "Ã¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00FA" do
    input = "Ãº"
    output = [["Character", "Ãº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00FB" do
    input = "Ã»"
    output = [["Character", "Ã»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00FC" do
    input = "Ã¼"
    output = [["Character", "Ã¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00FD" do
    input = "Ã½"
    output = [["Character", "Ã½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00FE" do
    input = "Ã¾"
    output = [["Character", "Ã¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+00FF" do
    input = "Ã¿"
    output = [["Character", "Ã¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+10000" do
    input = "ð€€"
    output = [["Character", "ð€€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+100000" do
    input = "ô€€€"
    output = [["Character", "ô€€€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+10FFFD" do
    input = "ô¿½"
    output = [["Character", "ô¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+1FFFD" do
    input = "ðŸ¿½"
    output = [["Character", "ðŸ¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+20000" do
    input = "ð €€"
    output = [["Character", "ð €€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+2FFFD" do
    input = "ð¯¿½"
    output = [["Character", "ð¯¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+30000" do
    input = "ð°€€"
    output = [["Character", "ð°€€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+3FFFD" do
    input = "ð¿¿½"
    output = [["Character", "ð¿¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+40000" do
    input = "ñ€€€"
    output = [["Character", "ñ€€€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+4FFFD" do
    input = "ñ¿½"
    output = [["Character", "ñ¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+50000" do
    input = "ñ€€"
    output = [["Character", "ñ€€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+5FFFD" do
    input = "ñŸ¿½"
    output = [["Character", "ñŸ¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+60000" do
    input = "ñ €€"
    output = [["Character", "ñ €€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid Unicode character U+6FFFD" do
    input = "ñ¯¿½"
    output = [["Character", "ñ¯¿½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end