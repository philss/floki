defmodule Floki.HTML.Generated.Tokenizer.NumericentitiesPart4Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests numericEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Valid numeric entity character U+3FFFD" do
    input = "&#x3fffd;"
    output = [["Character", "𿿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+40000" do
    input = "&#x40000;"
    output = [["Character", "񀀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+4FFFD" do
    input = "&#x4fffd;"
    output = [["Character", "񏿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+50000" do
    input = "&#x50000;"
    output = [["Character", "񐀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+5FFFD" do
    input = "&#x5fffd;"
    output = [["Character", "񟿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+60000" do
    input = "&#x60000;"
    output = [["Character", "񠀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+6FFFD" do
    input = "&#x6fffd;"
    output = [["Character", "񯿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+70000" do
    input = "&#x70000;"
    output = [["Character", "񰀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+7FFFD" do
    input = "&#x7fffd;"
    output = [["Character", "񿿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+80000" do
    input = "&#x80000;"
    output = [["Character", "򀀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+8FFFD" do
    input = "&#x8fffd;"
    output = [["Character", "򏿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+90000" do
    input = "&#x90000;"
    output = [["Character", "򐀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+9FFFD" do
    input = "&#x9fffd;"
    output = [["Character", "򟿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+A0000" do
    input = "&#xa0000;"
    output = [["Character", "򠀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+AFFFD" do
    input = "&#xafffd;"
    output = [["Character", "򯿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+B0000" do
    input = "&#xb0000;"
    output = [["Character", "򰀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+BFFFD" do
    input = "&#xbfffd;"
    output = [["Character", "򿿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+C0000" do
    input = "&#xc0000;"
    output = [["Character", "󀀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+CFFFD" do
    input = "&#xcfffd;"
    output = [["Character", "󏿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+D0000" do
    input = "&#xd0000;"
    output = [["Character", "󐀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+D7FF" do
    input = "&#xd7ff;"
    output = [["Character", "퟿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+DFFFD" do
    input = "&#xdfffd;"
    output = [["Character", "󟿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+E000" do
    input = "&#xe000;"
    output = [["Character", ""]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+E0000" do
    input = "&#xe0000;"
    output = [["Character", "󠀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+EFFFD" do
    input = "&#xefffd;"
    output = [["Character", "󯿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+F0000" do
    input = "&#xf0000;"
    output = [["Character", "󰀀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+FDCF" do
    input = "&#xfdcf;"
    output = [["Character", "﷏"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+FDF0" do
    input = "&#xfdf0;"
    output = [["Character", "ﷰ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+FFFD" do
    input = "&#xfffd;"
    output = [["Character", "�"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Valid numeric entity character U+FFFFD" do
    input = "&#xffffd;"
    output = [["Character", "󿿽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
