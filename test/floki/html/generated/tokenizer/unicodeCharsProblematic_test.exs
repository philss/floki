defmodule Floki.HTML.Generated.Tokenizer.UnicodecharsproblematicTest do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests unicodeCharsProblematic.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 CR followed by U+0000" do
    input = <<13, 0>>
    output = [["Character", <<10, 0>>]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Invalid Unicode character U+D800" do
    input = "\\uD800"
    output = [["Character", "\\uD800"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Invalid Unicode character U+D800 with valid following character" do
    input = "\\uD800a"
    output = [["Character", "\\uD800a"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Invalid Unicode character U+DFFF" do
    input = "\\uDFFF"
    output = [["Character", "\\uDFFF"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Invalid Unicode character U+DFFF with valid preceding character" do
    input = "a\\uDFFF"
    output = [["Character", "a\\uDFFF"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
