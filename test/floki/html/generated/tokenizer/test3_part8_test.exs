defmodule Floki.HTML.Generated.Tokenizer.Test3Part8Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests test3.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 <!DOCTYPEa PUBLIC\"?" do
    input = "<!DOCTYPEa PUBLIC\"?"
    output = [["DOCTYPE", "a", "?", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"@" do
    input = "<!DOCTYPEa PUBLIC\"@"
    output = [["DOCTYPE", "a", "@", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"A" do
    input = "<!DOCTYPEa PUBLIC\"A"
    output = [["DOCTYPE", "a", "A", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"B" do
    input = "<!DOCTYPEa PUBLIC\"B"
    output = [["DOCTYPE", "a", "B", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"Y" do
    input = "<!DOCTYPEa PUBLIC\"Y"
    output = [["DOCTYPE", "a", "Y", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"Z" do
    input = "<!DOCTYPEa PUBLIC\"Z"
    output = [["DOCTYPE", "a", "Z", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"\\u0000" do
    input = <<60, 33, 68, 79, 67, 84, 89, 80, 69, 97, 32, 80, 85, 66, 76, 73, 67, 34, 0>>
    output = [["DOCTYPE", "a", "�", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"\\u0009" do
    input = "<!DOCTYPEa PUBLIC\"\t"
    output = [["DOCTYPE", "a", "\t", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"\\u000A" do
    input = "<!DOCTYPEa PUBLIC\"\n"
    output = [["DOCTYPE", "a", "\n", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"\\u000B" do
    input = "<!DOCTYPEa PUBLIC\"\v"
    output = [["DOCTYPE", "a", "\v", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"\\u000C" do
    input = "<!DOCTYPEa PUBLIC\"\f"
    output = [["DOCTYPE", "a", "\f", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"\\uDBC0\\uDC00" do
    input = "<!DOCTYPEa PUBLIC\"􀀀"
    output = [["DOCTYPE", "a", "􀀀", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"`" do
    input = "<!DOCTYPEa PUBLIC\"`"
    output = [["DOCTYPE", "a", "`", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"a" do
    input = "<!DOCTYPEa PUBLIC\"a"
    output = [["DOCTYPE", "a", "a", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"b" do
    input = "<!DOCTYPEa PUBLIC\"b"
    output = [["DOCTYPE", "a", "b", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"y" do
    input = "<!DOCTYPEa PUBLIC\"y"
    output = [["DOCTYPE", "a", "y", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"z" do
    input = "<!DOCTYPEa PUBLIC\"z"
    output = [["DOCTYPE", "a", "z", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC\"{" do
    input = "<!DOCTYPEa PUBLIC\"{"
    output = [["DOCTYPE", "a", "{", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC#" do
    input = "<!DOCTYPEa PUBLIC#"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC&" do
    input = "<!DOCTYPEa PUBLIC&"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'" do
    input = "<!DOCTYPEa PUBLIC'"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC' " do
    input = "<!DOCTYPEa PUBLIC' "
    output = [["DOCTYPE", "a", " ", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'!" do
    input = "<!DOCTYPEa PUBLIC'!"
    output = [["DOCTYPE", "a", "!", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'\"" do
    input = "<!DOCTYPEa PUBLIC'\""
    output = [["DOCTYPE", "a", "\"", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'&" do
    input = "<!DOCTYPEa PUBLIC'&"
    output = [["DOCTYPE", "a", "&", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''" do
    input = "<!DOCTYPEa PUBLIC''"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'' " do
    input = "<!DOCTYPEa PUBLIC'' "
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''!" do
    input = "<!DOCTYPEa PUBLIC''!"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\"" do
    input = "<!DOCTYPEa PUBLIC''\""
    output = [["DOCTYPE", "a", "", "", false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''#" do
    input = "<!DOCTYPEa PUBLIC''#"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''&" do
    input = "<!DOCTYPEa PUBLIC''&"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'''" do
    input = "<!DOCTYPEa PUBLIC'''"
    output = [["DOCTYPE", "a", "", "", false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''(" do
    input = "<!DOCTYPEa PUBLIC''("
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''-" do
    input = "<!DOCTYPEa PUBLIC''-"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''/" do
    input = "<!DOCTYPEa PUBLIC''/"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''0" do
    input = "<!DOCTYPEa PUBLIC''0"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''1" do
    input = "<!DOCTYPEa PUBLIC''1"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''9" do
    input = "<!DOCTYPEa PUBLIC''9"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''<" do
    input = "<!DOCTYPEa PUBLIC''<"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''=" do
    input = "<!DOCTYPEa PUBLIC''="
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''>" do
    input = "<!DOCTYPEa PUBLIC''>"
    output = [["DOCTYPE", "a", "", nil, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''?" do
    input = "<!DOCTYPEa PUBLIC''?"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''@" do
    input = "<!DOCTYPEa PUBLIC''@"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''A" do
    input = "<!DOCTYPEa PUBLIC''A"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''B" do
    input = "<!DOCTYPEa PUBLIC''B"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''Y" do
    input = "<!DOCTYPEa PUBLIC''Y"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''Z" do
    input = "<!DOCTYPEa PUBLIC''Z"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\\u0000" do
    input = <<60, 33, 68, 79, 67, 84, 89, 80, 69, 97, 32, 80, 85, 66, 76, 73, 67, 39, 39, 0>>
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\\u0008" do
    input = "<!DOCTYPEa PUBLIC''\b"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\\u0009" do
    input = "<!DOCTYPEa PUBLIC''\t"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\\u000A" do
    input = "<!DOCTYPEa PUBLIC''\n"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\\u000B" do
    input = "<!DOCTYPEa PUBLIC''\v"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\\u000C" do
    input = "<!DOCTYPEa PUBLIC''\f"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\\u000D" do
    input = "<!DOCTYPEa PUBLIC''\r"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\\u001F" do
    input = <<60, 33, 68, 79, 67, 84, 89, 80, 69, 97, 32, 80, 85, 66, 76, 73, 67, 39, 39, 31>>
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''\\uDBC0\\uDC00" do
    input = "<!DOCTYPEa PUBLIC''􀀀"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''`" do
    input = "<!DOCTYPEa PUBLIC''`"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''a" do
    input = "<!DOCTYPEa PUBLIC''a"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''b" do
    input = "<!DOCTYPEa PUBLIC''b"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''y" do
    input = "<!DOCTYPEa PUBLIC''y"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''z" do
    input = "<!DOCTYPEa PUBLIC''z"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC''{" do
    input = "<!DOCTYPEa PUBLIC''{"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'(" do
    input = "<!DOCTYPEa PUBLIC'("
    output = [["DOCTYPE", "a", "(", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'-" do
    input = "<!DOCTYPEa PUBLIC'-"
    output = [["DOCTYPE", "a", "-", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'/" do
    input = "<!DOCTYPEa PUBLIC'/"
    output = [["DOCTYPE", "a", "/", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'0" do
    input = "<!DOCTYPEa PUBLIC'0"
    output = [["DOCTYPE", "a", "0", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'1" do
    input = "<!DOCTYPEa PUBLIC'1"
    output = [["DOCTYPE", "a", "1", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'9" do
    input = "<!DOCTYPEa PUBLIC'9"
    output = [["DOCTYPE", "a", "9", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'<" do
    input = "<!DOCTYPEa PUBLIC'<"
    output = [["DOCTYPE", "a", "<", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'=" do
    input = "<!DOCTYPEa PUBLIC'="
    output = [["DOCTYPE", "a", "=", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'>" do
    input = "<!DOCTYPEa PUBLIC'>"
    output = [["DOCTYPE", "a", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'?" do
    input = "<!DOCTYPEa PUBLIC'?"
    output = [["DOCTYPE", "a", "?", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'@" do
    input = "<!DOCTYPEa PUBLIC'@"
    output = [["DOCTYPE", "a", "@", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'A" do
    input = "<!DOCTYPEa PUBLIC'A"
    output = [["DOCTYPE", "a", "A", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'B" do
    input = "<!DOCTYPEa PUBLIC'B"
    output = [["DOCTYPE", "a", "B", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'Y" do
    input = "<!DOCTYPEa PUBLIC'Y"
    output = [["DOCTYPE", "a", "Y", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'Z" do
    input = "<!DOCTYPEa PUBLIC'Z"
    output = [["DOCTYPE", "a", "Z", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'\\u0000" do
    input = <<60, 33, 68, 79, 67, 84, 89, 80, 69, 97, 32, 80, 85, 66, 76, 73, 67, 39, 0>>
    output = [["DOCTYPE", "a", "�", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'\\u0009" do
    input = "<!DOCTYPEa PUBLIC'\t"
    output = [["DOCTYPE", "a", "\t", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'\\u000A" do
    input = "<!DOCTYPEa PUBLIC'\n"
    output = [["DOCTYPE", "a", "\n", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'\\u000B" do
    input = "<!DOCTYPEa PUBLIC'\v"
    output = [["DOCTYPE", "a", "\v", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'\\u000C" do
    input = "<!DOCTYPEa PUBLIC'\f"
    output = [["DOCTYPE", "a", "\f", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'\\uDBC0\\uDC00" do
    input = "<!DOCTYPEa PUBLIC'􀀀"
    output = [["DOCTYPE", "a", "􀀀", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'`" do
    input = "<!DOCTYPEa PUBLIC'`"
    output = [["DOCTYPE", "a", "`", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'a" do
    input = "<!DOCTYPEa PUBLIC'a"
    output = [["DOCTYPE", "a", "a", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'b" do
    input = "<!DOCTYPEa PUBLIC'b"
    output = [["DOCTYPE", "a", "b", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'y" do
    input = "<!DOCTYPEa PUBLIC'y"
    output = [["DOCTYPE", "a", "y", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'z" do
    input = "<!DOCTYPEa PUBLIC'z"
    output = [["DOCTYPE", "a", "z", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC'{" do
    input = "<!DOCTYPEa PUBLIC'{"
    output = [["DOCTYPE", "a", "{", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC(" do
    input = "<!DOCTYPEa PUBLIC("
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC-" do
    input = "<!DOCTYPEa PUBLIC-"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC/" do
    input = "<!DOCTYPEa PUBLIC/"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC0" do
    input = "<!DOCTYPEa PUBLIC0"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC1" do
    input = "<!DOCTYPEa PUBLIC1"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC9" do
    input = "<!DOCTYPEa PUBLIC9"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC<" do
    input = "<!DOCTYPEa PUBLIC<"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC=" do
    input = "<!DOCTYPEa PUBLIC="
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC>" do
    input = "<!DOCTYPEa PUBLIC>"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC?" do
    input = "<!DOCTYPEa PUBLIC?"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 <!DOCTYPEa PUBLIC@" do
    input = "<!DOCTYPEa PUBLIC@"
    output = [["DOCTYPE", "a", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end