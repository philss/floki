defmodule Floki.HTML.Generated.Tokenizer.Test2Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests test2.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 A bogus comment stops at >, even if preceeded by two dashes" do
    input = "<?foo-->"
    output = [["Comment", "?foo--"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Comment with dash" do
    input = "<!---x"
    output = [["Comment", "-x"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with > in double-quoted publicId" do
    input = "<!DOCTYPE html PUBLIC \">x"
    output = [["DOCTYPE", "html", "", nil, false], ["Character", "x"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with > in double-quoted systemId" do
    input = "<!DOCTYPE html PUBLIC \"foo\" \">x"
    output = [["DOCTYPE", "html", "foo", "", false], ["Character", "x"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with > in single-quoted publicId" do
    input = "<!DOCTYPE html PUBLIC '>x"
    output = [["DOCTYPE", "html", "", nil, false], ["Character", "x"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with > in single-quoted systemId" do
    input = "<!DOCTYPE html PUBLIC 'foo' '>x"
    output = [["DOCTYPE", "html", "foo", "", false], ["Character", "x"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with EOF after PUBLIC" do
    input = "<!DOCTYPE html PUBLIC"
    output = [["DOCTYPE", "html", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with EOF after PUBLIC '" do
    input = "<!DOCTYPE html PUBLIC '"
    output = [["DOCTYPE", "html", "", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with EOF after PUBLIC 'x" do
    input = "<!DOCTYPE html PUBLIC 'x"
    output = [["DOCTYPE", "html", "x", nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with publicId" do
    input = "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML Transitional 4.01//EN\">"
    output = [["DOCTYPE", "html", "-//W3C//DTD HTML Transitional 4.01//EN", nil, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with publicId and systemId" do
    input =
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML Transitional 4.01//EN\" \"-//W3C//DTD HTML Transitional 4.01//EN\">"

    output = [
      [
        "DOCTYPE",
        "html",
        "-//W3C//DTD HTML Transitional 4.01//EN",
        "-//W3C//DTD HTML Transitional 4.01//EN",
        true
      ]
    ]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE with systemId" do
    input = "<!DOCTYPE html SYSTEM \"-//W3C//DTD HTML Transitional 4.01//EN\">"
    output = [["DOCTYPE", "html", nil, "-//W3C//DTD HTML Transitional 4.01//EN", true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE without name" do
    input = "<!DOCTYPE>"
    output = [["DOCTYPE", nil, nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 DOCTYPE without space before name" do
    input = "<!DOCTYPEhtml>"
    output = [["DOCTYPE", "html", nil, nil, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Double-quote after attribute name" do
    input = "<h a \">"
    output = [["StartTag", "h", %{"\"" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Double-quoted attribute value" do
    input = "<h a=\"b\">"
    output = [["StartTag", "h", %{"a" => "b"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Empty attribute followed by uppercase attribute" do
    input = "<h a B=''>"
    output = [["StartTag", "h", %{"a" => "", "b" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Empty end tag with following characters" do
    input = "a</>bc"
    output = [["Character", "abc"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Empty end tag with following comment" do
    input = "a</><!--b-->c"
    output = [["Character", "a"], ["Comment", "b"], ["Character", "c"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Empty end tag with following end tag" do
    input = "a</></b>c"
    output = [["Character", "a"], ["EndTag", "b"], ["Character", "c"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Empty end tag with following tag" do
    input = "a</><b>c"
    output = [["Character", "a"], ["StartTag", "b", %{}], ["Character", "c"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Entity + newline" do
    input = "\nx\n&gt;\n"
    output = [["Character", "\nx\n>\n"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Entity without a name" do
    input = "&;"
    output = [["Character", "&;"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Hexadecimal entity pair representing a surrogate pair" do
    input = "&#xD869;&#xDED6;"
    output = [["Character", "��"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Hexadecimal entity representing a codepoint after 1114111 (U+10FFFF)" do
    input = "&#x1010FFFF;"
    output = [["Character", "�"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Hexadecimal entity representing the NUL character" do
    input = "&#x0000;"
    output = [["Character", "�"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Hexadecimal entity with mixed uppercase and lowercase" do
    input = "&#xaBcD;"
    output = [["Character", "ꯍ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Illegal end tag name" do
    input = "</1>"
    output = [["Comment", "1"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Incomplete doctype" do
    input = "<!DOCTYPE html "
    output = [["DOCTYPE", "html", nil, nil, false]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Incorrect DOCTYPE without a space before name" do
    input = "<!DOCTYPEfoo>"
    output = [["DOCTYPE", "foo", nil, nil, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Non-void element containing trailing /" do
    input = "<h/>"
    output = [["StartTag", "h", %{}, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Null Byte Replacement" do
    input = <<0>>
    output = [["Character", <<0>>]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Numeric entity representing a codepoint after 1114111 (U+10FFFF)" do
    input = "&#2225222;"
    output = [["Character", "�"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Numeric entity representing the NUL character" do
    input = "&#0000;"
    output = [["Character", "�"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Simili processing instruction" do
    input = "<?namespace>"
    output = [["Comment", "?namespace"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Single-quote after attribute name" do
    input = "<h a '>"
    output = [["StartTag", "h", %{"'" => "", "a" => ""}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Start tag with no attributes but space before the greater-than sign" do
    input = "<h >"
    output = [["StartTag", "h", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 StartTag containing /" do
    input = "<h/a='b'>"
    output = [["StartTag", "h", %{"a" => "b"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 StartTag containing <" do
    input = "<a<b>"
    output = [["StartTag", "a<b", %{}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Unescaped <" do
    input = "foo < bar"
    output = [["Character", "foo < bar"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Unescaped </" do
    input = "</"
    output = [["Character", "</"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Unescaped ampersand in attribute value" do
    input = "<h a='&'>"
    output = [["StartTag", "h", %{"a" => "&"}]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Void element with permitted slash" do
    input = "<br/>"
    output = [["StartTag", "br", %{}, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Void element with permitted slash (with attribute)" do
    input = "<br foo='bar'/>"
    output = [["StartTag", "br", %{"foo" => "bar"}, true]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end