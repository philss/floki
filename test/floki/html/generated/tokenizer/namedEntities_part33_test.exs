defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart33Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: iquest; with a semi-colon" do
    input = "&iquest;"
    output = [["Character", "Â¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: iscr; with a semi-colon" do
    input = "&iscr;"
    output = [["Character", "ð’¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isin; with a semi-colon" do
    input = "&isin;"
    output = [["Character", "âˆˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isinE; with a semi-colon" do
    input = "&isinE;"
    output = [["Character", "â‹¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isindot; with a semi-colon" do
    input = "&isindot;"
    output = [["Character", "â‹µ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isins; with a semi-colon" do
    input = "&isins;"
    output = [["Character", "â‹´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isinsv; with a semi-colon" do
    input = "&isinsv;"
    output = [["Character", "â‹³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isinv; with a semi-colon" do
    input = "&isinv;"
    output = [["Character", "âˆˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: it; with a semi-colon" do
    input = "&it;"
    output = [["Character", "â¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: itilde; with a semi-colon" do
    input = "&itilde;"
    output = [["Character", "Ä©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: iukcy; with a semi-colon" do
    input = "&iukcy;"
    output = [["Character", "Ñ–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: iuml without a semi-colon" do
    input = "&iuml"
    output = [["Character", "Ã¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: iuml; with a semi-colon" do
    input = "&iuml;"
    output = [["Character", "Ã¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jcirc; with a semi-colon" do
    input = "&jcirc;"
    output = [["Character", "Äµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jcy; with a semi-colon" do
    input = "&jcy;"
    output = [["Character", "Ð¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jfr; with a semi-colon" do
    input = "&jfr;"
    output = [["Character", "ð”§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jmath; with a semi-colon" do
    input = "&jmath;"
    output = [["Character", "È·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jopf; with a semi-colon" do
    input = "&jopf;"
    output = [["Character", "ð•›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jscr; with a semi-colon" do
    input = "&jscr;"
    output = [["Character", "ð’¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jsercy; with a semi-colon" do
    input = "&jsercy;"
    output = [["Character", "Ñ˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jukcy; with a semi-colon" do
    input = "&jukcy;"
    output = [["Character", "Ñ”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kappa; with a semi-colon" do
    input = "&kappa;"
    output = [["Character", "Îº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kappav; with a semi-colon" do
    input = "&kappav;"
    output = [["Character", "Ï°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kcedil; with a semi-colon" do
    input = "&kcedil;"
    output = [["Character", "Ä·"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kcy; with a semi-colon" do
    input = "&kcy;"
    output = [["Character", "Ðº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kfr; with a semi-colon" do
    input = "&kfr;"
    output = [["Character", "ð”¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kgreen; with a semi-colon" do
    input = "&kgreen;"
    output = [["Character", "Ä¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: khcy; with a semi-colon" do
    input = "&khcy;"
    output = [["Character", "Ñ…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kjcy; with a semi-colon" do
    input = "&kjcy;"
    output = [["Character", "Ñœ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kopf; with a semi-colon" do
    input = "&kopf;"
    output = [["Character", "ð•œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kscr; with a semi-colon" do
    input = "&kscr;"
    output = [["Character", "ð“€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lAarr; with a semi-colon" do
    input = "&lAarr;"
    output = [["Character", "â‡š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lArr; with a semi-colon" do
    input = "&lArr;"
    output = [["Character", "â‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lAtail; with a semi-colon" do
    input = "&lAtail;"
    output = [["Character", "â¤›"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lBarr; with a semi-colon" do
    input = "&lBarr;"
    output = [["Character", "â¤Ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lE; with a semi-colon" do
    input = "&lE;"
    output = [["Character", "â‰¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lEg; with a semi-colon" do
    input = "&lEg;"
    output = [["Character", "âª‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lHar; with a semi-colon" do
    input = "&lHar;"
    output = [["Character", "â¥¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lacute; with a semi-colon" do
    input = "&lacute;"
    output = [["Character", "Äº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: laemptyv; with a semi-colon" do
    input = "&laemptyv;"
    output = [["Character", "â¦´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lagran; with a semi-colon" do
    input = "&lagran;"
    output = [["Character", "â„’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lambda; with a semi-colon" do
    input = "&lambda;"
    output = [["Character", "Î»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lang; with a semi-colon" do
    input = "&lang;"
    output = [["Character", "âŸ¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: langd; with a semi-colon" do
    input = "&langd;"
    output = [["Character", "â¦‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: langle; with a semi-colon" do
    input = "&langle;"
    output = [["Character", "âŸ¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lap; with a semi-colon" do
    input = "&lap;"
    output = [["Character", "âª…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: laquo without a semi-colon" do
    input = "&laquo"
    output = [["Character", "Â«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: laquo; with a semi-colon" do
    input = "&laquo;"
    output = [["Character", "Â«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larr; with a semi-colon" do
    input = "&larr;"
    output = [["Character", "â†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrb; with a semi-colon" do
    input = "&larrb;"
    output = [["Character", "â‡¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrbfs; with a semi-colon" do
    input = "&larrbfs;"
    output = [["Character", "â¤Ÿ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrfs; with a semi-colon" do
    input = "&larrfs;"
    output = [["Character", "â¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrhk; with a semi-colon" do
    input = "&larrhk;"
    output = [["Character", "â†©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrlp; with a semi-colon" do
    input = "&larrlp;"
    output = [["Character", "â†«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrpl; with a semi-colon" do
    input = "&larrpl;"
    output = [["Character", "â¤¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrsim; with a semi-colon" do
    input = "&larrsim;"
    output = [["Character", "â¥³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrtl; with a semi-colon" do
    input = "&larrtl;"
    output = [["Character", "â†¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lat; with a semi-colon" do
    input = "&lat;"
    output = [["Character", "âª«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: latail; with a semi-colon" do
    input = "&latail;"
    output = [["Character", "â¤™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: late; with a semi-colon" do
    input = "&late;"
    output = [["Character", "âª­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lates; with a semi-colon" do
    input = "&lates;"
    output = [["Character", "âª­ï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbarr; with a semi-colon" do
    input = "&lbarr;"
    output = [["Character", "â¤Œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbbrk; with a semi-colon" do
    input = "&lbbrk;"
    output = [["Character", "â²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrace; with a semi-colon" do
    input = "&lbrace;"
    output = [["Character", "{"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrack; with a semi-colon" do
    input = "&lbrack;"
    output = [["Character", "["]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrke; with a semi-colon" do
    input = "&lbrke;"
    output = [["Character", "â¦‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrksld; with a semi-colon" do
    input = "&lbrksld;"
    output = [["Character", "â¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrkslu; with a semi-colon" do
    input = "&lbrkslu;"
    output = [["Character", "â¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lcaron; with a semi-colon" do
    input = "&lcaron;"
    output = [["Character", "Ä¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lcedil; with a semi-colon" do
    input = "&lcedil;"
    output = [["Character", "Ä¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lceil; with a semi-colon" do
    input = "&lceil;"
    output = [["Character", "âŒˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lcub; with a semi-colon" do
    input = "&lcub;"
    output = [["Character", "{"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lcy; with a semi-colon" do
    input = "&lcy;"
    output = [["Character", "Ð»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldca; with a semi-colon" do
    input = "&ldca;"
    output = [["Character", "â¤¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldquo; with a semi-colon" do
    input = "&ldquo;"
    output = [["Character", "â€œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldquor; with a semi-colon" do
    input = "&ldquor;"
    output = [["Character", "â€ž"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldrdhar; with a semi-colon" do
    input = "&ldrdhar;"
    output = [["Character", "â¥§"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldrushar; with a semi-colon" do
    input = "&ldrushar;"
    output = [["Character", "â¥‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldsh; with a semi-colon" do
    input = "&ldsh;"
    output = [["Character", "â†²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: le; with a semi-colon" do
    input = "&le;"
    output = [["Character", "â‰¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftarrow; with a semi-colon" do
    input = "&leftarrow;"
    output = [["Character", "â†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftarrowtail; with a semi-colon" do
    input = "&leftarrowtail;"
    output = [["Character", "â†¢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftharpoondown; with a semi-colon" do
    input = "&leftharpoondown;"
    output = [["Character", "â†½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftharpoonup; with a semi-colon" do
    input = "&leftharpoonup;"
    output = [["Character", "â†¼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftleftarrows; with a semi-colon" do
    input = "&leftleftarrows;"
    output = [["Character", "â‡‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftrightarrow; with a semi-colon" do
    input = "&leftrightarrow;"
    output = [["Character", "â†”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftrightarrows; with a semi-colon" do
    input = "&leftrightarrows;"
    output = [["Character", "â‡†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftrightharpoons; with a semi-colon" do
    input = "&leftrightharpoons;"
    output = [["Character", "â‡‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftrightsquigarrow; with a semi-colon" do
    input = "&leftrightsquigarrow;"
    output = [["Character", "â†­"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftthreetimes; with a semi-colon" do
    input = "&leftthreetimes;"
    output = [["Character", "â‹‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leg; with a semi-colon" do
    input = "&leg;"
    output = [["Character", "â‹š"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leq; with a semi-colon" do
    input = "&leq;"
    output = [["Character", "â‰¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leqq; with a semi-colon" do
    input = "&leqq;"
    output = [["Character", "â‰¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leqslant; with a semi-colon" do
    input = "&leqslant;"
    output = [["Character", "â©½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: les; with a semi-colon" do
    input = "&les;"
    output = [["Character", "â©½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lescc; with a semi-colon" do
    input = "&lescc;"
    output = [["Character", "âª¨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lesdot; with a semi-colon" do
    input = "&lesdot;"
    output = [["Character", "â©¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lesdoto; with a semi-colon" do
    input = "&lesdoto;"
    output = [["Character", "âª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lesdotor; with a semi-colon" do
    input = "&lesdotor;"
    output = [["Character", "âªƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lesg; with a semi-colon" do
    input = "&lesg;"
    output = [["Character", "â‹šï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
