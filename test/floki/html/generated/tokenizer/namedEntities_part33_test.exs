defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart33Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: iquest; with a semi-colon" do
    input = "&iquest;"
    output = [["Character", "¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: iscr; with a semi-colon" do
    input = "&iscr;"
    output = [["Character", "𝒾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isin; with a semi-colon" do
    input = "&isin;"
    output = [["Character", "∈"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isinE; with a semi-colon" do
    input = "&isinE;"
    output = [["Character", "⋹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isindot; with a semi-colon" do
    input = "&isindot;"
    output = [["Character", "⋵"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isins; with a semi-colon" do
    input = "&isins;"
    output = [["Character", "⋴"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isinsv; with a semi-colon" do
    input = "&isinsv;"
    output = [["Character", "⋳"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: isinv; with a semi-colon" do
    input = "&isinv;"
    output = [["Character", "∈"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: it; with a semi-colon" do
    input = "&it;"
    output = [["Character", "⁢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: itilde; with a semi-colon" do
    input = "&itilde;"
    output = [["Character", "ĩ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: iukcy; with a semi-colon" do
    input = "&iukcy;"
    output = [["Character", "і"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: iuml without a semi-colon" do
    input = "&iuml"
    output = [["Character", "ï"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: iuml; with a semi-colon" do
    input = "&iuml;"
    output = [["Character", "ï"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jcirc; with a semi-colon" do
    input = "&jcirc;"
    output = [["Character", "ĵ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jcy; with a semi-colon" do
    input = "&jcy;"
    output = [["Character", "й"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jfr; with a semi-colon" do
    input = "&jfr;"
    output = [["Character", "𝔧"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jmath; with a semi-colon" do
    input = "&jmath;"
    output = [["Character", "ȷ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jopf; with a semi-colon" do
    input = "&jopf;"
    output = [["Character", "𝕛"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jscr; with a semi-colon" do
    input = "&jscr;"
    output = [["Character", "𝒿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jsercy; with a semi-colon" do
    input = "&jsercy;"
    output = [["Character", "ј"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: jukcy; with a semi-colon" do
    input = "&jukcy;"
    output = [["Character", "є"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kappa; with a semi-colon" do
    input = "&kappa;"
    output = [["Character", "κ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kappav; with a semi-colon" do
    input = "&kappav;"
    output = [["Character", "ϰ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kcedil; with a semi-colon" do
    input = "&kcedil;"
    output = [["Character", "ķ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kcy; with a semi-colon" do
    input = "&kcy;"
    output = [["Character", "к"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kfr; with a semi-colon" do
    input = "&kfr;"
    output = [["Character", "𝔨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kgreen; with a semi-colon" do
    input = "&kgreen;"
    output = [["Character", "ĸ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: khcy; with a semi-colon" do
    input = "&khcy;"
    output = [["Character", "х"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kjcy; with a semi-colon" do
    input = "&kjcy;"
    output = [["Character", "ќ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kopf; with a semi-colon" do
    input = "&kopf;"
    output = [["Character", "𝕜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: kscr; with a semi-colon" do
    input = "&kscr;"
    output = [["Character", "𝓀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lAarr; with a semi-colon" do
    input = "&lAarr;"
    output = [["Character", "⇚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lArr; with a semi-colon" do
    input = "&lArr;"
    output = [["Character", "⇐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lAtail; with a semi-colon" do
    input = "&lAtail;"
    output = [["Character", "⤛"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lBarr; with a semi-colon" do
    input = "&lBarr;"
    output = [["Character", "⤎"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lE; with a semi-colon" do
    input = "&lE;"
    output = [["Character", "≦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lEg; with a semi-colon" do
    input = "&lEg;"
    output = [["Character", "⪋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lHar; with a semi-colon" do
    input = "&lHar;"
    output = [["Character", "⥢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lacute; with a semi-colon" do
    input = "&lacute;"
    output = [["Character", "ĺ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: laemptyv; with a semi-colon" do
    input = "&laemptyv;"
    output = [["Character", "⦴"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lagran; with a semi-colon" do
    input = "&lagran;"
    output = [["Character", "ℒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lambda; with a semi-colon" do
    input = "&lambda;"
    output = [["Character", "λ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lang; with a semi-colon" do
    input = "&lang;"
    output = [["Character", "⟨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: langd; with a semi-colon" do
    input = "&langd;"
    output = [["Character", "⦑"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: langle; with a semi-colon" do
    input = "&langle;"
    output = [["Character", "⟨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lap; with a semi-colon" do
    input = "&lap;"
    output = [["Character", "⪅"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: laquo without a semi-colon" do
    input = "&laquo"
    output = [["Character", "«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: laquo; with a semi-colon" do
    input = "&laquo;"
    output = [["Character", "«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larr; with a semi-colon" do
    input = "&larr;"
    output = [["Character", "←"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrb; with a semi-colon" do
    input = "&larrb;"
    output = [["Character", "⇤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrbfs; with a semi-colon" do
    input = "&larrbfs;"
    output = [["Character", "⤟"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrfs; with a semi-colon" do
    input = "&larrfs;"
    output = [["Character", "⤝"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrhk; with a semi-colon" do
    input = "&larrhk;"
    output = [["Character", "↩"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrlp; with a semi-colon" do
    input = "&larrlp;"
    output = [["Character", "↫"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrpl; with a semi-colon" do
    input = "&larrpl;"
    output = [["Character", "⤹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrsim; with a semi-colon" do
    input = "&larrsim;"
    output = [["Character", "⥳"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: larrtl; with a semi-colon" do
    input = "&larrtl;"
    output = [["Character", "↢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lat; with a semi-colon" do
    input = "&lat;"
    output = [["Character", "⪫"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: latail; with a semi-colon" do
    input = "&latail;"
    output = [["Character", "⤙"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: late; with a semi-colon" do
    input = "&late;"
    output = [["Character", "⪭"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lates; with a semi-colon" do
    input = "&lates;"
    output = [["Character", "⪭︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbarr; with a semi-colon" do
    input = "&lbarr;"
    output = [["Character", "⤌"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbbrk; with a semi-colon" do
    input = "&lbbrk;"
    output = [["Character", "❲"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrace; with a semi-colon" do
    input = "&lbrace;"
    output = [["Character", "{"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrack; with a semi-colon" do
    input = "&lbrack;"
    output = [["Character", "["]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrke; with a semi-colon" do
    input = "&lbrke;"
    output = [["Character", "⦋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrksld; with a semi-colon" do
    input = "&lbrksld;"
    output = [["Character", "⦏"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lbrkslu; with a semi-colon" do
    input = "&lbrkslu;"
    output = [["Character", "⦍"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lcaron; with a semi-colon" do
    input = "&lcaron;"
    output = [["Character", "ľ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lcedil; with a semi-colon" do
    input = "&lcedil;"
    output = [["Character", "ļ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lceil; with a semi-colon" do
    input = "&lceil;"
    output = [["Character", "⌈"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lcub; with a semi-colon" do
    input = "&lcub;"
    output = [["Character", "{"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lcy; with a semi-colon" do
    input = "&lcy;"
    output = [["Character", "л"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldca; with a semi-colon" do
    input = "&ldca;"
    output = [["Character", "⤶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldquo; with a semi-colon" do
    input = "&ldquo;"
    output = [["Character", "“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldquor; with a semi-colon" do
    input = "&ldquor;"
    output = [["Character", "„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldrdhar; with a semi-colon" do
    input = "&ldrdhar;"
    output = [["Character", "⥧"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldrushar; with a semi-colon" do
    input = "&ldrushar;"
    output = [["Character", "⥋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ldsh; with a semi-colon" do
    input = "&ldsh;"
    output = [["Character", "↲"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: le; with a semi-colon" do
    input = "&le;"
    output = [["Character", "≤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftarrow; with a semi-colon" do
    input = "&leftarrow;"
    output = [["Character", "←"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftarrowtail; with a semi-colon" do
    input = "&leftarrowtail;"
    output = [["Character", "↢"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftharpoondown; with a semi-colon" do
    input = "&leftharpoondown;"
    output = [["Character", "↽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftharpoonup; with a semi-colon" do
    input = "&leftharpoonup;"
    output = [["Character", "↼"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftleftarrows; with a semi-colon" do
    input = "&leftleftarrows;"
    output = [["Character", "⇇"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftrightarrow; with a semi-colon" do
    input = "&leftrightarrow;"
    output = [["Character", "↔"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftrightarrows; with a semi-colon" do
    input = "&leftrightarrows;"
    output = [["Character", "⇆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftrightharpoons; with a semi-colon" do
    input = "&leftrightharpoons;"
    output = [["Character", "⇋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftrightsquigarrow; with a semi-colon" do
    input = "&leftrightsquigarrow;"
    output = [["Character", "↭"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leftthreetimes; with a semi-colon" do
    input = "&leftthreetimes;"
    output = [["Character", "⋋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leg; with a semi-colon" do
    input = "&leg;"
    output = [["Character", "⋚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leq; with a semi-colon" do
    input = "&leq;"
    output = [["Character", "≤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leqq; with a semi-colon" do
    input = "&leqq;"
    output = [["Character", "≦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: leqslant; with a semi-colon" do
    input = "&leqslant;"
    output = [["Character", "⩽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: les; with a semi-colon" do
    input = "&les;"
    output = [["Character", "⩽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lescc; with a semi-colon" do
    input = "&lescc;"
    output = [["Character", "⪨"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lesdot; with a semi-colon" do
    input = "&lesdot;"
    output = [["Character", "⩿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lesdoto; with a semi-colon" do
    input = "&lesdoto;"
    output = [["Character", "⪁"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lesdotor; with a semi-colon" do
    input = "&lesdotor;"
    output = [["Character", "⪃"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: lesg; with a semi-colon" do
    input = "&lesg;"
    output = [["Character", "⋚︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end
