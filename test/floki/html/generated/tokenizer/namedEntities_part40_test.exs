defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart40Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: softcy; with a semi-colon" do
    input = "&softcy;"
    output = [["Character", "ÑŒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sol; with a semi-colon" do
    input = "&sol;"
    output = [["Character", "/"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: solb; with a semi-colon" do
    input = "&solb;"
    output = [["Character", "â§„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: solbar; with a semi-colon" do
    input = "&solbar;"
    output = [["Character", "âŒ¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sopf; with a semi-colon" do
    input = "&sopf;"
    output = [["Character", "ð•¤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: spades; with a semi-colon" do
    input = "&spades;"
    output = [["Character", "â™ "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: spadesuit; with a semi-colon" do
    input = "&spadesuit;"
    output = [["Character", "â™ "]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: spar; with a semi-colon" do
    input = "&spar;"
    output = [["Character", "âˆ¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqcap; with a semi-colon" do
    input = "&sqcap;"
    output = [["Character", "âŠ“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqcaps; with a semi-colon" do
    input = "&sqcaps;"
    output = [["Character", "âŠ“ï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqcup; with a semi-colon" do
    input = "&sqcup;"
    output = [["Character", "âŠ”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqcups; with a semi-colon" do
    input = "&sqcups;"
    output = [["Character", "âŠ”ï¸€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsub; with a semi-colon" do
    input = "&sqsub;"
    output = [["Character", "âŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsube; with a semi-colon" do
    input = "&sqsube;"
    output = [["Character", "âŠ‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsubset; with a semi-colon" do
    input = "&sqsubset;"
    output = [["Character", "âŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsubseteq; with a semi-colon" do
    input = "&sqsubseteq;"
    output = [["Character", "âŠ‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsup; with a semi-colon" do
    input = "&sqsup;"
    output = [["Character", "âŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsupe; with a semi-colon" do
    input = "&sqsupe;"
    output = [["Character", "âŠ’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsupset; with a semi-colon" do
    input = "&sqsupset;"
    output = [["Character", "âŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsupseteq; with a semi-colon" do
    input = "&sqsupseteq;"
    output = [["Character", "âŠ’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: squ; with a semi-colon" do
    input = "&squ;"
    output = [["Character", "â–¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: square; with a semi-colon" do
    input = "&square;"
    output = [["Character", "â–¡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: squarf; with a semi-colon" do
    input = "&squarf;"
    output = [["Character", "â–ª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: squf; with a semi-colon" do
    input = "&squf;"
    output = [["Character", "â–ª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: srarr; with a semi-colon" do
    input = "&srarr;"
    output = [["Character", "â†’"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sscr; with a semi-colon" do
    input = "&sscr;"
    output = [["Character", "ð“ˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ssetmn; with a semi-colon" do
    input = "&ssetmn;"
    output = [["Character", "âˆ–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ssmile; with a semi-colon" do
    input = "&ssmile;"
    output = [["Character", "âŒ£"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sstarf; with a semi-colon" do
    input = "&sstarf;"
    output = [["Character", "â‹†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: star; with a semi-colon" do
    input = "&star;"
    output = [["Character", "â˜†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: starf; with a semi-colon" do
    input = "&starf;"
    output = [["Character", "â˜…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: straightepsilon; with a semi-colon" do
    input = "&straightepsilon;"
    output = [["Character", "Ïµ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: straightphi; with a semi-colon" do
    input = "&straightphi;"
    output = [["Character", "Ï•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: strns; with a semi-colon" do
    input = "&strns;"
    output = [["Character", "Â¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sub; with a semi-colon" do
    input = "&sub;"
    output = [["Character", "âŠ‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subE; with a semi-colon" do
    input = "&subE;"
    output = [["Character", "â«…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subdot; with a semi-colon" do
    input = "&subdot;"
    output = [["Character", "âª½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sube; with a semi-colon" do
    input = "&sube;"
    output = [["Character", "âŠ†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subedot; with a semi-colon" do
    input = "&subedot;"
    output = [["Character", "â«ƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: submult; with a semi-colon" do
    input = "&submult;"
    output = [["Character", "â«"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subnE; with a semi-colon" do
    input = "&subnE;"
    output = [["Character", "â«‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subne; with a semi-colon" do
    input = "&subne;"
    output = [["Character", "âŠŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subplus; with a semi-colon" do
    input = "&subplus;"
    output = [["Character", "âª¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subrarr; with a semi-colon" do
    input = "&subrarr;"
    output = [["Character", "â¥¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subset; with a semi-colon" do
    input = "&subset;"
    output = [["Character", "âŠ‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subseteq; with a semi-colon" do
    input = "&subseteq;"
    output = [["Character", "âŠ†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subseteqq; with a semi-colon" do
    input = "&subseteqq;"
    output = [["Character", "â«…"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsetneq; with a semi-colon" do
    input = "&subsetneq;"
    output = [["Character", "âŠŠ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsetneqq; with a semi-colon" do
    input = "&subsetneqq;"
    output = [["Character", "â«‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsim; with a semi-colon" do
    input = "&subsim;"
    output = [["Character", "â«‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsub; with a semi-colon" do
    input = "&subsub;"
    output = [["Character", "â«•"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsup; with a semi-colon" do
    input = "&subsup;"
    output = [["Character", "â«“"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succ; with a semi-colon" do
    input = "&succ;"
    output = [["Character", "â‰»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succapprox; with a semi-colon" do
    input = "&succapprox;"
    output = [["Character", "âª¸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succcurlyeq; with a semi-colon" do
    input = "&succcurlyeq;"
    output = [["Character", "â‰½"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succeq; with a semi-colon" do
    input = "&succeq;"
    output = [["Character", "âª°"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succnapprox; with a semi-colon" do
    input = "&succnapprox;"
    output = [["Character", "âªº"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succneqq; with a semi-colon" do
    input = "&succneqq;"
    output = [["Character", "âª¶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succnsim; with a semi-colon" do
    input = "&succnsim;"
    output = [["Character", "â‹©"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succsim; with a semi-colon" do
    input = "&succsim;"
    output = [["Character", "â‰¿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sum; with a semi-colon" do
    input = "&sum;"
    output = [["Character", "âˆ‘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sung; with a semi-colon" do
    input = "&sung;"
    output = [["Character", "â™ª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup1 without a semi-colon" do
    input = "&sup1"
    output = [["Character", "Â¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup1; with a semi-colon" do
    input = "&sup1;"
    output = [["Character", "Â¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup2 without a semi-colon" do
    input = "&sup2"
    output = [["Character", "Â²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup2; with a semi-colon" do
    input = "&sup2;"
    output = [["Character", "Â²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup3 without a semi-colon" do
    input = "&sup3"
    output = [["Character", "Â³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup3; with a semi-colon" do
    input = "&sup3;"
    output = [["Character", "Â³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup; with a semi-colon" do
    input = "&sup;"
    output = [["Character", "âŠƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supE; with a semi-colon" do
    input = "&supE;"
    output = [["Character", "â«†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supdot; with a semi-colon" do
    input = "&supdot;"
    output = [["Character", "âª¾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supdsub; with a semi-colon" do
    input = "&supdsub;"
    output = [["Character", "â«˜"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supe; with a semi-colon" do
    input = "&supe;"
    output = [["Character", "âŠ‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supedot; with a semi-colon" do
    input = "&supedot;"
    output = [["Character", "â«„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: suphsol; with a semi-colon" do
    input = "&suphsol;"
    output = [["Character", "âŸ‰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: suphsub; with a semi-colon" do
    input = "&suphsub;"
    output = [["Character", "â«—"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: suplarr; with a semi-colon" do
    input = "&suplarr;"
    output = [["Character", "â¥»"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supmult; with a semi-colon" do
    input = "&supmult;"
    output = [["Character", "â«‚"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supnE; with a semi-colon" do
    input = "&supnE;"
    output = [["Character", "â«Œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supne; with a semi-colon" do
    input = "&supne;"
    output = [["Character", "âŠ‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supplus; with a semi-colon" do
    input = "&supplus;"
    output = [["Character", "â«€"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supset; with a semi-colon" do
    input = "&supset;"
    output = [["Character", "âŠƒ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supseteq; with a semi-colon" do
    input = "&supseteq;"
    output = [["Character", "âŠ‡"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supseteqq; with a semi-colon" do
    input = "&supseteqq;"
    output = [["Character", "â«†"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsetneq; with a semi-colon" do
    input = "&supsetneq;"
    output = [["Character", "âŠ‹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsetneqq; with a semi-colon" do
    input = "&supsetneqq;"
    output = [["Character", "â«Œ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsim; with a semi-colon" do
    input = "&supsim;"
    output = [["Character", "â«ˆ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsub; with a semi-colon" do
    input = "&supsub;"
    output = [["Character", "â«”"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsup; with a semi-colon" do
    input = "&supsup;"
    output = [["Character", "â«–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swArr; with a semi-colon" do
    input = "&swArr;"
    output = [["Character", "â‡™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swarhk; with a semi-colon" do
    input = "&swarhk;"
    output = [["Character", "â¤¦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swarr; with a semi-colon" do
    input = "&swarr;"
    output = [["Character", "â†™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swarrow; with a semi-colon" do
    input = "&swarrow;"
    output = [["Character", "â†™"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swnwar; with a semi-colon" do
    input = "&swnwar;"
    output = [["Character", "â¤ª"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: szlig without a semi-colon" do
    input = "&szlig"
    output = [["Character", "ÃŸ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: szlig; with a semi-colon" do
    input = "&szlig;"
    output = [["Character", "ÃŸ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: target; with a semi-colon" do
    input = "&target;"
    output = [["Character", "âŒ–"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: tau; with a semi-colon" do
    input = "&tau;"
    output = [["Character", "Ï„"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: tbrk; with a semi-colon" do
    input = "&tbrk;"
    output = [["Character", "âŽ´"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: tcaron; with a semi-colon" do
    input = "&tcaron;"
    output = [["Character", "Å¥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end