defmodule Floki.HTML.Generated.Tokenizer.NamedentitiesPart40Test do
  use ExUnit.Case, async: true

  # NOTE: This file was generated by "mix generate_tokenizer_tests namedEntities.test".
  # html5lib-tests rev: e52ff68cc7113a6ef3687747fa82691079bf9cc5

  alias Floki.HTML.Tokenizer

  test "tokenize/1 Named entity: softcy; with a semi-colon" do
    input = "&softcy;"
    output = [["Character", "ь"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sol; with a semi-colon" do
    input = "&sol;"
    output = [["Character", "/"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: solb; with a semi-colon" do
    input = "&solb;"
    output = [["Character", "⧄"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: solbar; with a semi-colon" do
    input = "&solbar;"
    output = [["Character", "⌿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sopf; with a semi-colon" do
    input = "&sopf;"
    output = [["Character", "𝕤"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: spades; with a semi-colon" do
    input = "&spades;"
    output = [["Character", "♠"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: spadesuit; with a semi-colon" do
    input = "&spadesuit;"
    output = [["Character", "♠"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: spar; with a semi-colon" do
    input = "&spar;"
    output = [["Character", "∥"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqcap; with a semi-colon" do
    input = "&sqcap;"
    output = [["Character", "⊓"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqcaps; with a semi-colon" do
    input = "&sqcaps;"
    output = [["Character", "⊓︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqcup; with a semi-colon" do
    input = "&sqcup;"
    output = [["Character", "⊔"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqcups; with a semi-colon" do
    input = "&sqcups;"
    output = [["Character", "⊔︀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsub; with a semi-colon" do
    input = "&sqsub;"
    output = [["Character", "⊏"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsube; with a semi-colon" do
    input = "&sqsube;"
    output = [["Character", "⊑"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsubset; with a semi-colon" do
    input = "&sqsubset;"
    output = [["Character", "⊏"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsubseteq; with a semi-colon" do
    input = "&sqsubseteq;"
    output = [["Character", "⊑"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsup; with a semi-colon" do
    input = "&sqsup;"
    output = [["Character", "⊐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsupe; with a semi-colon" do
    input = "&sqsupe;"
    output = [["Character", "⊒"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsupset; with a semi-colon" do
    input = "&sqsupset;"
    output = [["Character", "⊐"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sqsupseteq; with a semi-colon" do
    input = "&sqsupseteq;"
    output = [["Character", "⊒"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: squ; with a semi-colon" do
    input = "&squ;"
    output = [["Character", "□"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: square; with a semi-colon" do
    input = "&square;"
    output = [["Character", "□"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: squarf; with a semi-colon" do
    input = "&squarf;"
    output = [["Character", "▪"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: squf; with a semi-colon" do
    input = "&squf;"
    output = [["Character", "▪"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: srarr; with a semi-colon" do
    input = "&srarr;"
    output = [["Character", "→"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sscr; with a semi-colon" do
    input = "&sscr;"
    output = [["Character", "𝓈"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ssetmn; with a semi-colon" do
    input = "&ssetmn;"
    output = [["Character", "∖"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: ssmile; with a semi-colon" do
    input = "&ssmile;"
    output = [["Character", "⌣"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sstarf; with a semi-colon" do
    input = "&sstarf;"
    output = [["Character", "⋆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: star; with a semi-colon" do
    input = "&star;"
    output = [["Character", "☆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: starf; with a semi-colon" do
    input = "&starf;"
    output = [["Character", "★"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: straightepsilon; with a semi-colon" do
    input = "&straightepsilon;"
    output = [["Character", "ϵ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: straightphi; with a semi-colon" do
    input = "&straightphi;"
    output = [["Character", "ϕ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: strns; with a semi-colon" do
    input = "&strns;"
    output = [["Character", "¯"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sub; with a semi-colon" do
    input = "&sub;"
    output = [["Character", "⊂"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subE; with a semi-colon" do
    input = "&subE;"
    output = [["Character", "⫅"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subdot; with a semi-colon" do
    input = "&subdot;"
    output = [["Character", "⪽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sube; with a semi-colon" do
    input = "&sube;"
    output = [["Character", "⊆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subedot; with a semi-colon" do
    input = "&subedot;"
    output = [["Character", "⫃"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: submult; with a semi-colon" do
    input = "&submult;"
    output = [["Character", "⫁"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subnE; with a semi-colon" do
    input = "&subnE;"
    output = [["Character", "⫋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subne; with a semi-colon" do
    input = "&subne;"
    output = [["Character", "⊊"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subplus; with a semi-colon" do
    input = "&subplus;"
    output = [["Character", "⪿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subrarr; with a semi-colon" do
    input = "&subrarr;"
    output = [["Character", "⥹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subset; with a semi-colon" do
    input = "&subset;"
    output = [["Character", "⊂"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subseteq; with a semi-colon" do
    input = "&subseteq;"
    output = [["Character", "⊆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subseteqq; with a semi-colon" do
    input = "&subseteqq;"
    output = [["Character", "⫅"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsetneq; with a semi-colon" do
    input = "&subsetneq;"
    output = [["Character", "⊊"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsetneqq; with a semi-colon" do
    input = "&subsetneqq;"
    output = [["Character", "⫋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsim; with a semi-colon" do
    input = "&subsim;"
    output = [["Character", "⫇"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsub; with a semi-colon" do
    input = "&subsub;"
    output = [["Character", "⫕"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: subsup; with a semi-colon" do
    input = "&subsup;"
    output = [["Character", "⫓"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succ; with a semi-colon" do
    input = "&succ;"
    output = [["Character", "≻"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succapprox; with a semi-colon" do
    input = "&succapprox;"
    output = [["Character", "⪸"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succcurlyeq; with a semi-colon" do
    input = "&succcurlyeq;"
    output = [["Character", "≽"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succeq; with a semi-colon" do
    input = "&succeq;"
    output = [["Character", "⪰"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succnapprox; with a semi-colon" do
    input = "&succnapprox;"
    output = [["Character", "⪺"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succneqq; with a semi-colon" do
    input = "&succneqq;"
    output = [["Character", "⪶"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succnsim; with a semi-colon" do
    input = "&succnsim;"
    output = [["Character", "⋩"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: succsim; with a semi-colon" do
    input = "&succsim;"
    output = [["Character", "≿"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sum; with a semi-colon" do
    input = "&sum;"
    output = [["Character", "∑"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sung; with a semi-colon" do
    input = "&sung;"
    output = [["Character", "♪"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup1 without a semi-colon" do
    input = "&sup1"
    output = [["Character", "¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup1; with a semi-colon" do
    input = "&sup1;"
    output = [["Character", "¹"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup2 without a semi-colon" do
    input = "&sup2"
    output = [["Character", "²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup2; with a semi-colon" do
    input = "&sup2;"
    output = [["Character", "²"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup3 without a semi-colon" do
    input = "&sup3"
    output = [["Character", "³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup3; with a semi-colon" do
    input = "&sup3;"
    output = [["Character", "³"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: sup; with a semi-colon" do
    input = "&sup;"
    output = [["Character", "⊃"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supE; with a semi-colon" do
    input = "&supE;"
    output = [["Character", "⫆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supdot; with a semi-colon" do
    input = "&supdot;"
    output = [["Character", "⪾"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supdsub; with a semi-colon" do
    input = "&supdsub;"
    output = [["Character", "⫘"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supe; with a semi-colon" do
    input = "&supe;"
    output = [["Character", "⊇"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supedot; with a semi-colon" do
    input = "&supedot;"
    output = [["Character", "⫄"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: suphsol; with a semi-colon" do
    input = "&suphsol;"
    output = [["Character", "⟉"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: suphsub; with a semi-colon" do
    input = "&suphsub;"
    output = [["Character", "⫗"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: suplarr; with a semi-colon" do
    input = "&suplarr;"
    output = [["Character", "⥻"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supmult; with a semi-colon" do
    input = "&supmult;"
    output = [["Character", "⫂"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supnE; with a semi-colon" do
    input = "&supnE;"
    output = [["Character", "⫌"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supne; with a semi-colon" do
    input = "&supne;"
    output = [["Character", "⊋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supplus; with a semi-colon" do
    input = "&supplus;"
    output = [["Character", "⫀"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supset; with a semi-colon" do
    input = "&supset;"
    output = [["Character", "⊃"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supseteq; with a semi-colon" do
    input = "&supseteq;"
    output = [["Character", "⊇"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supseteqq; with a semi-colon" do
    input = "&supseteqq;"
    output = [["Character", "⫆"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsetneq; with a semi-colon" do
    input = "&supsetneq;"
    output = [["Character", "⊋"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsetneqq; with a semi-colon" do
    input = "&supsetneqq;"
    output = [["Character", "⫌"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsim; with a semi-colon" do
    input = "&supsim;"
    output = [["Character", "⫈"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsub; with a semi-colon" do
    input = "&supsub;"
    output = [["Character", "⫔"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: supsup; with a semi-colon" do
    input = "&supsup;"
    output = [["Character", "⫖"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swArr; with a semi-colon" do
    input = "&swArr;"
    output = [["Character", "⇙"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swarhk; with a semi-colon" do
    input = "&swarhk;"
    output = [["Character", "⤦"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swarr; with a semi-colon" do
    input = "&swarr;"
    output = [["Character", "↙"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swarrow; with a semi-colon" do
    input = "&swarrow;"
    output = [["Character", "↙"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: swnwar; with a semi-colon" do
    input = "&swnwar;"
    output = [["Character", "⤪"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: szlig without a semi-colon" do
    input = "&szlig"
    output = [["Character", "ß"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: szlig; with a semi-colon" do
    input = "&szlig;"
    output = [["Character", "ß"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: target; with a semi-colon" do
    input = "&target;"
    output = [["Character", "⌖"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: tau; with a semi-colon" do
    input = "&tau;"
    output = [["Character", "τ"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: tbrk; with a semi-colon" do
    input = "&tbrk;"
    output = [["Character", "⎴"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end

  test "tokenize/1 Named entity: tcaron; with a semi-colon" do
    input = "&tcaron;"
    output = [["Character", "ť"]]

    result =
      input
      |> Tokenizer.tokenize()
      |> TokenizerTestLoader.tokenization_result()

    assert result.tokens == output
  end
end